{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see https://medium.com/swlh/learning-with-deep-sarsa-openai-gym-c9a470d027a\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.cem import CEMAgent\n",
    "from rl.agents.sarsa import SARSAAgent\n",
    "from rl.memory import EpisodeParameterMemory\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import LinearAnnealedPolicy\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(13, 2)\n",
      "Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('gym_ent:ent-v1')\n",
    "\n",
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "\n",
    "nb_actions = env.action_space.n\n",
    "obs_dim = env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 26)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4)                 108       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 108\n",
      "Trainable params: 108\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Option 1 : Simple model\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('softmax'))\n",
    "print(model.summary())\n",
    "\n",
    "#sars = SARSAAgent(model=model, nb_actions=nb_actions, nb_steps_warmup=1000, train_interval=1)\n",
    "sars = SARSAAgent(model=model, nb_actions=nb_actions, nb_steps_warmup=2000, train_interval=1)\n",
    "sars.compile(optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     def __init__(self, nb_actions, memory, gamma=.99, batch_size=32, nb_steps_warmup=1000,\n",
    "#                  train_interval=1, memory_interval=1, target_model_update=10000,\n",
    "#                  delta_range=None, delta_clip=np.inf, custom_model_objects={}, **kwargs):\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "sars = DQNAgent(model=model, nb_actions=nb_actions, nb_steps_warmup=1000, train_interval=1)\n",
    "sars.compile(optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "#model.add(Activation('relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "#model.add(Activation('relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "#model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#sars = SARSAAgent(model=model, nb_actions=nb_actions, nb_steps_warmup=1000, train_interval=1)\n",
    "sars = SARSAAgent(model=model, nb_actions=nb_actions, nb_steps_warmup=500, train_interval=1)\n",
    "sars.compile(optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 400000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "   80/10000 [..............................] - ETA: 6s - reward: -0.3750    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lkoziol/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 41s 4ms/step - reward: -2.1956\n",
      "767 episodes - episode_reward: -28.626 [-30.000, 1.000] - loss: 0.003 - mean_q: 0.276\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 42s 4ms/step - reward: -2.1508\n",
      "750 episodes - episode_reward: -28.677 [-30.000, 1.000] - loss: 0.002 - mean_q: 0.251\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 43s 4ms/step - reward: -2.3939\n",
      "830 episodes - episode_reward: -28.842 [-30.000, 1.000] - loss: 0.002 - mean_q: 0.251\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 42s 4ms/step - reward: -2.3070\n",
      "800 episodes - episode_reward: -28.837 [-30.000, 1.000] - loss: 0.002 - mean_q: 0.251\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: -1.9858\n",
      "695 episodes - episode_reward: -28.573 [-30.000, 1.000] - loss: 0.002 - mean_q: 0.251\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 42s 4ms/step - reward: -2.1628\n",
      "754 episodes - episode_reward: -28.684 [-30.000, 1.000] - loss: 0.002 - mean_q: 0.252\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 42s 4ms/step - reward: -2.1021\n",
      "741 episodes - episode_reward: -28.368 [-30.000, 1.000] - loss: 0.002 - mean_q: 0.252\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 42s 4ms/step - reward: -2.0939\n",
      "730 episodes - episode_reward: -28.684 [-30.000, 1.000] - loss: 0.002 - mean_q: 0.251\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: -2.0215\n",
      "710 episodes - episode_reward: -28.472 [-30.000, 1.000] - loss: 0.002 - mean_q: 0.252\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: -2.1206\n",
      "742 episodes - episode_reward: -28.580 [-30.000, 1.000] - loss: 0.002 - mean_q: 0.251\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: -2.0996\n",
      "735 episodes - episode_reward: -28.566 [-30.000, 1.000] - loss: 0.002 - mean_q: 0.252\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: -1.8314\n",
      "658 episodes - episode_reward: -27.833 [-30.000, 1.000] - loss: 0.002 - mean_q: 0.252\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: -2.2136\n",
      "773 episodes - episode_reward: -28.636 [-30.000, 1.000] - loss: 0.002 - mean_q: 0.251\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: -1.7657\n",
      "633 episodes - episode_reward: -27.894 [-30.000, 1.000] - loss: 0.002 - mean_q: 0.252\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: -2.2645\n",
      "791 episodes - episode_reward: -28.628 [-30.000, 1.000] - loss: 0.002 - mean_q: 0.252\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: -1.8467\n",
      "660 episodes - episode_reward: -27.980 [-30.000, 1.000] - loss: 0.002 - mean_q: 0.252\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: -2.2737\n",
      "792 episodes - episode_reward: -28.708 [-30.000, 1.000] - loss: 0.002 - mean_q: 0.251\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: -2.1450\n",
      "746 episodes - episode_reward: -28.753 [-30.000, 1.000] - loss: 0.002 - mean_q: 0.251\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: -1.8625\n",
      "657 episodes - episode_reward: -28.349 [-30.000, 1.000] - loss: 0.002 - mean_q: 0.252\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: -2.1774\n",
      "763 episodes - episode_reward: -28.537 [-30.000, 1.000] - loss: 0.002 - mean_q: 0.251\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: -1.9250\n",
      "683 episodes - episode_reward: -28.184 [-30.000, 1.000] - loss: 0.002 - mean_q: 0.251\n",
      "\n",
      "Interval 22 (210000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: -2.3362\n",
      "818 episodes - episode_reward: -28.560 [-30.000, 1.000] - loss: 0.002 - mean_q: 0.251\n",
      "\n",
      "Interval 23 (220000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: -2.0515\n",
      "720 episodes - episode_reward: -28.493 [-30.000, 1.000] - loss: 0.002 - mean_q: 0.252\n",
      "\n",
      "Interval 24 (230000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: -2.2800\n",
      "791 episodes - episode_reward: -28.824 [-30.000, 1.000] - loss: 0.002 - mean_q: 0.251\n",
      "\n",
      "Interval 25 (240000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: -1.8982\n",
      "672 episodes - episode_reward: -28.247 [-30.000, 1.000] - loss: 0.002 - mean_q: 0.251\n",
      "\n",
      "Interval 26 (250000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: -2.1899\n",
      "762 episodes - episode_reward: -28.739 [-30.000, 1.000] - loss: 0.002 - mean_q: 0.252\n",
      "\n",
      "Interval 27 (260000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: -2.2833\n",
      "789 episodes - episode_reward: -28.939 [-30.000, 1.000] - loss: 0.002 - mean_q: 0.251\n",
      "\n",
      "Interval 28 (270000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: -2.1926\n",
      "766 episodes - episode_reward: -28.624 [-30.000, 1.000] - loss: 0.002 - mean_q: 0.251\n",
      "\n",
      "Interval 29 (280000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: -2.1892\n",
      "769 episodes - episode_reward: -28.468 [-30.000, 1.000] - loss: 0.002 - mean_q: 0.251\n",
      "\n",
      "Interval 30 (290000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: -2.1237\n",
      "742 episodes - episode_reward: -28.621 [-30.000, 1.000] - loss: 0.002 - mean_q: 0.251\n",
      "\n",
      "Interval 31 (300000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: -2.0123\n",
      "709 episodes - episode_reward: -28.382 [-30.000, 1.000] - loss: 0.002 - mean_q: 0.252\n",
      "\n",
      "Interval 32 (310000 steps performed)\n",
      "10000/10000 [==============================] - 42s 4ms/step - reward: -2.0547\n",
      "719 episodes - episode_reward: -28.577 [-30.000, 1.000] - loss: 0.002 - mean_q: 0.252\n",
      "\n",
      "Interval 33 (320000 steps performed)\n",
      "10000/10000 [==============================] - 42s 4ms/step - reward: -2.0516\n",
      "719 episodes - episode_reward: -28.534 [-30.000, 1.000] - loss: 0.002 - mean_q: 0.252\n",
      "\n",
      "Interval 34 (330000 steps performed)\n",
      "10000/10000 [==============================] - 43s 4ms/step - reward: -2.0632\n",
      "727 episodes - episode_reward: -28.380 [-30.000, 1.000] - loss: 0.002 - mean_q: 0.251\n",
      "\n",
      "Interval 35 (340000 steps performed)\n",
      "10000/10000 [==============================] - 43s 4ms/step - reward: -1.9644\n",
      "692 episodes - episode_reward: -28.387 [-30.000, 1.000] - loss: 0.002 - mean_q: 0.252\n",
      "\n",
      "Interval 36 (350000 steps performed)\n",
      "10000/10000 [==============================] - 43s 4ms/step - reward: -1.9394\n",
      "694 episodes - episode_reward: -27.945 [-30.000, 1.000] - loss: 0.002 - mean_q: 0.252\n",
      "\n",
      "Interval 37 (360000 steps performed)\n",
      "10000/10000 [==============================] - 43s 4ms/step - reward: -2.3997\n",
      "834 episodes - episode_reward: -28.773 [-30.000, 1.000] - loss: 0.002 - mean_q: 0.252\n",
      "\n",
      "Interval 38 (370000 steps performed)\n",
      "10000/10000 [==============================] - 43s 4ms/step - reward: -2.1417\n",
      "748 episodes - episode_reward: -28.632 [-30.000, 1.000] - loss: 0.002 - mean_q: 0.251\n",
      "\n",
      "Interval 39 (380000 steps performed)\n",
      "10000/10000 [==============================] - 43s 4ms/step - reward: -2.2401\n",
      "787 episodes - episode_reward: -28.464 [-30.000, 1.000] - loss: 0.002 - mean_q: 0.251\n",
      "\n",
      "Interval 40 (390000 steps performed)\n",
      "10000/10000 [==============================] - 43s 4ms/step - reward: -2.0911\n",
      "done, took 1660.027 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1e806f3820>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sars.fit(env, nb_steps=500, visualize=False, verbose=1)\n",
    "sars.fit(env, nb_steps=400000, visualize=False, verbose=1)\n",
    "# After training is done, we save the best weights.\n",
    "#sars.save_weights(f'sars_{ENV_NAME}_params.h5f', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n"
     ]
    }
   ],
   "source": [
    "sars.test(env, nb_episodes=10, visualize=True)\n",
    "# 8/25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory = EpisodeParameterMemory(limit=2000, window_length=1)\n",
    "# cem = CEMAgent(model=model, nb_actions=nb_actions, memory=memory,\n",
    "#                batch_size=50, nb_steps_warmup=500, train_interval=1, elite_frac=0.05)\n",
    "# cem.compile()\n",
    "\n",
    "# cem.fit(env, nb_steps=10000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cem.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render(close=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
