{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.cem import CEMAgent\n",
    "from rl.agents.sarsa import SARSAAgent\n",
    "from rl.memory import EpisodeParameterMemory\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import LinearAnnealedPolicy\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(8, 2)\n",
      "Discrete(4)\n",
      "4\n",
      "(8, 2)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('gym_ent:ent-v1')\n",
    "\n",
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "\n",
    "nb_actions = env.action_space.n\n",
    "obs_dim = env.observation_space.shape\n",
    "print(nb_actions)\n",
    "print(obs_dim)\n",
    "#env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4)                 68        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 68\n",
      "Trainable params: 68\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Option 1 : Simple model\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sars = SARSAAgent(model=model, nb_actions=nb_actions, nb_steps_warmup=2000, train_interval=100)\n",
    "sars.compile(optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 400000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "  184/10000 [..............................] - ETA: 5s - reward: 9.0543  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lkoziol/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 6s 546us/step - reward: 9.5903\n",
      "1097 episodes - episode_reward: 87.422 [0.000, 109.000] - loss: 485.075 - mean_q: 1.000\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 5s 522us/step - reward: 9.6491\n",
      "1109 episodes - episode_reward: 87.002 [0.000, 109.000] - loss: 288.785 - mean_q: 1.000\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 5s 521us/step - reward: 9.7601\n",
      "1099 episodes - episode_reward: 88.809 [0.000, 109.000] - loss: 357.866 - mean_q: 1.000\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 5s 528us/step - reward: 9.6796\n",
      "1104 episodes - episode_reward: 87.684 [0.000, 109.000] - loss: 474.009 - mean_q: 1.000\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 5s 521us/step - reward: 9.5087\n",
      "1113 episodes - episode_reward: 85.428 [0.000, 109.000] - loss: 442.126 - mean_q: 1.000\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 5s 511us/step - reward: 9.6708\n",
      "1092 episodes - episode_reward: 88.566 [0.000, 109.000] - loss: 663.419 - mean_q: 1.000\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 5s 519us/step - reward: 9.6906\n",
      "1094 episodes - episode_reward: 88.574 [0.000, 109.000] - loss: 329.425 - mean_q: 1.000\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 5s 520us/step - reward: 9.8706\n",
      "1094 episodes - episode_reward: 90.223 [0.000, 109.000] - loss: 749.916 - mean_q: 1.000\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 5s 526us/step - reward: 9.7094\n",
      "1106 episodes - episode_reward: 87.793 [0.000, 109.000] - loss: 422.077 - mean_q: 1.000\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 5s 519us/step - reward: 9.6797\n",
      "1103 episodes - episode_reward: 87.759 [0.000, 109.000] - loss: 485.129 - mean_q: 1.000\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 5s 526us/step - reward: 9.8510\n",
      "1090 episodes - episode_reward: 90.377 [0.000, 109.000] - loss: 423.088 - mean_q: 1.000\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 5s 534us/step - reward: 9.7200\n",
      "1100 episodes - episode_reward: 88.359 [0.000, 109.000] - loss: 551.110 - mean_q: 1.000\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 5s 524us/step - reward: 9.7303\n",
      "1097 episodes - episode_reward: 88.698 [0.000, 109.000] - loss: 592.849 - mean_q: 1.000\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 5s 526us/step - reward: 9.6998\n",
      "1102 episodes - episode_reward: 88.024 [0.000, 109.000] - loss: 1058.745 - mean_q: 1.000\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 5s 531us/step - reward: 9.7089\n",
      "1111 episodes - episode_reward: 87.389 [0.000, 109.000] - loss: 627.140 - mean_q: 1.000\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 5s 521us/step - reward: 9.6077\n",
      "1123 episodes - episode_reward: 85.549 [0.000, 109.000] - loss: 732.773 - mean_q: 1.000\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 5s 526us/step - reward: 9.6796\n",
      "1104 episodes - episode_reward: 87.678 [0.000, 109.000] - loss: 533.208 - mean_q: 1.000\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 5s 523us/step - reward: 9.8712\n",
      "1088 episodes - episode_reward: 90.735 [0.000, 109.000] - loss: 513.404 - mean_q: 1.000\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 5s 524us/step - reward: 9.6998\n",
      "1102 episodes - episode_reward: 88.013 [0.000, 109.000] - loss: 676.354 - mean_q: 1.000\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 5s 530us/step - reward: 9.7600\n",
      "1100 episodes - episode_reward: 88.732 [0.000, 109.000] - loss: 272.763 - mean_q: 1.000\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      "10000/10000 [==============================] - 5s 523us/step - reward: 9.6386\n",
      "1114 episodes - episode_reward: 86.524 [0.000, 109.000] - loss: 641.839 - mean_q: 1.000\n",
      "\n",
      "Interval 22 (210000 steps performed)\n",
      "10000/10000 [==============================] - 5s 528us/step - reward: 9.6795\n",
      "1105 episodes - episode_reward: 87.593 [0.000, 109.000] - loss: 387.081 - mean_q: 1.000\n",
      "\n",
      "Interval 23 (220000 steps performed)\n",
      "10000/10000 [==============================] - 5s 541us/step - reward: 9.7386\n",
      "1114 episodes - episode_reward: 87.421 [0.000, 109.000] - loss: 445.974 - mean_q: 1.000\n",
      "\n",
      "Interval 24 (230000 steps performed)\n",
      "10000/10000 [==============================] - 5s 521us/step - reward: 9.7706\n",
      "1094 episodes - episode_reward: 89.313 [0.000, 109.000] - loss: 525.533 - mean_q: 1.000\n",
      "\n",
      "Interval 25 (240000 steps performed)\n",
      "10000/10000 [==============================] - 5s 526us/step - reward: 9.6989\n",
      "1111 episodes - episode_reward: 87.302 [0.000, 109.000] - loss: 236.714 - mean_q: 1.000\n",
      "\n",
      "Interval 26 (250000 steps performed)\n",
      "10000/10000 [==============================] - 5s 527us/step - reward: 9.7704\n",
      "1096 episodes - episode_reward: 89.144 [0.000, 109.000] - loss: 423.179 - mean_q: 1.000\n",
      "\n",
      "Interval 27 (260000 steps performed)\n",
      "10000/10000 [==============================] - 5s 527us/step - reward: 9.8504\n",
      "1096 episodes - episode_reward: 89.877 [0.000, 109.000] - loss: 394.841 - mean_q: 1.000\n",
      "\n",
      "Interval 28 (270000 steps performed)\n",
      "10000/10000 [==============================] - 5s 528us/step - reward: 9.7003\n",
      "1097 episodes - episode_reward: 88.420 [0.000, 109.000] - loss: 640.785 - mean_q: 1.000\n",
      "\n",
      "Interval 29 (280000 steps performed)\n",
      "10000/10000 [==============================] - 5s 526us/step - reward: 9.7101\n",
      "1099 episodes - episode_reward: 88.360 [0.000, 109.000] - loss: 382.719 - mean_q: 1.000\n",
      "\n",
      "Interval 30 (290000 steps performed)\n",
      "10000/10000 [==============================] - 5s 522us/step - reward: 9.9113\n",
      "1087 episodes - episode_reward: 91.178 [0.000, 109.000] - loss: 385.930 - mean_q: 1.000\n",
      "\n",
      "Interval 31 (300000 steps performed)\n",
      "10000/10000 [==============================] - 5s 530us/step - reward: 9.8204\n",
      "1096 episodes - episode_reward: 89.604 [0.000, 109.000] - loss: 451.163 - mean_q: 1.000\n",
      "\n",
      "Interval 32 (310000 steps performed)\n",
      "10000/10000 [==============================] - 5s 525us/step - reward: 9.5882\n",
      "1118 episodes - episode_reward: 85.763 [0.000, 109.000] - loss: 575.013 - mean_q: 1.000\n",
      "\n",
      "Interval 33 (320000 steps performed)\n",
      "10000/10000 [==============================] - 5s 521us/step - reward: 9.7294\n",
      "1106 episodes - episode_reward: 87.966 [0.000, 109.000] - loss: 436.108 - mean_q: 1.000\n",
      "\n",
      "Interval 34 (330000 steps performed)\n",
      "10000/10000 [==============================] - 5s 528us/step - reward: 9.7189\n",
      "1111 episodes - episode_reward: 87.475 [0.000, 109.000] - loss: 922.909 - mean_q: 1.000\n",
      "\n",
      "Interval 35 (340000 steps performed)\n",
      "10000/10000 [==============================] - 5s 517us/step - reward: 9.8200\n",
      "1100 episodes - episode_reward: 89.275 [0.000, 109.000] - loss: 578.238 - mean_q: 1.000\n",
      "\n",
      "Interval 36 (350000 steps performed)\n",
      "10000/10000 [==============================] - 5s 537us/step - reward: 9.5885\n",
      "1115 episodes - episode_reward: 85.999 [0.000, 109.000] - loss: 324.788 - mean_q: 1.000\n",
      "\n",
      "Interval 37 (360000 steps performed)\n",
      "10000/10000 [==============================] - 5s 516us/step - reward: 9.9011\n",
      "1089 episodes - episode_reward: 90.916 [0.000, 109.000] - loss: 479.888 - mean_q: 1.000\n",
      "\n",
      "Interval 38 (370000 steps performed)\n",
      "10000/10000 [==============================] - 5s 524us/step - reward: 9.7806\n",
      "1094 episodes - episode_reward: 89.406 [0.000, 109.000] - loss: 431.294 - mean_q: 1.000\n",
      "\n",
      "Interval 39 (380000 steps performed)\n",
      "10000/10000 [==============================] - 5s 528us/step - reward: 9.6687\n",
      "1113 episodes - episode_reward: 86.866 [0.000, 109.000] - loss: 452.330 - mean_q: 1.000\n",
      "\n",
      "Interval 40 (390000 steps performed)\n",
      "10000/10000 [==============================] - 5s 528us/step - reward: 9.5494\n",
      "done, took 211.113 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fad82a41310>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sars.fit(env, nb_steps=400000, visualize=False, verbose=1)\n",
    "# After training is done, we save the best weights.\n",
    "#sars.save_weights(f'sars_{ENV_NAME}_params.h5f', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 25 episodes ...\n",
      "Episode 1: reward: 109.000, steps: 10\n",
      "Episode 2: reward: 109.000, steps: 10\n",
      "Episode 3: reward: 109.000, steps: 10\n",
      "Episode 4: reward: 109.000, steps: 10\n",
      "Episode 5: reward: 109.000, steps: 10\n",
      "Episode 6: reward: 109.000, steps: 10\n",
      "Episode 7: reward: 109.000, steps: 10\n",
      "Episode 8: reward: 109.000, steps: 10\n",
      "Episode 9: reward: 4.000, steps: 5\n",
      "Episode 10: reward: 109.000, steps: 10\n",
      "Episode 11: reward: 109.000, steps: 10\n",
      "Episode 12: reward: 109.000, steps: 10\n",
      "Episode 13: reward: 8.000, steps: 9\n",
      "Episode 14: reward: 109.000, steps: 10\n",
      "Episode 15: reward: 109.000, steps: 10\n",
      "Episode 16: reward: 109.000, steps: 10\n",
      "Episode 17: reward: 109.000, steps: 10\n",
      "Episode 18: reward: 109.000, steps: 10\n",
      "Episode 19: reward: 109.000, steps: 10\n",
      "Episode 20: reward: 109.000, steps: 10\n",
      "Episode 21: reward: 109.000, steps: 10\n",
      "Episode 22: reward: 109.000, steps: 10\n",
      "Episode 23: reward: 109.000, steps: 10\n",
      "Episode 24: reward: 109.000, steps: 10\n",
      "Episode 25: reward: 109.000, steps: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fad82a8c220>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sars.test(env, nb_episodes=25, visualize=True)\n",
    "# 8/25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 18s 2ms/step - reward: -0.5846\n",
      "done, took 17.598 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f53fc4d4880>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = EpisodeParameterMemory(limit=2000, window_length=1)\n",
    "cem = CEMAgent(model=model, nb_actions=nb_actions, memory=memory,\n",
    "               batch_size=50, nb_steps_warmup=500, train_interval=50, elite_frac=0.05)\n",
    "cem.compile()\n",
    "\n",
    "cem.fit(env, nb_steps=10000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 1.000, steps: 1\n",
      "Episode 2: reward: 1.000, steps: 1\n",
      "Episode 3: reward: 1.000, steps: 1\n",
      "Episode 4: reward: 1.000, steps: 1\n",
      "Episode 5: reward: 1.000, steps: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f54025e1130>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cem.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
