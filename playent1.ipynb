{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.cem import CEMAgent\n",
    "from rl.agents.sarsa import SARSAAgent\n",
    "from rl.memory import EpisodeParameterMemory\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import LinearAnnealedPolicy\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(12, 2)\n",
      "Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('gym_ent:ent-v1')\n",
    "\n",
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "\n",
    "nb_actions = env.action_space.n\n",
    "obs_dim = env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 24)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4)                 100       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 100\n",
      "Trainable params: 100\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Option 1 : Simple model\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('softmax'))\n",
    "print(model.summary())\n",
    "\n",
    "#sars = SARSAAgent(model=model, nb_actions=nb_actions, nb_steps_warmup=1000, train_interval=100)\n",
    "sars = SARSAAgent(model=model, nb_actions=nb_actions, nb_steps_warmup=2000, train_interval=100)\n",
    "sars.compile(optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#sars = SARSAAgent(model=model, nb_actions=nb_actions, nb_steps_warmup=1000, train_interval=100)\n",
    "sars = SARSAAgent(model=model, nb_actions=nb_actions, nb_steps_warmup=2000, train_interval=100)\n",
    "sars.compile(optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 400000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "   59/10000 [..............................] - ETA: 8s - reward: -15.2542     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lkoziol/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 11s 1ms/step - reward: -26.1700\n",
      "3377 episodes - episode_reward: -77.495 [-100.000, 100.000] - loss: 1415.274 - mean_q: 0.801\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 10s 980us/step - reward: -24.0200\n",
      "3212 episodes - episode_reward: -74.782 [-100.000, 100.000] - loss: 1793.752 - mean_q: 0.426\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 10s 1ms/step - reward: -24.9400\n",
      "3242 episodes - episode_reward: -76.928 [-100.000, 100.000] - loss: 2011.526 - mean_q: 0.418\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 10s 1ms/step - reward: -14.3200\n",
      "2464 episodes - episode_reward: -58.117 [-100.000, 100.000] - loss: 988.793 - mean_q: 0.337\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 10s 970us/step - reward: -14.4500\n",
      "2551 episodes - episode_reward: -56.644 [-100.000, 100.000] - loss: 844.959 - mean_q: 0.369\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 10s 1ms/step - reward: -19.8300\n",
      "2979 episodes - episode_reward: -66.566 [-100.000, 100.000] - loss: 1493.711 - mean_q: 0.705\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 10s 990us/step - reward: -15.4200\n",
      "2732 episodes - episode_reward: -56.442 [-100.000, 100.000] - loss: 862.652 - mean_q: 0.894\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 10s 997us/step - reward: -20.4700\n",
      "3011 episodes - episode_reward: -67.984 [-100.000, 100.000] - loss: 1551.325 - mean_q: 0.832\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 10s 990us/step - reward: -16.1300\n",
      "2777 episodes - episode_reward: -58.084 [-100.000, 100.000] - loss: 913.011 - mean_q: 0.952\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 10s 993us/step - reward: -19.7500\n",
      "2971 episodes - episode_reward: -66.476 [-100.000, 100.000] - loss: 1364.749 - mean_q: 0.577\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 9s 929us/step - reward: -7.1400\n",
      "1950 episodes - episode_reward: -36.615 [-100.000, 100.000] - loss: 1026.645 - mean_q: 0.324\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 9s 869us/step - reward: 5.3700\n",
      "1151 episodes - episode_reward: 46.655 [-100.000, 100.000] - loss: 436.179 - mean_q: 0.555\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 9s 879us/step - reward: 5.8100\n",
      "1125 episodes - episode_reward: 51.644 [-100.000, 100.000] - loss: 476.367 - mean_q: 0.916\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 9s 878us/step - reward: 5.3800\n",
      "1152 episodes - episode_reward: 46.701 [-100.000, 100.000] - loss: 437.513 - mean_q: 0.914\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 9s 869us/step - reward: 5.4300\n",
      "1159 episodes - episode_reward: 46.851 [-100.000, 100.000] - loss: 858.570 - mean_q: 0.966\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 9s 871us/step - reward: 5.6800\n",
      "1148 episodes - episode_reward: 49.477 [-100.000, 100.000] - loss: 520.144 - mean_q: 0.992\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 9s 877us/step - reward: 5.3400\n",
      "1158 episodes - episode_reward: 46.114 [-100.000, 100.000] - loss: 754.527 - mean_q: 0.996\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 9s 857us/step - reward: 5.4600\n",
      "1140 episodes - episode_reward: 47.895 [-100.000, 100.000] - loss: 819.715 - mean_q: 0.995\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 9s 881us/step - reward: 5.5100\n",
      "1133 episodes - episode_reward: 48.632 [-100.000, 100.000] - loss: 927.593 - mean_q: 0.996\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 9s 877us/step - reward: 5.1900\n",
      "1155 episodes - episode_reward: 44.935 [-100.000, 100.000] - loss: 902.437 - mean_q: 0.996\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      "10000/10000 [==============================] - 9s 870us/step - reward: 5.8700\n",
      "1143 episodes - episode_reward: 51.356 [-100.000, 100.000] - loss: 264.967 - mean_q: 0.997\n",
      "\n",
      "Interval 22 (210000 steps performed)\n",
      "10000/10000 [==============================] - 9s 874us/step - reward: 5.4000\n",
      "1160 episodes - episode_reward: 46.552 [-100.000, 100.000] - loss: 290.679 - mean_q: 0.997\n",
      "\n",
      "Interval 23 (220000 steps performed)\n",
      "10000/10000 [==============================] - 9s 871us/step - reward: 5.8800\n",
      "1136 episodes - episode_reward: 51.761 [-100.000, 100.000] - loss: 750.127 - mean_q: 0.997\n",
      "\n",
      "Interval 24 (230000 steps performed)\n",
      "10000/10000 [==============================] - 9s 923us/step - reward: 5.5800\n",
      "1148 episodes - episode_reward: 48.606 [-100.000, 100.000] - loss: 437.892 - mean_q: 0.998\n",
      "\n",
      "Interval 25 (240000 steps performed)\n",
      "10000/10000 [==============================] - 9s 903us/step - reward: 5.6000\n",
      "1152 episodes - episode_reward: 48.611 [-100.000, 100.000] - loss: 907.652 - mean_q: 0.998\n",
      "\n",
      "Interval 26 (250000 steps performed)\n",
      "10000/10000 [==============================] - 9s 863us/step - reward: 5.4700\n",
      "1155 episodes - episode_reward: 47.359 [-100.000, 100.000] - loss: 663.550 - mean_q: 0.999\n",
      "\n",
      "Interval 27 (260000 steps performed)\n",
      "10000/10000 [==============================] - 8s 847us/step - reward: 5.6400\n",
      "1132 episodes - episode_reward: 49.823 [-100.000, 100.000] - loss: 602.306 - mean_q: 0.999\n",
      "\n",
      "Interval 28 (270000 steps performed)\n",
      "10000/10000 [==============================] - 9s 855us/step - reward: 5.1500\n",
      "1163 episodes - episode_reward: 44.282 [-100.000, 100.000] - loss: 657.916 - mean_q: 0.999\n",
      "\n",
      "Interval 29 (280000 steps performed)\n",
      "10000/10000 [==============================] - 9s 859us/step - reward: 5.6400\n",
      "1142 episodes - episode_reward: 49.387 [-100.000, 100.000] - loss: 536.242 - mean_q: 0.999\n",
      "\n",
      "Interval 30 (290000 steps performed)\n",
      "10000/10000 [==============================] - 8s 848us/step - reward: 5.7000\n",
      "1142 episodes - episode_reward: 49.912 [-100.000, 100.000] - loss: 942.856 - mean_q: 0.999\n",
      "\n",
      "Interval 31 (300000 steps performed)\n",
      "10000/10000 [==============================] - 8s 842us/step - reward: 5.3700\n",
      "1151 episodes - episode_reward: 46.655 [-100.000, 100.000] - loss: 379.204 - mean_q: 0.999\n",
      "\n",
      "Interval 32 (310000 steps performed)\n",
      "10000/10000 [==============================] - 8s 825us/step - reward: 5.5500\n",
      "1147 episodes - episode_reward: 48.387 [-100.000, 100.000] - loss: 675.128 - mean_q: 1.000\n",
      "\n",
      "Interval 33 (320000 steps performed)\n",
      "10000/10000 [==============================] - 8s 831us/step - reward: 5.8400\n",
      "1144 episodes - episode_reward: 51.049 [-100.000, 100.000] - loss: 892.286 - mean_q: 1.000\n",
      "\n",
      "Interval 34 (330000 steps performed)\n",
      "10000/10000 [==============================] - 8s 824us/step - reward: 5.5800\n",
      "1150 episodes - episode_reward: 48.522 [-100.000, 100.000] - loss: 603.439 - mean_q: 1.000\n",
      "\n",
      "Interval 35 (340000 steps performed)\n",
      "10000/10000 [==============================] - 8s 834us/step - reward: 5.3600\n",
      "1154 episodes - episode_reward: 46.447 [-100.000, 100.000] - loss: 696.838 - mean_q: 1.000\n",
      "\n",
      "Interval 36 (350000 steps performed)\n",
      "10000/10000 [==============================] - 8s 836us/step - reward: 5.5500\n",
      "1143 episodes - episode_reward: 48.556 [-100.000, 100.000] - loss: 705.626 - mean_q: 1.000\n",
      "\n",
      "Interval 37 (360000 steps performed)\n",
      "10000/10000 [==============================] - 9s 911us/step - reward: 4.8800\n",
      "1158 episodes - episode_reward: 42.142 [-100.000, 100.000] - loss: 365.364 - mean_q: 1.000\n",
      "\n",
      "Interval 38 (370000 steps performed)\n",
      "10000/10000 [==============================] - 9s 901us/step - reward: 5.6700\n",
      "1137 episodes - episode_reward: 49.868 [-100.000, 100.000] - loss: 651.315 - mean_q: 1.000\n",
      "\n",
      "Interval 39 (380000 steps performed)\n",
      "10000/10000 [==============================] - 11s 1ms/step - reward: 5.1600\n",
      "1168 episodes - episode_reward: 44.178 [-100.000, 100.000] - loss: 571.383 - mean_q: 1.000\n",
      "\n",
      "Interval 40 (390000 steps performed)\n",
      "10000/10000 [==============================] - 8s 770us/step - reward: 5.2200\n",
      "done, took 363.513 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd938d0fb80>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sars.fit(env, nb_steps=500, visualize=False, verbose=1)\n",
    "sars.fit(env, nb_steps=400000, visualize=False, verbose=1)\n",
    "# After training is done, we save the best weights.\n",
    "#sars.save_weights(f'sars_{ENV_NAME}_params.h5f', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 100.000, steps: 10\n",
      "Episode 2: reward: 100.000, steps: 10\n",
      "Episode 3: reward: 100.000, steps: 10\n",
      "Episode 4: reward: 100.000, steps: 10\n",
      "Episode 5: reward: 100.000, steps: 10\n",
      "Episode 6: reward: -100.000, steps: 6\n",
      "Episode 7: reward: 100.000, steps: 10\n",
      "Episode 8: reward: -100.000, steps: 3\n",
      "Episode 9: reward: 100.000, steps: 10\n",
      "Episode 10: reward: 100.000, steps: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd937eafdf0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sars.test(env, nb_episodes=10, visualize=True)\n",
    "# 8/25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory = EpisodeParameterMemory(limit=2000, window_length=1)\n",
    "# cem = CEMAgent(model=model, nb_actions=nb_actions, memory=memory,\n",
    "#                batch_size=50, nb_steps_warmup=500, train_interval=50, elite_frac=0.05)\n",
    "# cem.compile()\n",
    "\n",
    "# cem.fit(env, nb_steps=10000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cem.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TclError",
     "evalue": "can't invoke \"destroy\" command: application has been destroyed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTclError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-775de56d6ae9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/sc2/rlplay/gym-ent/gym_ent/envs/ent_env.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/tkinter/__init__.py\u001b[0m in \u001b[0;36mdestroy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2301\u001b[0m         end the application of this Tcl interpreter.\"\"\"\n\u001b[1;32m   2302\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2303\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'destroy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2304\u001b[0m         \u001b[0mMisc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2305\u001b[0m         \u001b[0;32mglobal\u001b[0m \u001b[0m_default_root\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTclError\u001b[0m: can't invoke \"destroy\" command: application has been destroyed"
     ]
    }
   ],
   "source": [
    "env.render(close=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
