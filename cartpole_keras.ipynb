{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.cem import CEMAgent\n",
    "from rl.agents.sarsa import SARSAAgent\n",
    "from rl.memory import EpisodeParameterMemory\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import LinearAnnealedPolicy\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 34        \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 658\n",
      "Trainable params: 658\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "ENV_NAME = 'CartPole-v0'\n",
    "\n",
    "# Get the environment and extract the number of actions.\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "\n",
    "nb_actions = env.action_space.n\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "\n",
    "# Option 1 : Simple model\n",
    "# model = Sequential()\n",
    "# model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "# model.add(Dense(nb_actions))\n",
    "# model.add(Activation('softmax'))\n",
    "\n",
    "# Option 2: deep network\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SARSAAgent\n",
    "#sarsa\n",
    "#__init__(self, model, nb_actions, policy=None, test_policy=None, gamma=.99, nb_steps_warmup=10, train_interval=1, delta_clip=np.inf, *args, **kwargs)\n",
    "#cem\n",
    "#__init__(self, model, nb_actions, memory, batch_size=50, nb_steps_warmup=1000, train_interval=50, elite_frac=0.05, memory_interval=1, theta_init=None, noise_decay_const=0.0, noise_ampl=0.0, **kwargs)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sars = SARSAAgent(model=model, nb_actions=nb_actions, nb_steps_warmup=1000, train_interval=50)\n",
    "#sars.compile(optimizer='adam')\n",
    "\n",
    "memory = EpisodeParameterMemory(limit=2000, window_length=1)\n",
    "cem = CEMAgent(model=model, nb_actions=nb_actions, memory=memory,\n",
    "               batch_size=50, nb_steps_warmup=2000, train_interval=50, elite_frac=0.05)\n",
    "cem.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 2000000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "  193/10000 [..............................] - ETA: 7s - reward: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lkoziol/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 8s 812us/step - reward: 1.0000\n",
      "583 episodes - episode_reward: 17.122 [8.000, 107.000] - mean_best_reward: 54.556\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 8s 772us/step - reward: 1.0000\n",
      "566 episodes - episode_reward: 17.686 [8.000, 100.000] - mean_best_reward: 51.409\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 8s 793us/step - reward: 1.0000\n",
      "542 episodes - episode_reward: 18.445 [8.000, 109.000] - mean_best_reward: 46.364\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 8s 760us/step - reward: 1.0000\n",
      "526 episodes - episode_reward: 19.025 [8.000, 119.000] - mean_best_reward: 62.773\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 8s 753us/step - reward: 1.0000\n",
      "463 episodes - episode_reward: 21.575 [8.000, 179.000] - mean_best_reward: 59.000\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 8s 773us/step - reward: 1.0000\n",
      "484 episodes - episode_reward: 20.669 [8.000, 187.000] - mean_best_reward: 77.650\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 7s 745us/step - reward: 1.0000\n",
      "363 episodes - episode_reward: 27.504 [8.000, 131.000] - mean_best_reward: 79.929\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 7s 744us/step - reward: 1.0000\n",
      "358 episodes - episode_reward: 27.919 [8.000, 152.000] - mean_best_reward: 73.786\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 7s 650us/step - reward: 1.0000\n",
      "334 episodes - episode_reward: 29.982 [8.000, 200.000] - mean_best_reward: 81.143\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 7s 680us/step - reward: 1.0000\n",
      "219 episodes - episode_reward: 45.676 [10.000, 200.000] - mean_best_reward: 97.375\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 6s 631us/step - reward: 1.0000\n",
      "166 episodes - episode_reward: 60.193 [9.000, 200.000] - mean_best_reward: 141.125\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 7s 700us/step - reward: 1.0000\n",
      "176 episodes - episode_reward: 56.011 [9.000, 200.000] - mean_best_reward: 169.333\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 6s 619us/step - reward: 1.0000\n",
      "205 episodes - episode_reward: 49.522 [8.000, 200.000] - mean_best_reward: 152.125\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 7s 680us/step - reward: 1.0000\n",
      "174 episodes - episode_reward: 57.253 [9.000, 200.000] - mean_best_reward: 131.750\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 6s 616us/step - reward: 1.0000\n",
      "174 episodes - episode_reward: 56.724 [10.000, 200.000] - mean_best_reward: 157.167\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 7s 717us/step - reward: 1.0000\n",
      "157 episodes - episode_reward: 63.796 [9.000, 200.000] - mean_best_reward: 162.833\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 7s 714us/step - reward: 1.0000\n",
      "164 episodes - episode_reward: 61.671 [9.000, 200.000] - mean_best_reward: 168.875\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 7s 695us/step - reward: 1.0000\n",
      "165 episodes - episode_reward: 60.533 [9.000, 200.000] - mean_best_reward: 140.000\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 7s 717us/step - reward: 1.0000\n",
      "141 episodes - episode_reward: 71.057 [11.000, 200.000] - mean_best_reward: 156.167\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 7s 705us/step - reward: 1.0000\n",
      "156 episodes - episode_reward: 63.308 [11.000, 200.000] - mean_best_reward: 182.000\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      "10000/10000 [==============================] - 7s 715us/step - reward: 1.0000\n",
      "144 episodes - episode_reward: 70.319 [12.000, 200.000] - mean_best_reward: 174.333\n",
      "\n",
      "Interval 22 (210000 steps performed)\n",
      "10000/10000 [==============================] - 6s 645us/step - reward: 1.0000\n",
      "145 episodes - episode_reward: 68.738 [9.000, 200.000] - mean_best_reward: 160.833\n",
      "\n",
      "Interval 23 (220000 steps performed)\n",
      "10000/10000 [==============================] - 6s 635us/step - reward: 1.0000\n",
      "146 episodes - episode_reward: 68.979 [9.000, 200.000] - mean_best_reward: 191.833\n",
      "\n",
      "Interval 24 (230000 steps performed)\n",
      "10000/10000 [==============================] - 6s 640us/step - reward: 1.0000\n",
      "130 episodes - episode_reward: 76.554 [12.000, 200.000] - mean_best_reward: 184.250\n",
      "\n",
      "Interval 25 (240000 steps performed)\n",
      "10000/10000 [==============================] - 7s 665us/step - reward: 1.0000\n",
      "121 episodes - episode_reward: 82.818 [11.000, 200.000] - mean_best_reward: 169.500\n",
      "\n",
      "Interval 26 (250000 steps performed)\n",
      "10000/10000 [==============================] - 7s 709us/step - reward: 1.0000\n",
      "138 episodes - episode_reward: 72.529 [11.000, 200.000] - mean_best_reward: 174.250\n",
      "\n",
      "Interval 27 (260000 steps performed)\n",
      "10000/10000 [==============================] - 7s 672us/step - reward: 1.0000\n",
      "159 episodes - episode_reward: 62.887 [9.000, 200.000] - mean_best_reward: 190.833\n",
      "\n",
      "Interval 28 (270000 steps performed)\n",
      "10000/10000 [==============================] - 6s 593us/step - reward: 1.0000\n",
      "144 episodes - episode_reward: 69.562 [10.000, 200.000] - mean_best_reward: 173.333\n",
      "\n",
      "Interval 29 (280000 steps performed)\n",
      "10000/10000 [==============================] - 7s 685us/step - reward: 1.0000\n",
      "117 episodes - episode_reward: 85.274 [14.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 30 (290000 steps performed)\n",
      "10000/10000 [==============================] - 7s 706us/step - reward: 1.0000\n",
      "120 episodes - episode_reward: 83.233 [12.000, 200.000] - mean_best_reward: 180.000\n",
      "\n",
      "Interval 31 (300000 steps performed)\n",
      "10000/10000 [==============================] - 7s 707us/step - reward: 1.0000\n",
      "130 episodes - episode_reward: 76.877 [12.000, 200.000] - mean_best_reward: 179.833\n",
      "\n",
      "Interval 32 (310000 steps performed)\n",
      "10000/10000 [==============================] - 7s 706us/step - reward: 1.0000\n",
      "120 episodes - episode_reward: 83.625 [12.000, 200.000] - mean_best_reward: 193.250\n",
      "\n",
      "Interval 33 (320000 steps performed)\n",
      "10000/10000 [==============================] - 7s 706us/step - reward: 1.0000\n",
      "135 episodes - episode_reward: 73.830 [9.000, 200.000] - mean_best_reward: 196.000\n",
      "\n",
      "Interval 34 (330000 steps performed)\n",
      "10000/10000 [==============================] - 7s 700us/step - reward: 1.0000\n",
      "118 episodes - episode_reward: 85.008 [14.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 35 (340000 steps performed)\n",
      "10000/10000 [==============================] - 7s 715us/step - reward: 1.0000\n",
      "117 episodes - episode_reward: 84.778 [11.000, 200.000] - mean_best_reward: 182.250\n",
      "\n",
      "Interval 36 (350000 steps performed)\n",
      "10000/10000 [==============================] - 7s 706us/step - reward: 1.0000\n",
      "110 episodes - episode_reward: 91.173 [12.000, 200.000] - mean_best_reward: 193.167\n",
      "\n",
      "Interval 37 (360000 steps performed)\n",
      "10000/10000 [==============================] - 7s 719us/step - reward: 1.0000\n",
      "122 episodes - episode_reward: 82.328 [12.000, 200.000] - mean_best_reward: 180.250\n",
      "\n",
      "Interval 38 (370000 steps performed)\n",
      "10000/10000 [==============================] - 7s 710us/step - reward: 1.0000\n",
      "114 episodes - episode_reward: 86.237 [16.000, 200.000] - mean_best_reward: 198.000\n",
      "\n",
      "Interval 39 (380000 steps performed)\n",
      "10000/10000 [==============================] - 7s 714us/step - reward: 1.0000\n",
      "117 episodes - episode_reward: 87.085 [10.000, 200.000] - mean_best_reward: 196.000\n",
      "\n",
      "Interval 40 (390000 steps performed)\n",
      "10000/10000 [==============================] - 7s 697us/step - reward: 1.0000\n",
      "125 episodes - episode_reward: 79.328 [12.000, 200.000] - mean_best_reward: 186.000\n",
      "\n",
      "Interval 41 (400000 steps performed)\n",
      "10000/10000 [==============================] - 7s 724us/step - reward: 1.0000\n",
      "124 episodes - episode_reward: 80.798 [10.000, 200.000] - mean_best_reward: 193.833\n",
      "\n",
      "Interval 42 (410000 steps performed)\n",
      "10000/10000 [==============================] - 7s 715us/step - reward: 1.0000\n",
      "110 episodes - episode_reward: 90.918 [15.000, 200.000] - mean_best_reward: 196.750\n",
      "\n",
      "Interval 43 (420000 steps performed)\n",
      "10000/10000 [==============================] - 7s 714us/step - reward: 1.0000\n",
      "123 episodes - episode_reward: 81.520 [11.000, 200.000] - mean_best_reward: 191.000\n",
      "\n",
      "Interval 44 (430000 steps performed)\n",
      "10000/10000 [==============================] - 7s 719us/step - reward: 1.0000\n",
      "119 episodes - episode_reward: 84.303 [12.000, 200.000] - mean_best_reward: 197.833\n",
      "\n",
      "Interval 45 (440000 steps performed)\n",
      "10000/10000 [==============================] - 7s 724us/step - reward: 1.0000\n",
      "124 episodes - episode_reward: 80.452 [10.000, 200.000] - mean_best_reward: 189.250\n",
      "\n",
      "Interval 46 (450000 steps performed)\n",
      "10000/10000 [==============================] - 7s 723us/step - reward: 1.0000\n",
      "124 episodes - episode_reward: 80.774 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 47 (460000 steps performed)\n",
      "10000/10000 [==============================] - 7s 715us/step - reward: 1.0000\n",
      "115 episodes - episode_reward: 86.635 [11.000, 200.000] - mean_best_reward: 184.500\n",
      "\n",
      "Interval 48 (470000 steps performed)\n",
      "10000/10000 [==============================] - 7s 711us/step - reward: 1.0000\n",
      "108 episodes - episode_reward: 92.491 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 49 (480000 steps performed)\n",
      "10000/10000 [==============================] - 7s 713us/step - reward: 1.0000\n",
      "114 episodes - episode_reward: 87.544 [10.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 50 (490000 steps performed)\n",
      "10000/10000 [==============================] - 7s 681us/step - reward: 1.0000\n",
      "105 episodes - episode_reward: 94.514 [12.000, 200.000] - mean_best_reward: 198.833\n",
      "\n",
      "Interval 51 (500000 steps performed)\n",
      "10000/10000 [==============================] - 7s 710us/step - reward: 1.0000\n",
      "115 episodes - episode_reward: 88.148 [13.000, 200.000] - mean_best_reward: 199.750\n",
      "\n",
      "Interval 52 (510000 steps performed)\n",
      "10000/10000 [==============================] - 7s 713us/step - reward: 1.0000\n",
      "123 episodes - episode_reward: 80.634 [10.000, 200.000] - mean_best_reward: 195.500\n",
      "\n",
      "Interval 53 (520000 steps performed)\n",
      "10000/10000 [==============================] - 7s 697us/step - reward: 1.0000\n",
      "107 episodes - episode_reward: 92.925 [15.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 54 (530000 steps performed)\n",
      "10000/10000 [==============================] - 7s 700us/step - reward: 1.0000\n",
      "115 episodes - episode_reward: 87.652 [12.000, 200.000] - mean_best_reward: 194.833\n",
      "\n",
      "Interval 55 (540000 steps performed)\n",
      "10000/10000 [==============================] - 7s 702us/step - reward: 1.0000\n",
      "114 episodes - episode_reward: 88.035 [10.000, 200.000] - mean_best_reward: 195.750\n",
      "\n",
      "Interval 56 (550000 steps performed)\n",
      "10000/10000 [==============================] - 7s 708us/step - reward: 1.0000\n",
      "106 episodes - episode_reward: 94.755 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 57 (560000 steps performed)\n",
      "10000/10000 [==============================] - 7s 706us/step - reward: 1.0000\n",
      "107 episodes - episode_reward: 92.579 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 58 (570000 steps performed)\n",
      "10000/10000 [==============================] - 7s 708us/step - reward: 1.0000\n",
      "117 episodes - episode_reward: 85.786 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 59 (580000 steps performed)\n",
      "10000/10000 [==============================] - 7s 657us/step - reward: 1.0000\n",
      "105 episodes - episode_reward: 95.562 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 60 (590000 steps performed)\n",
      "10000/10000 [==============================] - 7s 698us/step - reward: 1.0000\n",
      "126 episodes - episode_reward: 79.500 [10.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 61 (600000 steps performed)\n",
      "10000/10000 [==============================] - 7s 703us/step - reward: 1.0000\n",
      "98 episodes - episode_reward: 101.602 [11.000, 200.000] - mean_best_reward: 192.000\n",
      "\n",
      "Interval 62 (610000 steps performed)\n",
      "10000/10000 [==============================] - 7s 707us/step - reward: 1.0000\n",
      "115 episodes - episode_reward: 87.200 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 63 (620000 steps performed)\n",
      "10000/10000 [==============================] - 7s 708us/step - reward: 1.0000\n",
      "121 episodes - episode_reward: 82.810 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 64 (630000 steps performed)\n",
      "10000/10000 [==============================] - 7s 707us/step - reward: 1.0000\n",
      "112 episodes - episode_reward: 88.018 [9.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 65 (640000 steps performed)\n",
      "10000/10000 [==============================] - 7s 701us/step - reward: 1.0000\n",
      "108 episodes - episode_reward: 93.778 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 66 (650000 steps performed)\n",
      "10000/10000 [==============================] - 7s 700us/step - reward: 1.0000\n",
      "94 episodes - episode_reward: 105.330 [15.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 67 (660000 steps performed)\n",
      "10000/10000 [==============================] - 7s 709us/step - reward: 1.0000\n",
      "108 episodes - episode_reward: 92.944 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 68 (670000 steps performed)\n",
      "10000/10000 [==============================] - 7s 719us/step - reward: 1.0000\n",
      "112 episodes - episode_reward: 89.750 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 69 (680000 steps performed)\n",
      "10000/10000 [==============================] - 7s 700us/step - reward: 1.0000\n",
      "110 episodes - episode_reward: 90.582 [10.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 70 (690000 steps performed)\n",
      "10000/10000 [==============================] - 7s 709us/step - reward: 1.0000\n",
      "115 episodes - episode_reward: 87.330 [10.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 71 (700000 steps performed)\n",
      "10000/10000 [==============================] - 7s 702us/step - reward: 1.0000\n",
      "111 episodes - episode_reward: 89.126 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 72 (710000 steps performed)\n",
      "10000/10000 [==============================] - 7s 706us/step - reward: 1.0000\n",
      "115 episodes - episode_reward: 87.687 [12.000, 200.000] - mean_best_reward: 198.500\n",
      "\n",
      "Interval 73 (720000 steps performed)\n",
      "10000/10000 [==============================] - 7s 705us/step - reward: 1.0000\n",
      "113 episodes - episode_reward: 87.779 [10.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 74 (730000 steps performed)\n",
      "10000/10000 [==============================] - 7s 709us/step - reward: 1.0000\n",
      "116 episodes - episode_reward: 86.698 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 75 (740000 steps performed)\n",
      "10000/10000 [==============================] - 7s 699us/step - reward: 1.0000\n",
      "108 episodes - episode_reward: 92.954 [16.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 76 (750000 steps performed)\n",
      "10000/10000 [==============================] - 7s 717us/step - reward: 1.0000\n",
      "109 episodes - episode_reward: 91.404 [10.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 77 (760000 steps performed)\n",
      "10000/10000 [==============================] - 7s 701us/step - reward: 1.0000\n",
      "110 episodes - episode_reward: 91.373 [9.000, 200.000] - mean_best_reward: 199.500\n",
      "\n",
      "Interval 78 (770000 steps performed)\n",
      "10000/10000 [==============================] - 7s 709us/step - reward: 1.0000\n",
      "101 episodes - episode_reward: 98.366 [10.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 79 (780000 steps performed)\n",
      "10000/10000 [==============================] - 7s 714us/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.071 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 80 (790000 steps performed)\n",
      "10000/10000 [==============================] - 7s 709us/step - reward: 1.0000\n",
      "100 episodes - episode_reward: 99.480 [15.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 81 (800000 steps performed)\n",
      "10000/10000 [==============================] - 7s 689us/step - reward: 1.0000\n",
      "102 episodes - episode_reward: 97.598 [10.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 82 (810000 steps performed)\n",
      "10000/10000 [==============================] - 7s 712us/step - reward: 1.0000\n",
      "109 episodes - episode_reward: 92.165 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 83 (820000 steps performed)\n",
      "10000/10000 [==============================] - 7s 705us/step - reward: 1.0000\n",
      "102 episodes - episode_reward: 98.892 [15.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 84 (830000 steps performed)\n",
      "10000/10000 [==============================] - 7s 705us/step - reward: 1.0000\n",
      "103 episodes - episode_reward: 97.262 [16.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 85 (840000 steps performed)\n",
      "10000/10000 [==============================] - 7s 694us/step - reward: 1.0000\n",
      "96 episodes - episode_reward: 102.594 [9.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 86 (850000 steps performed)\n",
      "10000/10000 [==============================] - 7s 716us/step - reward: 1.0000\n",
      "102 episodes - episode_reward: 99.353 [10.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 87 (860000 steps performed)\n",
      "10000/10000 [==============================] - 7s 695us/step - reward: 1.0000\n",
      "111 episodes - episode_reward: 89.901 [14.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 88 (870000 steps performed)\n",
      "10000/10000 [==============================] - 7s 705us/step - reward: 1.0000\n",
      "96 episodes - episode_reward: 104.375 [15.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 89 (880000 steps performed)\n",
      "10000/10000 [==============================] - 7s 711us/step - reward: 1.0000\n",
      "101 episodes - episode_reward: 98.980 [14.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 90 (890000 steps performed)\n",
      "10000/10000 [==============================] - 7s 702us/step - reward: 1.0000\n",
      "102 episodes - episode_reward: 97.794 [14.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 91 (900000 steps performed)\n",
      "10000/10000 [==============================] - 7s 716us/step - reward: 1.0000\n",
      "109 episodes - episode_reward: 92.239 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 92 (910000 steps performed)\n",
      "10000/10000 [==============================] - 7s 701us/step - reward: 1.0000\n",
      "91 episodes - episode_reward: 109.132 [13.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 93 (920000 steps performed)\n",
      "10000/10000 [==============================] - 7s 698us/step - reward: 1.0000\n",
      "105 episodes - episode_reward: 94.914 [13.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 94 (930000 steps performed)\n",
      "10000/10000 [==============================] - 7s 699us/step - reward: 1.0000\n",
      "94 episodes - episode_reward: 106.202 [12.000, 200.000] - mean_best_reward: 199.000\n",
      "\n",
      "Interval 95 (940000 steps performed)\n",
      "10000/10000 [==============================] - 7s 705us/step - reward: 1.0000\n",
      "111 episodes - episode_reward: 90.775 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 96 (950000 steps performed)\n",
      "10000/10000 [==============================] - 7s 707us/step - reward: 1.0000\n",
      "100 episodes - episode_reward: 100.220 [15.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 97 (960000 steps performed)\n",
      "10000/10000 [==============================] - 7s 708us/step - reward: 1.0000\n",
      "109 episodes - episode_reward: 91.936 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 98 (970000 steps performed)\n",
      "10000/10000 [==============================] - 7s 705us/step - reward: 1.0000\n",
      "98 episodes - episode_reward: 101.643 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 99 (980000 steps performed)\n",
      "10000/10000 [==============================] - 7s 714us/step - reward: 1.0000\n",
      "116 episodes - episode_reward: 86.491 [16.000, 200.000] - mean_best_reward: 198.500\n",
      "\n",
      "Interval 100 (990000 steps performed)\n",
      "10000/10000 [==============================] - 7s 699us/step - reward: 1.0000\n",
      "96 episodes - episode_reward: 103.677 [15.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 101 (1000000 steps performed)\n",
      "10000/10000 [==============================] - 7s 714us/step - reward: 1.0000\n",
      "106 episodes - episode_reward: 93.132 [13.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 102 (1010000 steps performed)\n",
      "10000/10000 [==============================] - 7s 703us/step - reward: 1.0000\n",
      "103 episodes - episode_reward: 98.417 [9.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 103 (1020000 steps performed)\n",
      "10000/10000 [==============================] - 7s 704us/step - reward: 1.0000\n",
      "89 episodes - episode_reward: 111.742 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 104 (1030000 steps performed)\n",
      "10000/10000 [==============================] - 7s 709us/step - reward: 1.0000\n",
      "100 episodes - episode_reward: 99.380 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 105 (1040000 steps performed)\n",
      "10000/10000 [==============================] - 7s 709us/step - reward: 1.0000\n",
      "102 episodes - episode_reward: 98.775 [14.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 106 (1050000 steps performed)\n",
      "10000/10000 [==============================] - 7s 706us/step - reward: 1.0000\n",
      "98 episodes - episode_reward: 102.939 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 107 (1060000 steps performed)\n",
      "10000/10000 [==============================] - 7s 710us/step - reward: 1.0000\n",
      "102 episodes - episode_reward: 97.422 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 108 (1070000 steps performed)\n",
      "10000/10000 [==============================] - 7s 701us/step - reward: 1.0000\n",
      "97 episodes - episode_reward: 103.536 [10.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 109 (1080000 steps performed)\n",
      "10000/10000 [==============================] - 7s 701us/step - reward: 1.0000\n",
      "97 episodes - episode_reward: 103.010 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 110 (1090000 steps performed)\n",
      "10000/10000 [==============================] - 7s 711us/step - reward: 1.0000\n",
      "97 episodes - episode_reward: 102.722 [9.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 111 (1100000 steps performed)\n",
      "10000/10000 [==============================] - 7s 701us/step - reward: 1.0000\n",
      "95 episodes - episode_reward: 104.916 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 112 (1110000 steps performed)\n",
      "10000/10000 [==============================] - 7s 708us/step - reward: 1.0000\n",
      "105 episodes - episode_reward: 96.143 [14.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 113 (1120000 steps performed)\n",
      "10000/10000 [==============================] - 7s 721us/step - reward: 1.0000\n",
      "112 episodes - episode_reward: 88.554 [14.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 114 (1130000 steps performed)\n",
      "10000/10000 [==============================] - 7s 702us/step - reward: 1.0000\n",
      "100 episodes - episode_reward: 100.540 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 115 (1140000 steps performed)\n",
      "10000/10000 [==============================] - 7s 699us/step - reward: 1.0000\n",
      "90 episodes - episode_reward: 111.300 [11.000, 200.000] - mean_best_reward: 198.250\n",
      "\n",
      "Interval 116 (1150000 steps performed)\n",
      "10000/10000 [==============================] - 7s 698us/step - reward: 1.0000\n",
      "109 episodes - episode_reward: 90.954 [9.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 117 (1160000 steps performed)\n",
      "10000/10000 [==============================] - 7s 709us/step - reward: 1.0000\n",
      "89 episodes - episode_reward: 113.404 [13.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 118 (1170000 steps performed)\n",
      "10000/10000 [==============================] - 7s 707us/step - reward: 1.0000\n",
      "104 episodes - episode_reward: 96.144 [10.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 119 (1180000 steps performed)\n",
      "10000/10000 [==============================] - 7s 711us/step - reward: 1.0000\n",
      "94 episodes - episode_reward: 106.096 [10.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 120 (1190000 steps performed)\n",
      "10000/10000 [==============================] - 7s 705us/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.212 [13.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 121 (1200000 steps performed)\n",
      "10000/10000 [==============================] - 7s 704us/step - reward: 1.0000\n",
      "86 episodes - episode_reward: 115.523 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 122 (1210000 steps performed)\n",
      "10000/10000 [==============================] - 7s 721us/step - reward: 1.0000\n",
      "116 episodes - episode_reward: 86.922 [10.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 123 (1220000 steps performed)\n",
      "10000/10000 [==============================] - 7s 709us/step - reward: 1.0000\n",
      "103 episodes - episode_reward: 96.379 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 124 (1230000 steps performed)\n",
      "10000/10000 [==============================] - 7s 701us/step - reward: 1.0000\n",
      "106 episodes - episode_reward: 94.142 [10.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 125 (1240000 steps performed)\n",
      "10000/10000 [==============================] - 7s 703us/step - reward: 1.0000\n",
      "87 episodes - episode_reward: 115.103 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 126 (1250000 steps performed)\n",
      "10000/10000 [==============================] - 7s 701us/step - reward: 1.0000\n",
      "108 episodes - episode_reward: 92.870 [13.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 127 (1260000 steps performed)\n",
      "10000/10000 [==============================] - 7s 708us/step - reward: 1.0000\n",
      "102 episodes - episode_reward: 98.216 [15.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 128 (1270000 steps performed)\n",
      "10000/10000 [==============================] - 7s 714us/step - reward: 1.0000\n",
      "97 episodes - episode_reward: 103.031 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 129 (1280000 steps performed)\n",
      "10000/10000 [==============================] - 7s 705us/step - reward: 1.0000\n",
      "88 episodes - episode_reward: 113.602 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 130 (1290000 steps performed)\n",
      "10000/10000 [==============================] - 7s 713us/step - reward: 1.0000\n",
      "100 episodes - episode_reward: 99.530 [10.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 131 (1300000 steps performed)\n",
      "10000/10000 [==============================] - 7s 707us/step - reward: 1.0000\n",
      "93 episodes - episode_reward: 106.914 [10.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 132 (1310000 steps performed)\n",
      "10000/10000 [==============================] - 7s 704us/step - reward: 1.0000\n",
      "102 episodes - episode_reward: 98.343 [16.000, 200.000] - mean_best_reward: 199.500\n",
      "\n",
      "Interval 133 (1320000 steps performed)\n",
      "10000/10000 [==============================] - 7s 709us/step - reward: 1.0000\n",
      "84 episodes - episode_reward: 120.286 [18.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 134 (1330000 steps performed)\n",
      "10000/10000 [==============================] - 7s 707us/step - reward: 1.0000\n",
      "107 episodes - episode_reward: 93.280 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 135 (1340000 steps performed)\n",
      "10000/10000 [==============================] - 7s 701us/step - reward: 1.0000\n",
      "101 episodes - episode_reward: 99.099 [14.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 136 (1350000 steps performed)\n",
      "10000/10000 [==============================] - 7s 700us/step - reward: 1.0000\n",
      "92 episodes - episode_reward: 108.163 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 137 (1360000 steps performed)\n",
      "10000/10000 [==============================] - 7s 710us/step - reward: 1.0000\n",
      "102 episodes - episode_reward: 97.755 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 138 (1370000 steps performed)\n",
      "10000/10000 [==============================] - 7s 703us/step - reward: 1.0000\n",
      "96 episodes - episode_reward: 105.156 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 139 (1380000 steps performed)\n",
      "10000/10000 [==============================] - 7s 697us/step - reward: 1.0000\n",
      "96 episodes - episode_reward: 103.823 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 140 (1390000 steps performed)\n",
      "10000/10000 [==============================] - 7s 700us/step - reward: 1.0000\n",
      "93 episodes - episode_reward: 107.645 [15.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 141 (1400000 steps performed)\n",
      "10000/10000 [==============================] - 7s 697us/step - reward: 1.0000\n",
      "96 episodes - episode_reward: 103.896 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 142 (1410000 steps performed)\n",
      "10000/10000 [==============================] - 7s 699us/step - reward: 1.0000\n",
      "102 episodes - episode_reward: 98.373 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 143 (1420000 steps performed)\n",
      "10000/10000 [==============================] - 7s 709us/step - reward: 1.0000\n",
      "101 episodes - episode_reward: 97.792 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 144 (1430000 steps performed)\n",
      "10000/10000 [==============================] - 7s 713us/step - reward: 1.0000\n",
      "101 episodes - episode_reward: 100.119 [9.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 145 (1440000 steps performed)\n",
      "10000/10000 [==============================] - 7s 706us/step - reward: 1.0000\n",
      "92 episodes - episode_reward: 108.739 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 146 (1450000 steps performed)\n",
      "10000/10000 [==============================] - 7s 705us/step - reward: 1.0000\n",
      "91 episodes - episode_reward: 108.868 [14.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 147 (1460000 steps performed)\n",
      "10000/10000 [==============================] - 7s 702us/step - reward: 1.0000\n",
      "96 episodes - episode_reward: 105.240 [10.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 148 (1470000 steps performed)\n",
      "10000/10000 [==============================] - 7s 699us/step - reward: 1.0000\n",
      "105 episodes - episode_reward: 95.305 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 149 (1480000 steps performed)\n",
      "10000/10000 [==============================] - 7s 704us/step - reward: 1.0000\n",
      "89 episodes - episode_reward: 111.303 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 150 (1490000 steps performed)\n",
      "10000/10000 [==============================] - 7s 705us/step - reward: 1.0000\n",
      "91 episodes - episode_reward: 110.341 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 151 (1500000 steps performed)\n",
      "10000/10000 [==============================] - 7s 706us/step - reward: 1.0000\n",
      "97 episodes - episode_reward: 102.907 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 152 (1510000 steps performed)\n",
      "10000/10000 [==============================] - 7s 714us/step - reward: 1.0000\n",
      "105 episodes - episode_reward: 96.019 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 153 (1520000 steps performed)\n",
      "10000/10000 [==============================] - 7s 710us/step - reward: 1.0000\n",
      "93 episodes - episode_reward: 106.387 [10.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 154 (1530000 steps performed)\n",
      "10000/10000 [==============================] - 7s 710us/step - reward: 1.0000\n",
      "101 episodes - episode_reward: 99.891 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 155 (1540000 steps performed)\n",
      "10000/10000 [==============================] - 7s 706us/step - reward: 1.0000\n",
      "101 episodes - episode_reward: 99.069 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 156 (1550000 steps performed)\n",
      "10000/10000 [==============================] - 7s 698us/step - reward: 1.0000\n",
      "96 episodes - episode_reward: 103.917 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 157 (1560000 steps performed)\n",
      "10000/10000 [==============================] - 7s 694us/step - reward: 1.0000\n",
      "97 episodes - episode_reward: 102.577 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 158 (1570000 steps performed)\n",
      "10000/10000 [==============================] - 7s 691us/step - reward: 1.0000\n",
      "87 episodes - episode_reward: 115.276 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 159 (1580000 steps performed)\n",
      "10000/10000 [==============================] - 7s 694us/step - reward: 1.0000\n",
      "93 episodes - episode_reward: 107.742 [15.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 160 (1590000 steps performed)\n",
      "10000/10000 [==============================] - 7s 705us/step - reward: 1.0000\n",
      "89 episodes - episode_reward: 112.180 [13.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 161 (1600000 steps performed)\n",
      "10000/10000 [==============================] - 7s 702us/step - reward: 1.0000\n",
      "91 episodes - episode_reward: 109.044 [17.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 162 (1610000 steps performed)\n",
      "10000/10000 [==============================] - 7s 711us/step - reward: 1.0000\n",
      "109 episodes - episode_reward: 91.963 [16.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 163 (1620000 steps performed)\n",
      "10000/10000 [==============================] - 7s 695us/step - reward: 1.0000\n",
      "91 episodes - episode_reward: 110.637 [19.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 164 (1630000 steps performed)\n",
      "10000/10000 [==============================] - 7s 708us/step - reward: 1.0000\n",
      "93 episodes - episode_reward: 106.763 [16.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 165 (1640000 steps performed)\n",
      "10000/10000 [==============================] - 7s 700us/step - reward: 1.0000\n",
      "98 episodes - episode_reward: 103.102 [13.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 166 (1650000 steps performed)\n",
      "10000/10000 [==============================] - 7s 706us/step - reward: 1.0000\n",
      "92 episodes - episode_reward: 108.207 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 167 (1660000 steps performed)\n",
      "10000/10000 [==============================] - 7s 705us/step - reward: 1.0000\n",
      "96 episodes - episode_reward: 102.865 [15.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 168 (1670000 steps performed)\n",
      "10000/10000 [==============================] - 7s 661us/step - reward: 1.0000\n",
      "89 episodes - episode_reward: 113.539 [14.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 169 (1680000 steps performed)\n",
      "10000/10000 [==============================] - 7s 703us/step - reward: 1.0000\n",
      "88 episodes - episode_reward: 113.273 [10.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 170 (1690000 steps performed)\n",
      "10000/10000 [==============================] - 7s 678us/step - reward: 1.0000\n",
      "101 episodes - episode_reward: 98.297 [13.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 171 (1700000 steps performed)\n",
      "10000/10000 [==============================] - 7s 700us/step - reward: 1.0000\n",
      "90 episodes - episode_reward: 112.322 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 172 (1710000 steps performed)\n",
      "10000/10000 [==============================] - 7s 699us/step - reward: 1.0000\n",
      "90 episodes - episode_reward: 111.567 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 173 (1720000 steps performed)\n",
      "10000/10000 [==============================] - 7s 700us/step - reward: 1.0000\n",
      "85 episodes - episode_reward: 116.435 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 174 (1730000 steps performed)\n",
      "10000/10000 [==============================] - 7s 708us/step - reward: 1.0000\n",
      "96 episodes - episode_reward: 105.448 [15.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 175 (1740000 steps performed)\n",
      "10000/10000 [==============================] - 7s 699us/step - reward: 1.0000\n",
      "102 episodes - episode_reward: 97.814 [9.000, 200.000] - mean_best_reward: 197.250\n",
      "\n",
      "Interval 176 (1750000 steps performed)\n",
      "10000/10000 [==============================] - 7s 704us/step - reward: 1.0000\n",
      "91 episodes - episode_reward: 109.143 [15.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 177 (1760000 steps performed)\n",
      "10000/10000 [==============================] - 7s 702us/step - reward: 1.0000\n",
      "98 episodes - episode_reward: 102.337 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 178 (1770000 steps performed)\n",
      "10000/10000 [==============================] - 7s 693us/step - reward: 1.0000\n",
      "90 episodes - episode_reward: 110.056 [17.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 179 (1780000 steps performed)\n",
      "10000/10000 [==============================] - 7s 706us/step - reward: 1.0000\n",
      "93 episodes - episode_reward: 109.032 [13.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 180 (1790000 steps performed)\n",
      "10000/10000 [==============================] - 7s 703us/step - reward: 1.0000\n",
      "90 episodes - episode_reward: 109.900 [9.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 181 (1800000 steps performed)\n",
      "10000/10000 [==============================] - 7s 708us/step - reward: 1.0000\n",
      "95 episodes - episode_reward: 105.084 [16.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 182 (1810000 steps performed)\n",
      "10000/10000 [==============================] - 6s 601us/step - reward: 1.0000\n",
      "97 episodes - episode_reward: 104.082 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 183 (1820000 steps performed)\n",
      "10000/10000 [==============================] - 7s 673us/step - reward: 1.0000\n",
      "82 episodes - episode_reward: 122.098 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 184 (1830000 steps performed)\n",
      "10000/10000 [==============================] - 7s 658us/step - reward: 1.0000\n",
      "87 episodes - episode_reward: 113.552 [16.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 185 (1840000 steps performed)\n",
      "10000/10000 [==============================] - 6s 603us/step - reward: 1.0000\n",
      "85 episodes - episode_reward: 118.529 [14.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 186 (1850000 steps performed)\n",
      "10000/10000 [==============================] - 7s 681us/step - reward: 1.0000\n",
      "97 episodes - episode_reward: 103.959 [14.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 187 (1860000 steps performed)\n",
      "10000/10000 [==============================] - 7s 699us/step - reward: 1.0000\n",
      "89 episodes - episode_reward: 111.966 [10.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 188 (1870000 steps performed)\n",
      "10000/10000 [==============================] - 7s 659us/step - reward: 1.0000\n",
      "90 episodes - episode_reward: 110.678 [16.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 189 (1880000 steps performed)\n",
      "10000/10000 [==============================] - 7s 698us/step - reward: 1.0000\n",
      "93 episodes - episode_reward: 108.032 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 190 (1890000 steps performed)\n",
      "10000/10000 [==============================] - 7s 689us/step - reward: 1.0000\n",
      "95 episodes - episode_reward: 105.221 [9.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 191 (1900000 steps performed)\n",
      "10000/10000 [==============================] - 7s 694us/step - reward: 1.0000\n",
      "102 episodes - episode_reward: 97.294 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 192 (1910000 steps performed)\n",
      "10000/10000 [==============================] - 7s 700us/step - reward: 1.0000\n",
      "84 episodes - episode_reward: 120.238 [18.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 193 (1920000 steps performed)\n",
      "10000/10000 [==============================] - 7s 700us/step - reward: 1.0000\n",
      "94 episodes - episode_reward: 105.574 [9.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 194 (1930000 steps performed)\n",
      "10000/10000 [==============================] - 7s 703us/step - reward: 1.0000\n",
      "97 episodes - episode_reward: 103.361 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 195 (1940000 steps performed)\n",
      "10000/10000 [==============================] - 7s 702us/step - reward: 1.0000\n",
      "94 episodes - episode_reward: 105.213 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 196 (1950000 steps performed)\n",
      "10000/10000 [==============================] - 7s 696us/step - reward: 1.0000\n",
      "100 episodes - episode_reward: 100.080 [10.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 197 (1960000 steps performed)\n",
      "10000/10000 [==============================] - 7s 698us/step - reward: 1.0000\n",
      "88 episodes - episode_reward: 115.170 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 198 (1970000 steps performed)\n",
      "10000/10000 [==============================] - 7s 704us/step - reward: 1.0000\n",
      "88 episodes - episode_reward: 113.068 [14.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 199 (1980000 steps performed)\n",
      "10000/10000 [==============================] - 7s 700us/step - reward: 1.0000\n",
      "102 episodes - episode_reward: 98.627 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 200 (1990000 steps performed)\n",
      "10000/10000 [==============================] - 7s 704us/step - reward: 1.0000\n",
      "done, took 1407.200 seconds\n"
     ]
    }
   ],
   "source": [
    "cem.fit(env, nb_steps=2000000, visualize=False, verbose=1)\n",
    "\n",
    "# After training is done, we save the best weights.\n",
    "cem.save_weights(f'cem_{ENV_NAME}_l3k_w10_ba50_st2m_params.h5f', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 200.000, steps: 200\n",
      "Episode 2: reward: 200.000, steps: 200\n",
      "Episode 3: reward: 200.000, steps: 200\n",
      "Episode 4: reward: 174.000, steps: 174\n",
      "Episode 5: reward: 200.000, steps: 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8ff3efc370>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cem.test(env, nb_episodes=5, visualize=True)\n",
    "# l2k_w10_ba50_st2m = 190"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
