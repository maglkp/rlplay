{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.cem import CEMAgent\n",
    "from rl.agents.sarsa import SARSAAgent\n",
    "from rl.memory import EpisodeParameterMemory\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import LinearAnnealedPolicy\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(2, 2)\n",
      "Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('fixed_treasure:fixed_treasure-v1')\n",
    "\n",
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "\n",
    "nb_actions = env.action_space.n\n",
    "obs_dim = env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "#model.add(Activation('relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "#model.add(Activation('relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "#model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#sars = SARSAAgent(model=model, nb_actions=nb_actions, nb_steps_warmup=200, train_interval=50)\n",
    "sars = SARSAAgent(model=model, nb_actions=nb_actions, nb_steps_warmup=0, train_interval=1)\n",
    "sars.compile(optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 20\n",
      "Trainable params: 20\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Option 1 : Simple model\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('softmax'))\n",
    "print(model.summary())\n",
    "\n",
    "sars = SARSAAgent(model=model, nb_actions=nb_actions, nb_steps_warmup=1, train_interval=1)\n",
    "sars.compile(optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "    1/10000 [..............................] - ETA: 14:53 - reward: 1.2000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lkoziol/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 41s 4ms/step - reward: 0.2576\n",
      "1068 episodes - episode_reward: 2.407 [-1.000, 21.100] - loss: 0.604 - mean_q: 0.726 - reward: 0.258\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: 0.1141\n",
      "1026 episodes - episode_reward: 1.110 [-1.000, 20.900] - loss: 0.611 - mean_q: 0.781 - reward: 0.114\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 40s 4ms/step - reward: 1.7268\n",
      "1444 episodes - episode_reward: 11.962 [-1.000, 21.200] - loss: 0.814 - mean_q: 0.899 - reward: 1.727\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: 0.3134\n",
      "1086 episodes - episode_reward: 2.880 [-1.000, 21.100] - loss: 0.679 - mean_q: 0.778 - reward: 0.313\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      " 3756/10000 [==========>...................] - ETA: 25s - reward: 0.0805"
     ]
    }
   ],
   "source": [
    "sars.fit(env, nb_steps=50000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 0.700, steps: 10\n",
      "Episode 2: reward: 0.700, steps: 10\n",
      "Episode 3: reward: 0.700, steps: 10\n",
      "Episode 4: reward: 0.700, steps: 10\n",
      "Episode 5: reward: 0.700, steps: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8438159970>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sars.test(env, nb_episodes=5, visualize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
