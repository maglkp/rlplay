{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lkoziol/anaconda3/lib/python3.8/site-packages/setuptools/distutils_patch.py:25: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.cem import CEMAgent\n",
    "from rl.agents.sarsa import SARSAAgent\n",
    "from rl.memory import EpisodeParameterMemory\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import LinearAnnealedPolicy\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
    "\n",
    "from stable_baselines3 import A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(2, 2)\n",
      "Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('fixed_treasure:fixed_treasure-v1')\n",
    "\n",
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "\n",
    "nb_actions = env.action_space.n\n",
    "obs_dim = env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "#model.add(Activation('relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "#model.add(Activation('relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "#model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#sars = SARSAAgent(model=model, nb_actions=nb_actions, nb_steps_warmup=200, train_interval=50)\n",
    "sars = SARSAAgent(model=model, nb_actions=nb_actions, nb_steps_warmup=0, train_interval=1)\n",
    "sars.compile(optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = A2C('MlpPolicy', 'fixed_treasure:fixed_treasure-v1', n_steps=1).learn(100000)\n",
    "#model.save(\"a2c_fixed_treasure_500k_plusminus100_steps3_no_distance\")\n",
    "model.save(\"a2c_fixed_treasure_dist1_100k_plusminus100_005step_n_steps1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = A2C.load(\"a2c_fixed_treasure_dist1_100k_plusminus100_005step_n_steps1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "start\n",
      "convict=[9 7] loot=[8 8] reward:1.4000000000000001\n",
      "s:None r:1.4000000000000001 o:[[9 7]\n",
      " [8 8]]\n",
      "convict=[10  7] loot=[8 8] reward:1.35\n",
      "s:None r:1.35 o:[[10  7]\n",
      " [ 8  8]]\n",
      "convict=[11  7] loot=[8 8] reward:1.3000000000000003\n",
      "s:None r:1.3000000000000003 o:[[11  7]\n",
      " [ 8  8]]\n",
      "convict=[12  7] loot=[8 8] reward:1.25\n",
      "s:None r:1.25 o:[[12  7]\n",
      " [ 8  8]]\n",
      "convict=[13  7] loot=[8 8] reward:1.2000000000000002\n",
      "s:None r:1.2000000000000002 o:[[13  7]\n",
      " [ 8  8]]\n",
      "convict=[14  7] loot=[8 8] reward:1.1500000000000001\n",
      "s:None r:1.1500000000000001 o:[[14  7]\n",
      " [ 8  8]]\n",
      "convict=[15  7] loot=[8 8] reward:1.1\n",
      "s:None r:1.1 o:[[15  7]\n",
      " [ 8  8]]\n",
      "convict=[15  7] loot=[8 8] reward:1.1\n",
      "s:None r:1.1 o:[[15  7]\n",
      " [ 8  8]]\n",
      "convict=[15  7] loot=[8 8] reward:1.1\n",
      "s:None r:1.1 o:[[15  7]\n",
      " [ 8  8]]\n",
      "convict=[15  7] loot=[8 8] reward:-100\n",
      "s:None r:-100 o:[[15  7]\n",
      " [ 8  8]]\n",
      "\n",
      "start\n",
      "convict=[9 7] loot=[8 8] reward:1.4000000000000001\n",
      "s:None r:1.4000000000000001 o:[[9 7]\n",
      " [8 8]]\n",
      "convict=[10  7] loot=[8 8] reward:1.35\n",
      "s:None r:1.35 o:[[10  7]\n",
      " [ 8  8]]\n",
      "convict=[11  7] loot=[8 8] reward:1.3000000000000003\n",
      "s:None r:1.3000000000000003 o:[[11  7]\n",
      " [ 8  8]]\n",
      "convict=[12  7] loot=[8 8] reward:1.25\n",
      "s:None r:1.25 o:[[12  7]\n",
      " [ 8  8]]\n",
      "convict=[13  7] loot=[8 8] reward:1.2000000000000002\n",
      "s:None r:1.2000000000000002 o:[[13  7]\n",
      " [ 8  8]]\n",
      "convict=[14  7] loot=[8 8] reward:1.1500000000000001\n",
      "s:None r:1.1500000000000001 o:[[14  7]\n",
      " [ 8  8]]\n",
      "convict=[15  7] loot=[8 8] reward:1.1\n",
      "s:None r:1.1 o:[[15  7]\n",
      " [ 8  8]]\n",
      "convict=[15  7] loot=[8 8] reward:1.1\n",
      "s:None r:1.1 o:[[15  7]\n",
      " [ 8  8]]\n",
      "convict=[15  7] loot=[8 8] reward:1.1\n",
      "s:None r:1.1 o:[[15  7]\n",
      " [ 8  8]]\n",
      "convict=[15  7] loot=[8 8] reward:-100\n",
      "s:None r:-100 o:[[15  7]\n",
      " [ 8  8]]\n",
      "\n",
      "start\n",
      "convict=[9 7] loot=[8 8] reward:1.4000000000000001\n",
      "s:None r:1.4000000000000001 o:[[9 7]\n",
      " [8 8]]\n",
      "convict=[10  7] loot=[8 8] reward:1.35\n",
      "s:None r:1.35 o:[[10  7]\n",
      " [ 8  8]]\n",
      "convict=[11  7] loot=[8 8] reward:1.3000000000000003\n",
      "s:None r:1.3000000000000003 o:[[11  7]\n",
      " [ 8  8]]\n",
      "convict=[12  7] loot=[8 8] reward:1.25\n",
      "s:None r:1.25 o:[[12  7]\n",
      " [ 8  8]]\n",
      "convict=[13  7] loot=[8 8] reward:1.2000000000000002\n",
      "s:None r:1.2000000000000002 o:[[13  7]\n",
      " [ 8  8]]\n",
      "convict=[14  7] loot=[8 8] reward:1.1500000000000001\n",
      "s:None r:1.1500000000000001 o:[[14  7]\n",
      " [ 8  8]]\n",
      "convict=[15  7] loot=[8 8] reward:1.1\n",
      "s:None r:1.1 o:[[15  7]\n",
      " [ 8  8]]\n",
      "convict=[15  7] loot=[8 8] reward:1.1\n",
      "s:None r:1.1 o:[[15  7]\n",
      " [ 8  8]]\n",
      "convict=[15  7] loot=[8 8] reward:1.1\n",
      "s:None r:1.1 o:[[15  7]\n",
      " [ 8  8]]\n",
      "convict=[15  7] loot=[8 8] reward:-100\n",
      "s:None r:-100 o:[[15  7]\n",
      " [ 8  8]]\n"
     ]
    }
   ],
   "source": [
    "#env = gym.make('random_treasure:random_treasure-v1')\n",
    "\n",
    "for i in range(3):\n",
    "    obs = env.reset()\n",
    "    print(\"\\nstart\")\n",
    "    while True:\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, rewards, dones, info = env.step(action)\n",
    "        print(\"s:\" + str(_states) + \" r:\" + str(rewards) + \" o:\" + str(obs))        \n",
    "        if dones:\n",
    "            break\n",
    "        env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 20\n",
      "Trainable params: 20\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Option 1 : Simple model\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('softmax'))\n",
    "print(model.summary())\n",
    "\n",
    "sars = SARSAAgent(model=model, nb_actions=nb_actions, nb_steps_warmup=0, train_interval=1)\n",
    "sars.compile(optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 300000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "    1/10000 [..............................] - ETA: 15:58 - reward: 1.2000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lkoziol/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 40s 4ms/step - reward: -0.0445\n",
      "1000 episodes - episode_reward: -0.445 [-1.000, 2.100] - loss: 0.523 - mean_q: 0.691 - reward: -0.045\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 40s 4ms/step - reward: -0.0502\n",
      "1000 episodes - episode_reward: -0.502 [-1.000, 1.200] - loss: 0.527 - mean_q: 0.712 - reward: -0.050\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: 0.1223\n",
      "1001 episodes - episode_reward: 1.216 [-1.000, 20.900] - loss: 0.710 - mean_q: 0.648 - reward: 0.122\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 40s 4ms/step - reward: 0.0707\n",
      "1000 episodes - episode_reward: 0.706 [-1.000, 2.500] - loss: 0.690 - mean_q: 0.544 - reward: 0.071\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 39s 4ms/step - reward: 0.1125\n",
      "1004 episodes - episode_reward: 1.118 [-1.000, 21.100] - loss: 0.730 - mean_q: 0.490 - reward: 0.112\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 40s 4ms/step - reward: 0.1659\n",
      "1003 episodes - episode_reward: 1.662 [0.800, 18.350] - loss: 0.788 - mean_q: 0.381 - reward: 0.166\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: 0.1722\n",
      "1004 episodes - episode_reward: 1.707 [0.800, 18.050] - loss: 0.782 - mean_q: 0.500 - reward: 0.172\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 40s 4ms/step - reward: 0.1602\n",
      "1002 episodes - episode_reward: 1.608 [0.800, 18.150] - loss: 0.780 - mean_q: 0.597 - reward: 0.160\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 40s 4ms/step - reward: 0.1664\n",
      "1002 episodes - episode_reward: 1.651 [0.800, 18.250] - loss: 0.782 - mean_q: 0.677 - reward: 0.166\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: 0.1588\n",
      "1001 episodes - episode_reward: 1.588 [0.900, 17.950] - loss: 0.783 - mean_q: 0.712 - reward: 0.159\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 40s 4ms/step - reward: 0.1643\n",
      "1002 episodes - episode_reward: 1.639 [0.800, 18.250] - loss: 0.785 - mean_q: 0.738 - reward: 0.164\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: 0.1589\n",
      "1001 episodes - episode_reward: 1.587 [0.900, 15.500] - loss: 0.784 - mean_q: 0.758 - reward: 0.159\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: 0.1605\n",
      "1002 episodes - episode_reward: 1.608 [0.800, 15.500] - loss: 0.786 - mean_q: 0.776 - reward: 0.160\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: 0.1669\n",
      "1003 episodes - episode_reward: 1.663 [0.800, 17.950] - loss: 0.787 - mean_q: 0.785 - reward: 0.167\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: 0.1622\n",
      "1001 episodes - episode_reward: 1.617 [0.900, 18.250] - loss: 0.791 - mean_q: 0.801 - reward: 0.162\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: 0.1735\n",
      "1006 episodes - episode_reward: 1.730 [0.800, 18.350] - loss: 0.790 - mean_q: 0.805 - reward: 0.173\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: 0.1621\n",
      "1002 episodes - episode_reward: 1.618 [0.800, 15.500] - loss: 0.790 - mean_q: 0.805 - reward: 0.162\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: 0.1730\n",
      "1005 episodes - episode_reward: 1.720 [0.800, 18.250] - loss: 0.790 - mean_q: 0.799 - reward: 0.173\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: 0.1641\n",
      "1002 episodes - episode_reward: 1.633 [1.000, 18.150] - loss: 0.787 - mean_q: 0.817 - reward: 0.164\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: 0.1712\n",
      "1005 episodes - episode_reward: 1.706 [1.100, 18.250] - loss: 0.793 - mean_q: 0.832 - reward: 0.171\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      "10000/10000 [==============================] - 40s 4ms/step - reward: 0.1594\n",
      "1000 episodes - episode_reward: 1.588 [0.800, 15.500] - loss: 0.794 - mean_q: 0.818 - reward: 0.159\n",
      "\n",
      "Interval 22 (210000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: 0.1652\n",
      "1003 episodes - episode_reward: 1.647 [0.900, 15.500] - loss: 0.794 - mean_q: 0.808 - reward: 0.165\n",
      "\n",
      "Interval 23 (220000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: 0.1636\n",
      "1002 episodes - episode_reward: 1.638 [1.000, 18.250] - loss: 0.793 - mean_q: 0.820 - reward: 0.164\n",
      "\n",
      "Interval 24 (230000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: 0.1618\n",
      "1002 episodes - episode_reward: 1.618 [1.100, 18.150] - loss: 0.791 - mean_q: 0.833 - reward: 0.162\n",
      "\n",
      "Interval 25 (240000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: 0.1665\n",
      "1003 episodes - episode_reward: 1.657 [0.800, 18.350] - loss: 0.790 - mean_q: 0.831 - reward: 0.167\n",
      "\n",
      "Interval 26 (250000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: 0.1748\n",
      "1006 episodes - episode_reward: 1.740 [0.800, 17.950] - loss: 0.793 - mean_q: 0.835 - reward: 0.175\n",
      "\n",
      "Interval 27 (260000 steps performed)\n",
      "10000/10000 [==============================] - 40s 4ms/step - reward: 0.1609\n",
      "1002 episodes - episode_reward: 1.606 [0.900, 15.500] - loss: 0.792 - mean_q: 0.846 - reward: 0.161\n",
      "\n",
      "Interval 28 (270000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: 0.1666\n",
      "1003 episodes - episode_reward: 1.662 [1.000, 18.250] - loss: 0.793 - mean_q: 0.855 - reward: 0.167\n",
      "\n",
      "Interval 29 (280000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: 0.1671\n",
      "1003 episodes - episode_reward: 1.667 [0.800, 20.800] - loss: 0.793 - mean_q: 0.859 - reward: 0.167\n",
      "\n",
      "Interval 30 (290000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: 0.1593\n",
      "done, took 1220.752 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbba00c0940>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sars.fit(env, nb_steps=300000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 1.450, steps: 10\n",
      "Episode 2: reward: 1.450, steps: 10\n",
      "Episode 3: reward: 1.450, steps: 10\n",
      "Episode 4: reward: 1.450, steps: 10\n"
     ]
    },
    {
     "ename": "TclError",
     "evalue": "invalid command name \".!frame.!canvas\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTclError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-92044af211ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/rl/core.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, env, nb_episodes, action_repetition, callbacks, visualize, nb_max_episode_steps, nb_max_start_steps, start_step_policy, verbose)\u001b[0m\n\u001b[1;32m    351\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m                         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_action_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m                     \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/rl/callbacks.py\u001b[0m in \u001b[0;36mon_action_end\u001b[0;34m(self, action, logs)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'on_action_end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                 \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_action_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/rl/callbacks.py\u001b[0m in \u001b[0;36mon_action_end\u001b[0;34m(self, action, logs)\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_action_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;34m\"\"\" Render environment at the end of each action \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sc2/rlplay/envs/fixed-treasure/fixed_treasure/envs/fixed_treasure.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"all\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mconvict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/tkinter/__init__.py\u001b[0m in \u001b[0;36mdelete\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   2816\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2817\u001b[0m         \u001b[0;34m\"\"\"Delete items identified by all tag or ids contained in ARGS.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2818\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'delete'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTclError\u001b[0m: invalid command name \".!frame.!canvas\""
     ]
    }
   ],
   "source": [
    "sars.test(env, nb_episodes=5, visualize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
