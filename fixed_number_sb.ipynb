{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lkoziol/anaconda3/lib/python3.8/site-packages/setuptools/distutils_patch.py:25: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from stable_baselines3 import A2C\n",
    "\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(4,)\n",
      "Discrete(5)\n",
      "5\n",
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "#env = gym.make('fixed_number:fixed_number-v1')\n",
    "env = gym.make('random_treasure_flat:random_treasure_flat-v1')\n",
    "\n",
    "\n",
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "\n",
    "nb_actions = env.action_space.n\n",
    "obs_dim = env.observation_space.shape\n",
    "print(nb_actions)\n",
    "print(obs_dim)\n",
    "\n",
    "#env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = A2C('MlpPolicy', 'fixed_number:fixed_number-v1').learn(400)\n",
    "#model.save(\"a2c_fixed_number_eolfunction_400\")\n",
    "model = A2C('MlpPolicy', 'random_treasure_flat:random_treasure_flat-v1').learn(500000)\n",
    "model.save(\"a2c_treasure_flat_distance_4close\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.youtube.com/watch?v=bD6V3rcr_54\n",
    "def build_model(states, actions):\n",
    "    model = Sequential()    \n",
    "    #model.add(Dense(64, activation='relu', input_shape=states))\n",
    "    #model.add(Dense(64, activation='relu', input_shape=states))\n",
    "    model.add(Flatten(input_shape=(1,) + states))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))    \n",
    "    return model\n",
    "\n",
    "def build_agent(model, actions):\n",
    "\tpolicy = BoltzmannQPolicy()\n",
    "\tmemory = SequentialMemory(limit=500000, window_length=1)\n",
    "\t#dqn = DQNAgent(model=model, memory=memory, policy=policy, nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "    \n",
    "\tdqn = DQNAgent(model=model, memory=memory, policy=policy, nb_actions=actions, nb_steps_warmup=10)\n",
    "\treturn dqn\n",
    "\n",
    "#model = build_model(states, actions)\n",
    "#dqn = build_agent(model, actions)\n",
    "#dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "#dqn.fit(env, nb_steps=50, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 500000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "    1/10000 [..............................] - ETA: 4:40 - reward: 1.3500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lkoziol/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n",
      "/home/lkoziol/anaconda3/lib/python3.8/site-packages/rl/memory.py:40: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 54s 5ms/step - reward: 1.5619\n",
      "400 episodes - episode_reward: 39.048 [24.400, 116.050] - loss: 1.069 - mae: 4.235 - mean_q: 6.702 - reward: 1.562\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 7.4739\n",
      "400 episodes - episode_reward: 186.847 [26.900, 257.200] - loss: 2.375 - mae: 8.122 - mean_q: 12.723 - reward: 7.474\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 10.0676\n",
      "400 episodes - episode_reward: 251.689 [24.150, 257.200] - loss: 6.799 - mae: 13.552 - mean_q: 24.496 - reward: 10.068\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 10.2707\n",
      "400 episodes - episode_reward: 256.769 [227.000, 257.200] - loss: 18.679 - mae: 22.301 - mean_q: 37.100 - reward: 10.271\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 10.2367\n",
      "400 episodes - episode_reward: 255.919 [217.000, 257.200] - loss: 35.420 - mae: 31.950 - mean_q: 49.108 - reward: 10.237\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 10.2448\n",
      "400 episodes - episode_reward: 256.120 [206.950, 257.200] - loss: 53.130 - mae: 40.363 - mean_q: 59.566 - reward: 10.245\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: 10.1252\n",
      "400 episodes - episode_reward: 253.130 [106.450, 257.200] - loss: 68.630 - mae: 46.996 - mean_q: 66.951 - reward: 10.125\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: 9.9453\n",
      "400 episodes - episode_reward: 248.632 [66.000, 257.200] - loss: 91.002 - mae: 55.181 - mean_q: 76.330 - reward: 9.945\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 9.9292\n",
      "400 episodes - episode_reward: 248.229 [156.700, 257.200] - loss: 110.820 - mae: 61.719 - mean_q: 83.723 - reward: 9.929\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 9.7945\n",
      "400 episodes - episode_reward: 244.862 [96.400, 257.200] - loss: 138.204 - mae: 68.729 - mean_q: 92.381 - reward: 9.794\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 69s 7ms/step - reward: 9.6093\n",
      "400 episodes - episode_reward: 240.232 [35.550, 257.200] - loss: 157.036 - mae: 73.700 - mean_q: 97.519 - reward: 9.609\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 67s 7ms/step - reward: 8.8868\n",
      "400 episodes - episode_reward: 222.169 [56.100, 257.200] - loss: 178.938 - mae: 79.162 - mean_q: 103.521 - reward: 8.887\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 68s 7ms/step - reward: 9.6171\n",
      "400 episodes - episode_reward: 240.428 [56.200, 257.200] - loss: 199.359 - mae: 83.063 - mean_q: 109.260 - reward: 9.617\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 65s 7ms/step - reward: 8.9280\n",
      "400 episodes - episode_reward: 223.199 [28.850, 257.200] - loss: 232.753 - mae: 89.506 - mean_q: 116.972 - reward: 8.928\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 70s 7ms/step - reward: 9.4566\n",
      "400 episodes - episode_reward: 236.416 [31.350, 257.200] - loss: 254.703 - mae: 92.710 - mean_q: 121.581 - reward: 9.457\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 71s 7ms/step - reward: 9.3921\n",
      "400 episodes - episode_reward: 234.803 [33.950, 257.200] - loss: 270.086 - mae: 95.738 - mean_q: 124.468 - reward: 9.392\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 73s 7ms/step - reward: 8.6460\n",
      "400 episodes - episode_reward: 216.150 [33.700, 257.200] - loss: 286.425 - mae: 99.398 - mean_q: 129.036 - reward: 8.646\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 74s 7ms/step - reward: 9.1763\n",
      "400 episodes - episode_reward: 229.407 [34.850, 257.200] - loss: 309.279 - mae: 103.851 - mean_q: 134.010 - reward: 9.176\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 75s 8ms/step - reward: 8.3358\n",
      "400 episodes - episode_reward: 208.395 [32.750, 257.200] - loss: 324.668 - mae: 105.561 - mean_q: 135.906 - reward: 8.336\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 76s 8ms/step - reward: 8.2226\n",
      "400 episodes - episode_reward: 205.566 [32.400, 257.200] - loss: 341.867 - mae: 109.281 - mean_q: 140.455 - reward: 8.223\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      "10000/10000 [==============================] - 77s 8ms/step - reward: 8.4524\n",
      "400 episodes - episode_reward: 211.310 [30.400, 257.200] - loss: 363.354 - mae: 111.561 - mean_q: 143.246 - reward: 8.452\n",
      "\n",
      "Interval 22 (210000 steps performed)\n",
      "10000/10000 [==============================] - 79s 8ms/step - reward: 7.4940\n",
      "400 episodes - episode_reward: 187.349 [30.350, 257.200] - loss: 359.148 - mae: 112.042 - mean_q: 143.174 - reward: 7.494\n",
      "\n",
      "Interval 23 (220000 steps performed)\n",
      "10000/10000 [==============================] - 83s 8ms/step - reward: 8.6562\n",
      "400 episodes - episode_reward: 216.405 [30.250, 257.200] - loss: 374.657 - mae: 113.158 - mean_q: 145.885 - reward: 8.656\n",
      "\n",
      "Interval 24 (230000 steps performed)\n",
      "10000/10000 [==============================] - 85s 8ms/step - reward: 6.1250\n",
      "400 episodes - episode_reward: 153.124 [30.050, 257.200] - loss: 376.941 - mae: 115.437 - mean_q: 145.914 - reward: 6.125\n",
      "\n",
      "Interval 25 (240000 steps performed)\n",
      "10000/10000 [==============================] - 86s 9ms/step - reward: 9.1900\n",
      "400 episodes - episode_reward: 229.750 [30.350, 257.200] - loss: 388.117 - mae: 114.957 - mean_q: 148.041 - reward: 9.190\n",
      "\n",
      "Interval 26 (250000 steps performed)\n",
      "10000/10000 [==============================] - 87s 9ms/step - reward: 9.4392\n",
      "400 episodes - episode_reward: 235.980 [32.800, 257.200] - loss: 399.308 - mae: 115.159 - mean_q: 148.488 - reward: 9.439\n",
      "\n",
      "Interval 27 (260000 steps performed)\n",
      "10000/10000 [==============================] - 89s 9ms/step - reward: 9.8131\n",
      "400 episodes - episode_reward: 245.329 [31.450, 257.200] - loss: 424.820 - mae: 119.038 - mean_q: 153.874 - reward: 9.813\n",
      "\n",
      "Interval 28 (270000 steps performed)\n",
      "10000/10000 [==============================] - 90s 9ms/step - reward: 9.1364\n",
      "400 episodes - episode_reward: 228.411 [34.550, 257.200] - loss: 458.163 - mae: 123.205 - mean_q: 158.380 - reward: 9.136\n",
      "\n",
      "Interval 29 (280000 steps performed)\n",
      "10000/10000 [==============================] - 92s 9ms/step - reward: 9.8406\n",
      "400 episodes - episode_reward: 246.015 [75.900, 257.200] - loss: 454.716 - mae: 122.339 - mean_q: 159.196 - reward: 9.841\n",
      "\n",
      "Interval 30 (290000 steps performed)\n",
      "10000/10000 [==============================] - 93s 9ms/step - reward: 8.7842\n",
      "400 episodes - episode_reward: 219.604 [32.300, 257.200] - loss: 488.761 - mae: 128.295 - mean_q: 163.658 - reward: 8.784\n",
      "\n",
      "Interval 31 (300000 steps performed)\n",
      "10000/10000 [==============================] - 95s 9ms/step - reward: 7.8290\n",
      "400 episodes - episode_reward: 195.724 [28.750, 257.200] - loss: 475.919 - mae: 128.791 - mean_q: 163.874 - reward: 7.829\n",
      "\n",
      "Interval 32 (310000 steps performed)\n",
      "10000/10000 [==============================] - 96s 10ms/step - reward: 9.1749\n",
      "400 episodes - episode_reward: 229.373 [30.300, 257.200] - loss: 511.856 - mae: 129.851 - mean_q: 166.946 - reward: 9.175\n",
      "\n",
      "Interval 33 (320000 steps performed)\n",
      "10000/10000 [==============================] - 98s 10ms/step - reward: 9.3432\n",
      "400 episodes - episode_reward: 233.580 [31.750, 257.200] - loss: 530.189 - mae: 131.997 - mean_q: 169.480 - reward: 9.343\n",
      "\n",
      "Interval 34 (330000 steps performed)\n",
      "10000/10000 [==============================] - 100s 10ms/step - reward: 8.0249\n",
      "400 episodes - episode_reward: 200.624 [25.050, 257.200] - loss: 540.227 - mae: 134.180 - mean_q: 172.240 - reward: 8.025\n",
      "\n",
      "Interval 35 (340000 steps performed)\n",
      "10000/10000 [==============================] - 103s 10ms/step - reward: 8.7073\n",
      "400 episodes - episode_reward: 217.683 [29.600, 257.200] - loss: 527.854 - mae: 134.302 - mean_q: 172.490 - reward: 8.707\n",
      "\n",
      "Interval 36 (350000 steps performed)\n",
      "10000/10000 [==============================] - 104s 10ms/step - reward: 8.5585\n",
      "400 episodes - episode_reward: 213.961 [33.250, 257.200] - loss: 554.513 - mae: 136.223 - mean_q: 173.695 - reward: 8.558\n",
      "\n",
      "Interval 37 (360000 steps performed)\n",
      "10000/10000 [==============================] - 106s 11ms/step - reward: 9.1747\n",
      "400 episodes - episode_reward: 229.368 [31.950, 257.200] - loss: 574.862 - mae: 137.618 - mean_q: 176.786 - reward: 9.175\n",
      "\n",
      "Interval 38 (370000 steps performed)\n",
      "10000/10000 [==============================] - 108s 11ms/step - reward: 9.1018\n",
      "400 episodes - episode_reward: 227.544 [30.650, 257.200] - loss: 584.137 - mae: 137.593 - mean_q: 176.849 - reward: 9.102\n",
      "\n",
      "Interval 39 (380000 steps performed)\n",
      "10000/10000 [==============================] - 110s 11ms/step - reward: 8.7362\n",
      "400 episodes - episode_reward: 218.406 [30.800, 257.200] - loss: 566.324 - mae: 138.237 - mean_q: 176.369 - reward: 8.736\n",
      "\n",
      "Interval 40 (390000 steps performed)\n",
      "10000/10000 [==============================] - 133s 13ms/step - reward: 9.0175\n",
      "400 episodes - episode_reward: 225.439 [28.950, 257.200] - loss: 600.106 - mae: 140.760 - mean_q: 180.079 - reward: 9.018\n",
      "\n",
      "Interval 41 (400000 steps performed)\n",
      "10000/10000 [==============================] - 114s 11ms/step - reward: 8.5632\n",
      "400 episodes - episode_reward: 214.079 [30.650, 257.200] - loss: 613.114 - mae: 143.562 - mean_q: 182.294 - reward: 8.563\n",
      "\n",
      "Interval 42 (410000 steps performed)\n",
      "10000/10000 [==============================] - 116s 12ms/step - reward: 9.0829\n",
      "400 episodes - episode_reward: 227.073 [33.700, 257.200] - loss: 621.483 - mae: 141.984 - mean_q: 182.606 - reward: 9.083\n",
      "\n",
      "Interval 43 (420000 steps performed)\n",
      "10000/10000 [==============================] - 117s 12ms/step - reward: 8.2283\n",
      "400 episodes - episode_reward: 205.708 [29.200, 257.200] - loss: 627.262 - mae: 144.331 - mean_q: 184.328 - reward: 8.228\n",
      "\n",
      "Interval 44 (430000 steps performed)\n",
      "10000/10000 [==============================] - 119s 12ms/step - reward: 8.0978\n",
      "400 episodes - episode_reward: 202.445 [32.600, 257.200] - loss: 647.891 - mae: 145.532 - mean_q: 186.535 - reward: 8.098\n",
      "\n",
      "Interval 45 (440000 steps performed)\n",
      "10000/10000 [==============================] - 121s 12ms/step - reward: 5.6758\n",
      "400 episodes - episode_reward: 141.895 [32.750, 257.200] - loss: 659.704 - mae: 149.229 - mean_q: 189.226 - reward: 5.676\n",
      "\n",
      "Interval 46 (450000 steps performed)\n",
      "10000/10000 [==============================] - 123s 12ms/step - reward: 6.4017\n",
      "400 episodes - episode_reward: 160.042 [32.850, 257.200] - loss: 647.769 - mae: 147.588 - mean_q: 188.950 - reward: 6.402\n",
      "\n",
      "Interval 47 (460000 steps performed)\n",
      "10000/10000 [==============================] - 125s 13ms/step - reward: 6.9473\n",
      "400 episodes - episode_reward: 173.682 [30.800, 257.200] - loss: 665.445 - mae: 147.924 - mean_q: 189.974 - reward: 6.947\n",
      "\n",
      "Interval 48 (470000 steps performed)\n",
      "10000/10000 [==============================] - 122s 12ms/step - reward: 7.6699\n",
      "400 episodes - episode_reward: 191.748 [28.150, 257.200] - loss: 670.815 - mae: 149.591 - mean_q: 191.323 - reward: 7.670\n",
      "\n",
      "Interval 49 (480000 steps performed)\n",
      "10000/10000 [==============================] - 122s 12ms/step - reward: 7.7181\n",
      "400 episodes - episode_reward: 192.953 [26.100, 257.200] - loss: 642.386 - mae: 147.229 - mean_q: 186.875 - reward: 7.718\n",
      "\n",
      "Interval 50 (490000 steps performed)\n",
      "10000/10000 [==============================] - 122s 12ms/step - reward: 8.7787\n",
      "done, took 4409.726 seconds\n"
     ]
    }
   ],
   "source": [
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n\n",
    "model = build_model(states, actions)\n",
    "dqn = build_agent(model, actions)\n",
    "\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=500000, visualize=False, verbose=1)\n",
    "dqn.save_weights(\"dqn_fixed_number_close4_500k_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = A2C('MlpPolicy', 'fixed_number:fixed_number-v1').learn(4000)\n",
    "model.save(\"a2c_fixed_number_eolfunction_400k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = A2C.load(\"a2c_treasure_flat_distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DQNAgent' object has no attribute 'target_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-570bdf8fd4d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdqn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dqn_fixed_number_eolfunction_500k'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/rl/agents/dqn.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_target_model_hard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/rl/agents/dqn.py\u001b[0m in \u001b[0;36mupdate_target_model_hard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_target_model_hard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DQNAgent' object has no attribute 'target_model'"
     ]
    }
   ],
   "source": [
    "model = build_model(states, actions)\n",
    "dqn = build_agent(model, actions)\n",
    "\n",
    "dqn.load_weights('dqn_fixed_number_eolfunction_500k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 20 episodes ...\n",
      "state=[ 8  8 10  9]\n",
      "state=[ 9  8 10  9]\n",
      "state=[ 9  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "Episode 1: reward: 257.200, steps: 25\n",
      "state=[ 8  8 10  9]\n",
      "state=[ 9  8 10  9]\n",
      "state=[ 9  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "Episode 2: reward: 257.200, steps: 25\n",
      "state=[ 8  8 10  9]\n",
      "state=[ 9  8 10  9]\n",
      "state=[ 9  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "Episode 3: reward: 257.200, steps: 25\n",
      "state=[ 8  8 10  9]\n",
      "state=[ 9  8 10  9]\n",
      "state=[ 9  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "Episode 4: reward: 257.200, steps: 25\n",
      "state=[ 8  8 10  9]\n",
      "state=[ 9  8 10  9]\n",
      "state=[ 9  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "Episode 5: reward: 257.200, steps: 25\n",
      "state=[ 8  8 10  9]\n",
      "state=[ 9  8 10  9]\n",
      "state=[ 9  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "Episode 6: reward: 257.200, steps: 25\n",
      "state=[ 8  8 10  9]\n",
      "state=[ 9  8 10  9]\n",
      "state=[ 9  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "Episode 7: reward: 257.200, steps: 25\n",
      "state=[ 8  8 10  9]\n",
      "state=[ 9  8 10  9]\n",
      "state=[ 9  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "Episode 8: reward: 257.200, steps: 25\n",
      "state=[ 8  8 10  9]\n",
      "state=[ 9  8 10  9]\n",
      "state=[ 9  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "Episode 9: reward: 257.200, steps: 25\n",
      "state=[ 8  8 10  9]\n",
      "state=[ 9  8 10  9]\n",
      "state=[ 9  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "Episode 10: reward: 257.200, steps: 25\n",
      "state=[ 8  8 10  9]\n",
      "state=[ 9  8 10  9]\n",
      "state=[ 9  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "Episode 11: reward: 257.200, steps: 25\n",
      "state=[ 8  8 10  9]\n",
      "state=[ 9  8 10  9]\n",
      "state=[ 9  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "Episode 12: reward: 257.200, steps: 25\n",
      "state=[ 8  8 10  9]\n",
      "state=[ 9  8 10  9]\n",
      "state=[ 9  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "Episode 13: reward: 257.200, steps: 25\n",
      "state=[ 8  8 10  9]\n",
      "state=[ 9  8 10  9]\n",
      "state=[ 9  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "Episode 14: reward: 257.200, steps: 25\n",
      "state=[ 8  8 10  9]\n",
      "state=[ 9  8 10  9]\n",
      "state=[ 9  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "Episode 15: reward: 257.200, steps: 25\n",
      "state=[ 8  8 10  9]\n",
      "state=[ 9  8 10  9]\n",
      "state=[ 9  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "Episode 16: reward: 257.200, steps: 25\n",
      "state=[ 8  8 10  9]\n",
      "state=[ 9  8 10  9]\n",
      "state=[ 9  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "Episode 17: reward: 257.200, steps: 25\n",
      "state=[ 8  8 10  9]\n",
      "state=[ 9  8 10  9]\n",
      "state=[ 9  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "Episode 18: reward: 257.200, steps: 25\n",
      "state=[ 8  8 10  9]\n",
      "state=[ 9  8 10  9]\n",
      "state=[ 9  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "Episode 19: reward: 257.200, steps: 25\n",
      "state=[ 8  8 10  9]\n",
      "state=[ 9  8 10  9]\n",
      "state=[ 9  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "Episode 20: reward: 257.200, steps: 25\n",
      "{'episode_reward': [257.2, 257.2, 257.2, 257.2, 257.2, 257.2, 257.2, 257.2, 257.2, 257.2, 257.2, 257.2, 257.2, 257.2, 257.2, 257.2, 257.2, 257.2, 257.2, 257.2], 'nb_steps': [25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25]}\n"
     ]
    }
   ],
   "source": [
    "scores = dqn.test(env, nb_episodes=20, visualize=True)\n",
    "#print(np.mean(scores.history['episode_reward']))\n",
    "#dir(dqn)\n",
    "print(scores.history)\n",
    "#dir(scores)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "start obs [ 8  7 10  9]\n",
      "s:None r:1.35 o:[ 8  8 10  9] a:2\n",
      "state=[ 8  8 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:2\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "\n",
      "start obs [ 8  7 10  9]\n",
      "s:None r:1.35 o:[ 8  8 10  9] a:2\n",
      "state=[ 8  8 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:2\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "\n",
      "start obs [ 8  7 10  9]\n",
      "s:None r:1.35 o:[ 8  8 10  9] a:2\n",
      "state=[ 8  8 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:2\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "\n",
      "start obs [ 8  7 10  9]\n",
      "s:None r:1.35 o:[ 8  8 10  9] a:2\n",
      "state=[ 8  8 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:2\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "\n",
      "start obs [ 8  7 10  9]\n",
      "s:None r:1.35 o:[ 8  8 10  9] a:2\n",
      "state=[ 8  8 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:2\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "\n",
      "start obs [ 8  7 10  9]\n",
      "s:None r:1.35 o:[ 8  8 10  9] a:2\n",
      "state=[ 8  8 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:2\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "\n",
      "start obs [ 8  7 10  9]\n",
      "s:None r:1.35 o:[ 8  8 10  9] a:2\n",
      "state=[ 8  8 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:2\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "\n",
      "start obs [ 8  7 10  9]\n",
      "s:None r:1.35 o:[ 8  8 10  9] a:2\n",
      "state=[ 8  8 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:2\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "\n",
      "start obs [ 8  7 10  9]\n",
      "s:None r:1.35 o:[ 8  8 10  9] a:2\n",
      "state=[ 8  8 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:2\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "\n",
      "start obs [ 8  7 10  9]\n",
      "s:None r:1.35 o:[ 8  8 10  9] a:2\n",
      "state=[ 8  8 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:2\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:4\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('random_treasure_flat:random_treasure_flat-v1')\n",
    "\n",
    "for i in range(10):\n",
    "    obs = env.reset()\n",
    "    print(\"\\nstart \" + 'obs ' + str(obs))\n",
    "\n",
    "    while True:\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, rewards, dones, info = env.step(action)\n",
    "        print(\"s:\" + str(_states) + \" r:\" + str(rewards) + \" o:\" + str(obs) + \" a:\" + str(action))        \n",
    "        if dones:\n",
    "            break\n",
    "        env.render()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
