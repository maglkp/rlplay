{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(4,)\n",
      "Discrete(5)\n",
      "5\n",
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "#env = gym.make('fixed_number:fixed_number-v1')\n",
    "env = gym.make('random_treasure_flat:random_treasure_flat-v1')\n",
    "\n",
    "\n",
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "\n",
    "nb_actions = env.action_space.n\n",
    "obs_dim = env.observation_space.shape\n",
    "print(nb_actions)\n",
    "print(obs_dim)\n",
    "\n",
    "#env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = A2C('MlpPolicy', 'fixed_number:fixed_number-v1').learn(400)\n",
    "#model.save(\"a2c_fixed_number_eolfunction_400\")\n",
    "#model = PPO('MlpPolicy', 'random_treasure_flat:random_treasure_flat-v1').learn(500000)\n",
    "log_path = os.path.join('training_logs')\n",
    "model = A2C('MlpPolicy', 'random_treasure_flat:random_treasure_flat-v1', tensorboard_log=log_path).learn(500000)\n",
    "#tensorboard --logdir='training_logs/A2C_1'\n",
    "#model.save(\"a2c_treasure_flat_distance_ppo_500k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34.96000039577484, 1.3268015165103992)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#evaluate_policy(model, env, n_eval_episodes=10, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.youtube.com/watch?v=bD6V3rcr_54\n",
    "def build_model(states, actions):\n",
    "    model = Sequential()    \n",
    "    #model.add(Dense(64, activation='relu', input_shape=states))\n",
    "    #model.add(Dense(64, activation='relu', input_shape=states))\n",
    "    model.add(Flatten(input_shape=(1,) + states))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))    \n",
    "    return model\n",
    "\n",
    "def build_agent(model, actions):\n",
    "\tpolicy = BoltzmannQPolicy()\n",
    "\tmemory = SequentialMemory(limit=1500000, window_length=1)\n",
    "\t#dqn = DQNAgent(model=model, memory=memory, policy=policy, nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "    \n",
    "\tdqn = DQNAgent(model=model, memory=memory, policy=policy, nb_actions=actions, nb_steps_warmup=1)\n",
    "\treturn dqn\n",
    "\n",
    "#model = build_model(states, actions)\n",
    "#dqn = build_agent(model, actions)\n",
    "#dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "#dqn.fit(env, nb_steps=50, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1300000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "    1/10000 [..............................] - ETA: 7:48 - reward: 1.3500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lkoziol/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n",
      "/home/lkoziol/anaconda3/lib/python3.8/site-packages/rl/memory.py:40: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 56s 6ms/step - reward: 1.5375\n",
      "400 episodes - episode_reward: 38.437 [14.950, 146.450] - loss: 0.741 - mae: 3.268 - mean_q: 4.589 - reward: 1.537\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 6.2668\n",
      "400 episodes - episode_reward: 156.669 [16.850, 257.200] - loss: 1.457 - mae: 5.553 - mean_q: 8.853 - reward: 6.267\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 7.4806\n",
      "400 episodes - episode_reward: 187.015 [16.650, 257.200] - loss: 4.282 - mae: 9.289 - mean_q: 16.579 - reward: 7.481\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 58s 6ms/step - reward: 8.8132\n",
      "400 episodes - episode_reward: 220.330 [19.250, 257.200] - loss: 11.322 - mae: 14.903 - mean_q: 25.575 - reward: 8.813\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 59s 6ms/step - reward: 9.2659\n",
      "400 episodes - episode_reward: 231.648 [17.450, 257.200] - loss: 23.563 - mae: 21.990 - mean_q: 35.258 - reward: 9.266\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 60s 6ms/step - reward: 9.4063\n",
      "400 episodes - episode_reward: 235.156 [16.000, 257.200] - loss: 35.975 - mae: 28.839 - mean_q: 43.885 - reward: 9.406\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 61s 6ms/step - reward: 9.5198\n",
      "400 episodes - episode_reward: 237.994 [17.650, 257.200] - loss: 49.695 - mae: 35.012 - mean_q: 51.556 - reward: 9.520\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: 9.4912\n",
      "400 episodes - episode_reward: 237.280 [19.800, 257.200] - loss: 68.257 - mae: 42.513 - mean_q: 60.578 - reward: 9.491\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 9.6569\n",
      "400 episodes - episode_reward: 241.423 [165.500, 257.200] - loss: 90.168 - mae: 50.216 - mean_q: 69.916 - reward: 9.657\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 65s 6ms/step - reward: 9.6349\n",
      "400 episodes - episode_reward: 240.872 [156.100, 257.200] - loss: 112.473 - mae: 56.992 - mean_q: 78.114 - reward: 9.635\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 66s 7ms/step - reward: 9.4729\n",
      "400 episodes - episode_reward: 236.822 [145.400, 257.200] - loss: 137.485 - mae: 63.838 - mean_q: 86.484 - reward: 9.473\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 67s 7ms/step - reward: 9.3407\n",
      "400 episodes - episode_reward: 233.518 [36.100, 257.200] - loss: 160.135 - mae: 68.937 - mean_q: 92.797 - reward: 9.341\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 68s 7ms/step - reward: 9.3707\n",
      "400 episodes - episode_reward: 234.269 [36.100, 257.200] - loss: 181.182 - mae: 73.766 - mean_q: 100.036 - reward: 9.371\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 70s 7ms/step - reward: 9.3856\n",
      "400 episodes - episode_reward: 234.641 [136.000, 257.200] - loss: 208.354 - mae: 80.206 - mean_q: 107.604 - reward: 9.386\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 71s 7ms/step - reward: 9.0170\n",
      "400 episodes - episode_reward: 225.426 [55.550, 257.200] - loss: 235.826 - mae: 85.966 - mean_q: 113.697 - reward: 9.017\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 73s 7ms/step - reward: 8.8000\n",
      "400 episodes - episode_reward: 220.000 [34.800, 257.200] - loss: 249.921 - mae: 90.294 - mean_q: 118.312 - reward: 8.800\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 74s 7ms/step - reward: 8.5881\n",
      "400 episodes - episode_reward: 214.703 [34.850, 257.200] - loss: 258.497 - mae: 91.926 - mean_q: 119.823 - reward: 8.588\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 75s 8ms/step - reward: 9.1189\n",
      "400 episodes - episode_reward: 227.972 [34.650, 257.200] - loss: 280.580 - mae: 95.201 - mean_q: 124.392 - reward: 9.119\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 77s 8ms/step - reward: 8.9656\n",
      "400 episodes - episode_reward: 224.139 [31.850, 257.200] - loss: 299.208 - mae: 98.253 - mean_q: 128.529 - reward: 8.966\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 78s 8ms/step - reward: 9.3124\n",
      "400 episodes - episode_reward: 232.810 [34.400, 257.200] - loss: 318.610 - mae: 101.033 - mean_q: 132.171 - reward: 9.312\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      "10000/10000 [==============================] - 80s 8ms/step - reward: 9.2653\n",
      "400 episodes - episode_reward: 231.632 [34.400, 257.200] - loss: 341.095 - mae: 104.269 - mean_q: 136.471 - reward: 9.265\n",
      "\n",
      "Interval 22 (210000 steps performed)\n",
      "10000/10000 [==============================] - 81s 8ms/step - reward: 8.9704\n",
      "400 episodes - episode_reward: 224.261 [29.200, 257.200] - loss: 346.261 - mae: 105.663 - mean_q: 137.521 - reward: 8.970\n",
      "\n",
      "Interval 23 (220000 steps performed)\n",
      "10000/10000 [==============================] - 82s 8ms/step - reward: 8.8656\n",
      "400 episodes - episode_reward: 221.640 [29.500, 257.200] - loss: 384.064 - mae: 111.420 - mean_q: 144.623 - reward: 8.866\n",
      "\n",
      "Interval 24 (230000 steps performed)\n",
      "10000/10000 [==============================] - 84s 8ms/step - reward: 8.7821\n",
      "400 episodes - episode_reward: 219.552 [29.000, 257.200] - loss: 392.178 - mae: 113.887 - mean_q: 147.044 - reward: 8.782\n",
      "\n",
      "Interval 25 (240000 steps performed)\n",
      "10000/10000 [==============================] - 85s 8ms/step - reward: 8.7612\n",
      "400 episodes - episode_reward: 219.030 [25.850, 257.200] - loss: 399.863 - mae: 114.107 - mean_q: 147.143 - reward: 8.761\n",
      "\n",
      "Interval 26 (250000 steps performed)\n",
      "10000/10000 [==============================] - 86s 9ms/step - reward: 8.8356\n",
      "400 episodes - episode_reward: 220.891 [29.650, 257.200] - loss: 425.773 - mae: 117.721 - mean_q: 151.950 - reward: 8.836\n",
      "\n",
      "Interval 27 (260000 steps performed)\n",
      "10000/10000 [==============================] - 88s 9ms/step - reward: 8.4573\n",
      "400 episodes - episode_reward: 211.432 [34.950, 257.200] - loss: 425.282 - mae: 118.309 - mean_q: 152.791 - reward: 8.457\n",
      "\n",
      "Interval 28 (270000 steps performed)\n",
      "10000/10000 [==============================] - 89s 9ms/step - reward: 8.5904\n",
      "400 episodes - episode_reward: 214.760 [22.150, 257.200] - loss: 418.470 - mae: 117.028 - mean_q: 151.552 - reward: 8.590\n",
      "\n",
      "Interval 29 (280000 steps performed)\n",
      "10000/10000 [==============================] - 91s 9ms/step - reward: 8.4050\n",
      "400 episodes - episode_reward: 210.125 [22.200, 257.200] - loss: 415.804 - mae: 118.059 - mean_q: 151.813 - reward: 8.405\n",
      "\n",
      "Interval 30 (290000 steps performed)\n",
      "10000/10000 [==============================] - 92s 9ms/step - reward: 8.2442\n",
      "400 episodes - episode_reward: 206.104 [28.400, 257.200] - loss: 416.896 - mae: 118.472 - mean_q: 151.811 - reward: 8.244\n",
      "\n",
      "Interval 31 (300000 steps performed)\n",
      "10000/10000 [==============================] - 93s 9ms/step - reward: 7.1904\n",
      "400 episodes - episode_reward: 179.761 [26.500, 257.200] - loss: 433.033 - mae: 121.229 - mean_q: 155.122 - reward: 7.190\n",
      "\n",
      "Interval 32 (310000 steps performed)\n",
      "10000/10000 [==============================] - 96s 10ms/step - reward: 7.8900\n",
      "400 episodes - episode_reward: 197.251 [30.250, 257.200] - loss: 452.133 - mae: 122.207 - mean_q: 156.745 - reward: 7.890\n",
      "\n",
      "Interval 33 (320000 steps performed)\n",
      "10000/10000 [==============================] - 97s 10ms/step - reward: 8.5049\n",
      "400 episodes - episode_reward: 212.622 [27.950, 257.200] - loss: 472.429 - mae: 124.786 - mean_q: 160.600 - reward: 8.505\n",
      "\n",
      "Interval 34 (330000 steps performed)\n",
      "10000/10000 [==============================] - 99s 10ms/step - reward: 8.3913\n",
      "400 episodes - episode_reward: 209.783 [26.700, 257.200] - loss: 491.436 - mae: 128.440 - mean_q: 164.384 - reward: 8.391\n",
      "\n",
      "Interval 35 (340000 steps performed)\n",
      "10000/10000 [==============================] - 101s 10ms/step - reward: 7.3777\n",
      "400 episodes - episode_reward: 184.443 [27.450, 257.200] - loss: 487.966 - mae: 128.606 - mean_q: 163.703 - reward: 7.378\n",
      "\n",
      "Interval 36 (350000 steps performed)\n",
      "10000/10000 [==============================] - 103s 10ms/step - reward: 8.3520\n",
      "400 episodes - episode_reward: 208.799 [26.800, 257.200] - loss: 472.706 - mae: 124.843 - mean_q: 160.544 - reward: 8.352\n",
      "\n",
      "Interval 37 (360000 steps performed)\n",
      "10000/10000 [==============================] - 104s 10ms/step - reward: 8.0924\n",
      "400 episodes - episode_reward: 202.310 [24.750, 257.200] - loss: 500.187 - mae: 128.276 - mean_q: 165.202 - reward: 8.092\n",
      "\n",
      "Interval 38 (370000 steps performed)\n",
      "10000/10000 [==============================] - 106s 11ms/step - reward: 7.6712\n",
      "400 episodes - episode_reward: 191.779 [24.900, 257.200] - loss: 515.626 - mae: 130.333 - mean_q: 167.429 - reward: 7.671\n",
      "\n",
      "Interval 39 (380000 steps performed)\n",
      "10000/10000 [==============================] - 108s 11ms/step - reward: 7.4768\n",
      "400 episodes - episode_reward: 186.921 [20.700, 257.200] - loss: 522.472 - mae: 131.422 - mean_q: 168.674 - reward: 7.477\n",
      "\n",
      "Interval 40 (390000 steps performed)\n",
      "10000/10000 [==============================] - 110s 11ms/step - reward: 7.1009\n",
      "400 episodes - episode_reward: 177.523 [19.500, 257.200] - loss: 525.284 - mae: 133.428 - mean_q: 169.937 - reward: 7.101\n",
      "\n",
      "Interval 41 (400000 steps performed)\n",
      "10000/10000 [==============================] - 112s 11ms/step - reward: 7.0157\n",
      "400 episodes - episode_reward: 175.394 [22.600, 257.200] - loss: 505.745 - mae: 130.319 - mean_q: 166.483 - reward: 7.016\n",
      "\n",
      "Interval 42 (410000 steps performed)\n",
      "10000/10000 [==============================] - 114s 11ms/step - reward: 7.9380\n",
      "400 episodes - episode_reward: 198.451 [21.550, 257.200] - loss: 503.641 - mae: 128.355 - mean_q: 164.947 - reward: 7.938\n",
      "\n",
      "Interval 43 (420000 steps performed)\n",
      "10000/10000 [==============================] - 115s 12ms/step - reward: 8.4756\n",
      "400 episodes - episode_reward: 211.891 [26.000, 257.200] - loss: 511.642 - mae: 128.100 - mean_q: 165.616 - reward: 8.476\n",
      "\n",
      "Interval 44 (430000 steps performed)\n",
      "10000/10000 [==============================] - 117s 12ms/step - reward: 8.3054\n",
      "400 episodes - episode_reward: 207.636 [22.150, 257.200] - loss: 511.439 - mae: 129.727 - mean_q: 166.651 - reward: 8.305\n",
      "\n",
      "Interval 45 (440000 steps performed)\n",
      "10000/10000 [==============================] - 119s 12ms/step - reward: 7.2165\n",
      "400 episodes - episode_reward: 180.412 [23.900, 257.200] - loss: 502.498 - mae: 129.072 - mean_q: 164.140 - reward: 7.216\n",
      "\n",
      "Interval 46 (450000 steps performed)\n",
      "10000/10000 [==============================] - 121s 12ms/step - reward: 7.9349\n",
      "400 episodes - episode_reward: 198.373 [20.150, 257.200] - loss: 511.053 - mae: 130.027 - mean_q: 166.647 - reward: 7.935\n",
      "\n",
      "Interval 47 (460000 steps performed)\n",
      "10000/10000 [==============================] - 127s 13ms/step - reward: 8.4326\n",
      "400 episodes - episode_reward: 210.814 [22.550, 257.200] - loss: 537.476 - mae: 131.811 - mean_q: 168.979 - reward: 8.433\n",
      "\n",
      "Interval 48 (470000 steps performed)\n",
      "10000/10000 [==============================] - 124s 12ms/step - reward: 8.7272\n",
      "400 episodes - episode_reward: 218.181 [24.100, 257.200] - loss: 550.568 - mae: 132.175 - mean_q: 171.287 - reward: 8.727\n",
      "\n",
      "Interval 49 (480000 steps performed)\n",
      "10000/10000 [==============================] - 127s 13ms/step - reward: 8.6233\n",
      "400 episodes - episode_reward: 215.582 [22.600, 257.200] - loss: 545.178 - mae: 134.997 - mean_q: 172.998 - reward: 8.623\n",
      "\n",
      "Interval 50 (490000 steps performed)\n",
      "10000/10000 [==============================] - 128s 13ms/step - reward: 7.3922\n",
      "400 episodes - episode_reward: 184.806 [21.300, 257.200] - loss: 586.544 - mae: 139.751 - mean_q: 177.612 - reward: 7.392\n",
      "\n",
      "Interval 51 (500000 steps performed)\n",
      "10000/10000 [==============================] - 129s 13ms/step - reward: 6.9581\n",
      "400 episodes - episode_reward: 173.951 [19.100, 257.200] - loss: 580.852 - mae: 139.039 - mean_q: 177.011 - reward: 6.958\n",
      "\n",
      "Interval 52 (510000 steps performed)\n",
      "10000/10000 [==============================] - 131s 13ms/step - reward: 7.0296\n",
      "400 episodes - episode_reward: 175.741 [22.150, 257.200] - loss: 597.975 - mae: 140.502 - mean_q: 179.379 - reward: 7.030\n",
      "\n",
      "Interval 53 (520000 steps performed)\n",
      "10000/10000 [==============================] - 133s 13ms/step - reward: 6.6751\n",
      "400 episodes - episode_reward: 166.877 [22.450, 257.200] - loss: 598.929 - mae: 141.070 - mean_q: 179.288 - reward: 6.675\n",
      "\n",
      "Interval 54 (530000 steps performed)\n",
      "10000/10000 [==============================] - 135s 14ms/step - reward: 6.2988\n",
      "400 episodes - episode_reward: 157.471 [18.850, 257.200] - loss: 617.247 - mae: 143.601 - mean_q: 182.397 - reward: 6.299\n",
      "\n",
      "Interval 55 (540000 steps performed)\n",
      "10000/10000 [==============================] - 136s 14ms/step - reward: 6.8197\n",
      "400 episodes - episode_reward: 170.492 [20.150, 257.200] - loss: 635.001 - mae: 144.651 - mean_q: 184.566 - reward: 6.820\n",
      "\n",
      "Interval 56 (550000 steps performed)\n",
      "10000/10000 [==============================] - 139s 14ms/step - reward: 7.2806\n",
      "400 episodes - episode_reward: 182.016 [19.700, 257.200] - loss: 627.393 - mae: 143.755 - mean_q: 183.606 - reward: 7.281\n",
      "\n",
      "Interval 57 (560000 steps performed)\n",
      "10000/10000 [==============================] - 141s 14ms/step - reward: 7.5482\n",
      "400 episodes - episode_reward: 188.706 [22.200, 257.200] - loss: 633.003 - mae: 144.655 - mean_q: 184.596 - reward: 7.548\n",
      "\n",
      "Interval 58 (570000 steps performed)\n",
      "10000/10000 [==============================] - 142s 14ms/step - reward: 5.8401\n",
      "400 episodes - episode_reward: 146.004 [23.900, 257.200] - loss: 600.561 - mae: 142.979 - mean_q: 180.652 - reward: 5.840\n",
      "\n",
      "Interval 59 (580000 steps performed)\n",
      "10000/10000 [==============================] - 143s 14ms/step - reward: 6.6337\n",
      "400 episodes - episode_reward: 165.842 [22.850, 257.200] - loss: 601.562 - mae: 141.203 - mean_q: 178.608 - reward: 6.634\n",
      "\n",
      "Interval 60 (590000 steps performed)\n",
      "10000/10000 [==============================] - 146s 15ms/step - reward: 6.6949\n",
      "400 episodes - episode_reward: 167.373 [18.600, 257.200] - loss: 597.218 - mae: 140.719 - mean_q: 178.656 - reward: 6.695\n",
      "\n",
      "Interval 61 (600000 steps performed)\n",
      "10000/10000 [==============================] - 147s 15ms/step - reward: 6.3905\n",
      "400 episodes - episode_reward: 159.763 [18.650, 257.200] - loss: 590.509 - mae: 139.395 - mean_q: 177.015 - reward: 6.391\n",
      "\n",
      "Interval 62 (610000 steps performed)\n",
      "10000/10000 [==============================] - 150s 15ms/step - reward: 7.0688\n",
      "400 episodes - episode_reward: 176.720 [19.400, 257.200] - loss: 578.649 - mae: 138.395 - mean_q: 176.419 - reward: 7.069\n",
      "\n",
      "Interval 63 (620000 steps performed)\n",
      "10000/10000 [==============================] - 151s 15ms/step - reward: 7.5603\n",
      "400 episodes - episode_reward: 189.009 [17.750, 257.200] - loss: 569.003 - mae: 137.495 - mean_q: 175.456 - reward: 7.560\n",
      "\n",
      "Interval 64 (630000 steps performed)\n",
      "10000/10000 [==============================] - 153s 15ms/step - reward: 7.0169\n",
      "400 episodes - episode_reward: 175.423 [25.100, 257.200] - loss: 563.925 - mae: 136.328 - mean_q: 173.165 - reward: 7.017\n",
      "\n",
      "Interval 65 (640000 steps performed)\n",
      "10000/10000 [==============================] - 155s 16ms/step - reward: 7.5467\n",
      "400 episodes - episode_reward: 188.668 [22.900, 257.200] - loss: 568.533 - mae: 135.618 - mean_q: 173.949 - reward: 7.547\n",
      "\n",
      "Interval 66 (650000 steps performed)\n",
      "10000/10000 [==============================] - 157s 16ms/step - reward: 8.2290\n",
      "400 episodes - episode_reward: 205.726 [25.500, 257.200] - loss: 559.998 - mae: 135.156 - mean_q: 172.897 - reward: 8.229\n",
      "\n",
      "Interval 67 (660000 steps performed)\n",
      "10000/10000 [==============================] - 160s 16ms/step - reward: 7.5144\n",
      "400 episodes - episode_reward: 187.861 [22.150, 257.200] - loss: 566.036 - mae: 136.283 - mean_q: 174.269 - reward: 7.514\n",
      "\n",
      "Interval 68 (670000 steps performed)\n",
      "10000/10000 [==============================] - 159s 16ms/step - reward: 7.2366\n",
      "400 episodes - episode_reward: 180.915 [22.400, 257.200] - loss: 563.388 - mae: 136.808 - mean_q: 174.557 - reward: 7.237\n",
      "\n",
      "Interval 69 (680000 steps performed)\n",
      "10000/10000 [==============================] - 161s 16ms/step - reward: 6.9864\n",
      "400 episodes - episode_reward: 174.659 [23.100, 257.200] - loss: 570.639 - mae: 138.568 - mean_q: 176.077 - reward: 6.986\n",
      "\n",
      "Interval 70 (690000 steps performed)\n",
      "10000/10000 [==============================] - 164s 16ms/step - reward: 7.8502\n",
      "400 episodes - episode_reward: 196.255 [22.350, 257.200] - loss: 600.322 - mae: 139.889 - mean_q: 178.501 - reward: 7.850\n",
      "\n",
      "Interval 71 (700000 steps performed)\n",
      "10000/10000 [==============================] - 166s 17ms/step - reward: 7.0884\n",
      "400 episodes - episode_reward: 177.210 [19.650, 257.200] - loss: 605.348 - mae: 140.691 - mean_q: 179.372 - reward: 7.088\n",
      "\n",
      "Interval 72 (710000 steps performed)\n",
      "10000/10000 [==============================] - 169s 17ms/step - reward: 7.3300\n",
      "400 episodes - episode_reward: 183.251 [18.050, 257.200] - loss: 619.809 - mae: 141.513 - mean_q: 180.784 - reward: 7.330\n",
      "\n",
      "Interval 73 (720000 steps performed)\n",
      "10000/10000 [==============================] - 175s 18ms/step - reward: 7.7692\n",
      "400 episodes - episode_reward: 194.231 [25.700, 257.200] - loss: 607.799 - mae: 141.386 - mean_q: 180.292 - reward: 7.769\n",
      "\n",
      "Interval 74 (730000 steps performed)\n",
      "10000/10000 [==============================] - 175s 17ms/step - reward: 7.5567\n",
      "400 episodes - episode_reward: 188.918 [25.300, 257.200] - loss: 611.077 - mae: 141.170 - mean_q: 179.708 - reward: 7.557\n",
      "\n",
      "Interval 75 (740000 steps performed)\n",
      "10000/10000 [==============================] - 177s 18ms/step - reward: 6.9474\n",
      "400 episodes - episode_reward: 173.686 [26.650, 257.200] - loss: 570.859 - mae: 137.292 - mean_q: 174.496 - reward: 6.947\n",
      "\n",
      "Interval 76 (750000 steps performed)\n",
      "10000/10000 [==============================] - 180s 18ms/step - reward: 7.1151\n",
      "400 episodes - episode_reward: 177.878 [26.600, 257.200] - loss: 552.357 - mae: 135.752 - mean_q: 172.083 - reward: 7.115\n",
      "\n",
      "Interval 77 (760000 steps performed)\n",
      "10000/10000 [==============================] - 183s 18ms/step - reward: 7.1684\n",
      "400 episodes - episode_reward: 179.210 [22.500, 257.200] - loss: 558.209 - mae: 135.071 - mean_q: 171.764 - reward: 7.168\n",
      "\n",
      "Interval 78 (770000 steps performed)\n",
      "10000/10000 [==============================] - 187s 19ms/step - reward: 7.4379\n",
      "400 episodes - episode_reward: 185.948 [21.050, 257.200] - loss: 564.304 - mae: 136.267 - mean_q: 173.959 - reward: 7.438\n",
      "\n",
      "Interval 79 (780000 steps performed)\n",
      "10000/10000 [==============================] - 188s 19ms/step - reward: 7.9386\n",
      "400 episodes - episode_reward: 198.464 [21.050, 257.200] - loss: 553.008 - mae: 134.508 - mean_q: 172.354 - reward: 7.939\n",
      "\n",
      "Interval 80 (790000 steps performed)\n",
      "10000/10000 [==============================] - 190s 19ms/step - reward: 8.0192\n",
      "400 episodes - episode_reward: 200.479 [22.350, 257.200] - loss: 546.553 - mae: 132.922 - mean_q: 169.888 - reward: 8.019\n",
      "\n",
      "Interval 81 (800000 steps performed)\n",
      "10000/10000 [==============================] - 197s 20ms/step - reward: 8.0353\n",
      "400 episodes - episode_reward: 200.883 [24.000, 257.200] - loss: 560.403 - mae: 135.582 - mean_q: 173.202 - reward: 8.035\n",
      "\n",
      "Interval 82 (810000 steps performed)\n",
      "10000/10000 [==============================] - 273s 27ms/step - reward: 7.7818\n",
      "400 episodes - episode_reward: 194.546 [24.100, 257.200] - loss: 563.209 - mae: 136.194 - mean_q: 173.061 - reward: 7.782\n",
      "\n",
      "Interval 83 (820000 steps performed)\n",
      "10000/10000 [==============================] - 618s 62ms/step - reward: 8.4606\n",
      "400 episodes - episode_reward: 211.514 [26.450, 257.200] - loss: 550.099 - mae: 133.753 - mean_q: 171.193 - reward: 8.461\n",
      "\n",
      "Interval 84 (830000 steps performed)\n",
      "10000/10000 [==============================] - 307s 31ms/step - reward: 8.4714\n",
      "400 episodes - episode_reward: 211.784 [24.350, 257.200] - loss: 552.446 - mae: 133.435 - mean_q: 170.990 - reward: 8.471\n",
      "\n",
      "Interval 85 (840000 steps performed)\n",
      "10000/10000 [==============================] - 265s 26ms/step - reward: 8.9929\n",
      "400 episodes - episode_reward: 224.823 [29.100, 257.200] - loss: 571.402 - mae: 135.352 - mean_q: 173.513 - reward: 8.993\n",
      "\n",
      "Interval 86 (850000 steps performed)\n",
      "10000/10000 [==============================] - 275s 28ms/step - reward: 8.0458\n",
      "400 episodes - episode_reward: 201.144 [25.100, 257.200] - loss: 574.355 - mae: 137.074 - mean_q: 174.944 - reward: 8.046\n",
      "\n",
      "Interval 87 (860000 steps performed)\n",
      "10000/10000 [==============================] - 279s 28ms/step - reward: 8.0908\n",
      "400 episodes - episode_reward: 202.270 [28.300, 257.200] - loss: 586.997 - mae: 138.202 - mean_q: 176.417 - reward: 8.091\n",
      "\n",
      "Interval 88 (870000 steps performed)\n",
      "10000/10000 [==============================] - 280s 28ms/step - reward: 8.0783\n",
      "400 episodes - episode_reward: 201.959 [25.400, 257.200] - loss: 595.453 - mae: 139.357 - mean_q: 177.585 - reward: 8.078\n",
      "\n",
      "Interval 89 (880000 steps performed)\n",
      "10000/10000 [==============================] - 237s 24ms/step - reward: 7.7751\n",
      "400 episodes - episode_reward: 194.377 [26.000, 257.200] - loss: 620.432 - mae: 142.005 - mean_q: 180.470 - reward: 7.775\n",
      "\n",
      "Interval 90 (890000 steps performed)\n",
      "10000/10000 [==============================] - 219s 22ms/step - reward: 8.4618\n",
      "400 episodes - episode_reward: 211.545 [28.400, 257.200] - loss: 599.801 - mae: 139.298 - mean_q: 178.354 - reward: 8.462\n",
      "\n",
      "Interval 91 (900000 steps performed)\n",
      "10000/10000 [==============================] - 222s 22ms/step - reward: 7.8027\n",
      "400 episodes - episode_reward: 195.067 [28.150, 257.200] - loss: 598.549 - mae: 140.761 - mean_q: 179.096 - reward: 7.803\n",
      "\n",
      "Interval 92 (910000 steps performed)\n",
      "10000/10000 [==============================] - 224s 22ms/step - reward: 7.5498\n",
      "400 episodes - episode_reward: 188.744 [27.950, 257.200] - loss: 589.058 - mae: 138.646 - mean_q: 176.683 - reward: 7.550\n",
      "\n",
      "Interval 93 (920000 steps performed)\n",
      "10000/10000 [==============================] - 226s 23ms/step - reward: 7.7274\n",
      "400 episodes - episode_reward: 193.184 [27.100, 257.200] - loss: 594.776 - mae: 138.679 - mean_q: 177.137 - reward: 7.727\n",
      "\n",
      "Interval 94 (930000 steps performed)\n",
      "10000/10000 [==============================] - 228s 23ms/step - reward: 8.0727\n",
      "400 episodes - episode_reward: 201.817 [27.300, 257.200] - loss: 596.872 - mae: 138.940 - mean_q: 177.630 - reward: 8.073\n",
      "\n",
      "Interval 95 (940000 steps performed)\n",
      "10000/10000 [==============================] - 231s 23ms/step - reward: 7.3629\n",
      "400 episodes - episode_reward: 184.072 [26.700, 257.200] - loss: 592.346 - mae: 139.315 - mean_q: 176.850 - reward: 7.363\n",
      "\n",
      "Interval 96 (950000 steps performed)\n",
      "10000/10000 [==============================] - 234s 23ms/step - reward: 8.0266\n",
      "400 episodes - episode_reward: 200.666 [27.350, 257.200] - loss: 597.952 - mae: 139.885 - mean_q: 178.657 - reward: 8.027\n",
      "\n",
      "Interval 97 (960000 steps performed)\n",
      "10000/10000 [==============================] - 236s 24ms/step - reward: 7.8452\n",
      "400 episodes - episode_reward: 196.129 [25.650, 257.200] - loss: 607.053 - mae: 139.529 - mean_q: 178.878 - reward: 7.845\n",
      "\n",
      "Interval 98 (970000 steps performed)\n",
      "10000/10000 [==============================] - 239s 24ms/step - reward: 8.0659\n",
      "400 episodes - episode_reward: 201.648 [25.550, 257.200] - loss: 585.342 - mae: 138.487 - mean_q: 177.160 - reward: 8.066\n",
      "\n",
      "Interval 99 (980000 steps performed)\n",
      "10000/10000 [==============================] - 244s 24ms/step - reward: 7.8890\n",
      "400 episodes - episode_reward: 197.224 [25.050, 257.200] - loss: 606.330 - mae: 140.100 - mean_q: 179.309 - reward: 7.889\n",
      "\n",
      "Interval 100 (990000 steps performed)\n",
      "10000/10000 [==============================] - 247s 25ms/step - reward: 7.1918\n",
      "400 episodes - episode_reward: 179.796 [27.050, 257.200] - loss: 595.865 - mae: 140.156 - mean_q: 178.048 - reward: 7.192\n",
      "\n",
      "Interval 101 (1000000 steps performed)\n",
      "10000/10000 [==============================] - 248s 25ms/step - reward: 8.0971\n",
      "400 episodes - episode_reward: 202.428 [23.600, 257.200] - loss: 596.035 - mae: 139.589 - mean_q: 178.494 - reward: 8.097\n",
      "\n",
      "Interval 102 (1010000 steps performed)\n",
      "10000/10000 [==============================] - 253s 25ms/step - reward: 7.7933\n",
      "400 episodes - episode_reward: 194.834 [28.800, 257.200] - loss: 582.978 - mae: 138.975 - mean_q: 176.696 - reward: 7.793\n",
      "\n",
      "Interval 103 (1020000 steps performed)\n",
      "10000/10000 [==============================] - 256s 26ms/step - reward: 8.0188\n",
      "400 episodes - episode_reward: 200.470 [27.000, 257.200] - loss: 588.485 - mae: 140.223 - mean_q: 178.143 - reward: 8.019\n",
      "\n",
      "Interval 104 (1030000 steps performed)\n",
      "10000/10000 [==============================] - 258s 26ms/step - reward: 7.4827\n",
      "400 episodes - episode_reward: 187.067 [22.100, 257.200] - loss: 599.710 - mae: 140.610 - mean_q: 178.836 - reward: 7.483\n",
      "\n",
      "Interval 105 (1040000 steps performed)\n",
      "10000/10000 [==============================] - 262s 26ms/step - reward: 7.7219\n",
      "400 episodes - episode_reward: 193.046 [26.250, 257.200] - loss: 590.116 - mae: 140.037 - mean_q: 178.165 - reward: 7.722\n",
      "\n",
      "Interval 106 (1050000 steps performed)\n",
      "10000/10000 [==============================] - 263s 26ms/step - reward: 8.2598\n",
      "400 episodes - episode_reward: 206.495 [26.450, 257.200] - loss: 588.274 - mae: 138.142 - mean_q: 176.464 - reward: 8.260\n",
      "\n",
      "Interval 107 (1060000 steps performed)\n",
      "10000/10000 [==============================] - 266s 27ms/step - reward: 8.0772\n",
      "400 episodes - episode_reward: 201.930 [26.700, 257.200] - loss: 574.809 - mae: 137.979 - mean_q: 176.042 - reward: 8.077\n",
      "\n",
      "Interval 108 (1070000 steps performed)\n",
      "10000/10000 [==============================] - 270s 27ms/step - reward: 7.8131\n",
      "400 episodes - episode_reward: 195.327 [25.300, 257.200] - loss: 576.566 - mae: 138.193 - mean_q: 175.354 - reward: 7.813\n",
      "\n",
      "Interval 109 (1080000 steps performed)\n",
      "10000/10000 [==============================] - 273s 27ms/step - reward: 8.0586\n",
      "400 episodes - episode_reward: 201.464 [28.200, 257.200] - loss: 580.912 - mae: 138.733 - mean_q: 176.388 - reward: 8.059\n",
      "\n",
      "Interval 110 (1090000 steps performed)\n",
      "10000/10000 [==============================] - 276s 28ms/step - reward: 7.2692\n",
      "400 episodes - episode_reward: 181.729 [26.700, 257.200] - loss: 572.146 - mae: 138.447 - mean_q: 175.721 - reward: 7.269\n",
      "\n",
      "Interval 111 (1100000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: 7.5725\n",
      "400 episodes - episode_reward: 189.313 [26.850, 257.200] - loss: 562.094 - mae: 135.570 - mean_q: 172.064 - reward: 7.573\n",
      "\n",
      "Interval 112 (1110000 steps performed)\n",
      "10000/10000 [==============================] - 284s 28ms/step - reward: 7.8741\n",
      "400 episodes - episode_reward: 196.853 [24.950, 257.200] - loss: 584.407 - mae: 137.408 - mean_q: 175.334 - reward: 7.874\n",
      "\n",
      "Interval 113 (1120000 steps performed)\n",
      "10000/10000 [==============================] - 283s 28ms/step - reward: 7.7754\n",
      "400 episodes - episode_reward: 194.385 [24.900, 257.200] - loss: 575.162 - mae: 137.602 - mean_q: 175.851 - reward: 7.775\n",
      "\n",
      "Interval 114 (1130000 steps performed)\n",
      "10000/10000 [==============================] - 286s 29ms/step - reward: 8.0563\n",
      "400 episodes - episode_reward: 201.407 [18.150, 257.200] - loss: 599.761 - mae: 139.602 - mean_q: 177.690 - reward: 8.056\n",
      "\n",
      "Interval 115 (1140000 steps performed)\n",
      "10000/10000 [==============================] - 286s 29ms/step - reward: 8.4500\n",
      "400 episodes - episode_reward: 211.250 [26.050, 257.200] - loss: 591.603 - mae: 137.921 - mean_q: 176.143 - reward: 8.450\n",
      "\n",
      "Interval 116 (1150000 steps performed)\n",
      "10000/10000 [==============================] - 290s 29ms/step - reward: 8.1381\n",
      "400 episodes - episode_reward: 203.453 [23.900, 257.200] - loss: 600.017 - mae: 139.948 - mean_q: 178.571 - reward: 8.138\n",
      "\n",
      "Interval 117 (1160000 steps performed)\n",
      "10000/10000 [==============================] - 291s 29ms/step - reward: 8.3493\n",
      "400 episodes - episode_reward: 208.732 [21.150, 257.200] - loss: 580.455 - mae: 138.107 - mean_q: 176.420 - reward: 8.349\n",
      "\n",
      "Interval 118 (1170000 steps performed)\n",
      "10000/10000 [==============================] - 293s 29ms/step - reward: 8.4884\n",
      "400 episodes - episode_reward: 212.209 [28.250, 257.200] - loss: 610.898 - mae: 139.336 - mean_q: 178.251 - reward: 8.488\n",
      "\n",
      "Interval 119 (1180000 steps performed)\n",
      "10000/10000 [==============================] - 296s 30ms/step - reward: 8.3022\n",
      "400 episodes - episode_reward: 207.556 [26.800, 257.200] - loss: 614.817 - mae: 140.929 - mean_q: 180.017 - reward: 8.302\n",
      "\n",
      "Interval 120 (1190000 steps performed)\n",
      "10000/10000 [==============================] - 296s 30ms/step - reward: 7.8586\n",
      "400 episodes - episode_reward: 196.465 [27.900, 257.200] - loss: 606.880 - mae: 141.678 - mean_q: 179.771 - reward: 7.859\n",
      "\n",
      "Interval 121 (1200000 steps performed)\n",
      "10000/10000 [==============================] - 300s 30ms/step - reward: 8.1433\n",
      "400 episodes - episode_reward: 203.583 [28.350, 257.200] - loss: 613.441 - mae: 142.340 - mean_q: 180.869 - reward: 8.143\n",
      "\n",
      "Interval 122 (1210000 steps performed)\n",
      "10000/10000 [==============================] - 305s 31ms/step - reward: 8.1145\n",
      "400 episodes - episode_reward: 202.863 [28.650, 257.200] - loss: 631.452 - mae: 143.859 - mean_q: 183.456 - reward: 8.115\n",
      "\n",
      "Interval 123 (1220000 steps performed)\n",
      "10000/10000 [==============================] - 306s 31ms/step - reward: 8.4346\n",
      "400 episodes - episode_reward: 210.866 [29.700, 257.200] - loss: 638.321 - mae: 143.598 - mean_q: 184.195 - reward: 8.435\n",
      "\n",
      "Interval 124 (1230000 steps performed)\n",
      "10000/10000 [==============================] - 307s 31ms/step - reward: 7.7740\n",
      "400 episodes - episode_reward: 194.349 [28.600, 257.200] - loss: 630.396 - mae: 144.032 - mean_q: 182.547 - reward: 7.774\n",
      "\n",
      "Interval 125 (1240000 steps performed)\n",
      "10000/10000 [==============================] - 307s 31ms/step - reward: 7.4325\n",
      "400 episodes - episode_reward: 185.812 [28.200, 257.200] - loss: 622.753 - mae: 142.989 - mean_q: 180.944 - reward: 7.432\n",
      "\n",
      "Interval 126 (1250000 steps performed)\n",
      "10000/10000 [==============================] - 334s 33ms/step - reward: 7.4412\n",
      "400 episodes - episode_reward: 186.031 [28.450, 257.200] - loss: 603.348 - mae: 142.124 - mean_q: 180.059 - reward: 7.441\n",
      "\n",
      "Interval 127 (1260000 steps performed)\n",
      "10000/10000 [==============================] - 325s 32ms/step - reward: 7.2892\n",
      "400 episodes - episode_reward: 182.230 [26.700, 257.200] - loss: 618.166 - mae: 141.636 - mean_q: 180.394 - reward: 7.289\n",
      "\n",
      "Interval 128 (1270000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 7.8456\n",
      "400 episodes - episode_reward: 196.140 [28.850, 257.200] - loss: 624.942 - mae: 141.668 - mean_q: 181.254 - reward: 7.846\n",
      "\n",
      "Interval 129 (1280000 steps performed)\n",
      "10000/10000 [==============================] - 320s 32ms/step - reward: 7.7167\n",
      "400 episodes - episode_reward: 192.917 [19.650, 257.200] - loss: 611.432 - mae: 141.425 - mean_q: 179.869 - reward: 7.717\n",
      "\n",
      "Interval 130 (1290000 steps performed)\n",
      "10000/10000 [==============================] - 329s 33ms/step - reward: 7.7530\n",
      "done, took 23067.000 seconds\n"
     ]
    }
   ],
   "source": [
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n\n",
    "model = build_model(states, actions)\n",
    "dqn = build_agent(model, actions)\n",
    "\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=1300000, visualize=False, verbose=1)\n",
    "dqn.save_weights(\"dqn_fixed_number_6random_nowarm_1300k\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = A2C('MlpPolicy', 'fixed_number:fixed_number-v1').learn(4000)\n",
    "model.save(\"a2c_fixed_number_eolfunction_400k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = A2C.load(\"a2c_treasure_flat_distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DQNAgent' object has no attribute 'target_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-570bdf8fd4d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdqn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dqn_fixed_number_eolfunction_500k'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/rl/agents/dqn.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_target_model_hard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/rl/agents/dqn.py\u001b[0m in \u001b[0;36mupdate_target_model_hard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_target_model_hard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DQNAgent' object has no attribute 'target_model'"
     ]
    }
   ],
   "source": [
    "model = build_model(states, actions)\n",
    "dqn = build_agent(model, actions)\n",
    "\n",
    "dqn.load_weights('dqn_fixed_number_eolfunction_500k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 20 episodes ...\n",
      "state=[8 6 6 5]\n",
      "state=[8 5 6 5]\n",
      "state=[7 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "Episode 1: reward: 257.200, steps: 25\n",
      "state=[ 9  7 10  5]\n",
      "state=[ 9  6 10  5]\n",
      "state=[10  6 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "Episode 2: reward: 257.200, steps: 25\n",
      "state=[ 9  7 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "Episode 3: reward: 28.800, steps: 25\n",
      "state=[ 9  7 10  9]\n",
      "state=[10  7 10  9]\n",
      "state=[10  8 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "Episode 4: reward: 257.200, steps: 25\n",
      "state=[ 9  7 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "Episode 5: reward: 28.800, steps: 25\n",
      "state=[ 9  7 10  9]\n",
      "state=[10  7 10  9]\n",
      "state=[10  8 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "Episode 6: reward: 257.200, steps: 25\n",
      "state=[ 9  7 10  5]\n",
      "state=[ 9  6 10  5]\n",
      "state=[10  6 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "Episode 7: reward: 257.200, steps: 25\n",
      "state=[8 6 6 5]\n",
      "state=[8 5 6 5]\n",
      "state=[7 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "Episode 8: reward: 257.200, steps: 25\n",
      "state=[ 9  7 10  5]\n",
      "state=[ 9  6 10  5]\n",
      "state=[10  6 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "Episode 9: reward: 257.200, steps: 25\n",
      "state=[8 6 6 5]\n",
      "state=[8 5 6 5]\n",
      "state=[7 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "Episode 10: reward: 257.200, steps: 25\n",
      "state=[8 6 3 3]\n",
      "state=[8 5 3 3]\n",
      "state=[8 4 3 3]\n",
      "state=[7 4 3 3]\n",
      "state=[6 4 3 3]\n",
      "state=[6 3 3 3]\n",
      "state=[5 3 3 3]\n",
      "state=[4 3 3 3]\n",
      "state=[3 3 3 3]\n",
      "state=[3 3 3 3]\n",
      "state=[3 3 3 3]\n",
      "state=[3 3 3 3]\n",
      "state=[3 3 3 3]\n",
      "state=[3 3 3 3]\n",
      "state=[3 3 3 3]\n",
      "state=[3 3 3 3]\n",
      "state=[3 3 3 3]\n",
      "state=[3 3 3 3]\n",
      "state=[3 3 3 3]\n",
      "state=[3 3 3 3]\n",
      "state=[3 3 3 3]\n",
      "state=[3 3 3 3]\n",
      "state=[3 3 3 3]\n",
      "state=[3 3 3 3]\n",
      "state=[3 3 3 3]\n",
      "Episode 11: reward: 205.700, steps: 25\n",
      "state=[8 6 6 5]\n",
      "state=[8 5 6 5]\n",
      "state=[7 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "Episode 12: reward: 257.200, steps: 25\n",
      "state=[ 9  7 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "state=[ 9  6 11 11]\n",
      "Episode 13: reward: 28.800, steps: 25\n",
      "state=[8 6 6 5]\n",
      "state=[8 5 6 5]\n",
      "state=[7 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "state=[6 5 6 5]\n",
      "Episode 14: reward: 257.200, steps: 25\n",
      "state=[ 9  7 10  9]\n",
      "state=[10  7 10  9]\n",
      "state=[10  8 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "Episode 15: reward: 257.200, steps: 25\n",
      "state=[8 6 3 3]\n",
      "state=[8 5 3 3]\n",
      "state=[8 4 3 3]\n",
      "state=[7 4 3 3]\n",
      "state=[6 4 3 3]\n",
      "state=[6 3 3 3]\n",
      "state=[5 3 3 3]\n",
      "state=[4 3 3 3]\n",
      "state=[3 3 3 3]\n",
      "state=[3 3 3 3]\n",
      "state=[3 3 3 3]\n",
      "state=[3 3 3 3]\n",
      "state=[3 3 3 3]\n",
      "state=[3 3 3 3]\n",
      "state=[3 3 3 3]\n",
      "state=[3 3 3 3]\n",
      "state=[3 3 3 3]\n",
      "state=[3 3 3 3]\n",
      "state=[3 3 3 3]\n",
      "state=[3 3 3 3]\n",
      "state=[3 3 3 3]\n",
      "state=[3 3 3 3]\n",
      "state=[3 3 3 3]\n",
      "state=[3 3 3 3]\n",
      "state=[3 3 3 3]\n",
      "Episode 16: reward: 205.700, steps: 25\n",
      "state=[ 9  7 10  9]\n",
      "state=[10  7 10  9]\n",
      "state=[10  8 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "Episode 17: reward: 257.200, steps: 25\n",
      "state=[ 9  7 10  5]\n",
      "state=[ 9  6 10  5]\n",
      "state=[10  6 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "state=[10  5 10  5]\n",
      "Episode 18: reward: 257.200, steps: 25\n",
      "state=[ 9  7 10  9]\n",
      "state=[10  7 10  9]\n",
      "state=[10  8 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "state=[10  9 10  9]\n",
      "Episode 19: reward: 257.200, steps: 25\n",
      "state=[8 6 3 3]\n",
      "state=[8 5 3 3]\n",
      "state=[8 4 3 3]\n",
      "state=[7 4 3 3]\n",
      "state=[6 4 3 3]\n",
      "state=[6 3 3 3]\n",
      "state=[5 3 3 3]\n",
      "state=[4 3 3 3]\n",
      "state=[3 3 3 3]\n",
      "state=[3 3 3 3]\n",
      "state=[3 3 3 3]\n",
      "state=[3 3 3 3]\n",
      "state=[3 3 3 3]\n",
      "state=[3 3 3 3]\n",
      "state=[3 3 3 3]\n",
      "state=[3 3 3 3]\n",
      "state=[3 3 3 3]\n",
      "state=[3 3 3 3]\n",
      "state=[3 3 3 3]\n",
      "state=[3 3 3 3]\n",
      "state=[3 3 3 3]\n",
      "state=[3 3 3 3]\n",
      "state=[3 3 3 3]\n",
      "state=[3 3 3 3]\n",
      "state=[3 3 3 3]\n",
      "Episode 20: reward: 205.700, steps: 25\n",
      "{'episode_reward': [257.2, 257.2, 28.799999999999986, 257.2, 28.799999999999986, 257.2, 257.2, 257.2, 257.2, 257.2, 205.7, 257.2, 28.799999999999986, 257.2, 257.2, 205.7, 257.2, 257.2, 257.2, 205.7], 'nb_steps': [25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25]}\n"
     ]
    }
   ],
   "source": [
    "scores = dqn.test(env, nb_episodes=20, visualize=True)\n",
    "#print(np.mean(scores.history['episode_reward']))\n",
    "#dir(dqn)\n",
    "print(scores.history)\n",
    "#dir(scores)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "start obs [8 7 6 9]\n",
      "s:None r:1.35 o:[8 8 6 9] a:2\n",
      "state=[8 8 6 9]\n",
      "s:None r:1.4 o:[8 9 6 9] a:2\n",
      "state=[8 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:3\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "\n",
      "start obs [ 8  7 10  5]\n",
      "s:None r:1.3 o:[ 8  7 10  5] a:4\n",
      "state=[ 8  7 10  5]\n",
      "s:None r:1.3 o:[ 8  7 10  5] a:4\n",
      "state=[ 8  7 10  5]\n",
      "s:None r:1.3 o:[ 8  7 10  5] a:4\n",
      "state=[ 8  7 10  5]\n",
      "s:None r:1.3 o:[ 8  7 10  5] a:4\n",
      "state=[ 8  7 10  5]\n",
      "s:None r:1.3 o:[ 8  7 10  5] a:4\n",
      "state=[ 8  7 10  5]\n",
      "s:None r:1.3 o:[ 8  7 10  5] a:4\n",
      "state=[ 8  7 10  5]\n",
      "s:None r:1.3 o:[ 8  7 10  5] a:4\n",
      "state=[ 8  7 10  5]\n",
      "s:None r:1.3 o:[ 8  7 10  5] a:4\n",
      "state=[ 8  7 10  5]\n",
      "s:None r:1.3 o:[ 8  7 10  5] a:4\n",
      "state=[ 8  7 10  5]\n",
      "s:None r:1.3 o:[ 8  7 10  5] a:4\n",
      "state=[ 8  7 10  5]\n",
      "s:None r:1.3 o:[ 8  7 10  5] a:4\n",
      "state=[ 8  7 10  5]\n",
      "s:None r:1.3 o:[ 8  7 10  5] a:4\n",
      "state=[ 8  7 10  5]\n",
      "s:None r:1.3 o:[ 8  7 10  5] a:4\n",
      "state=[ 8  7 10  5]\n",
      "s:None r:1.3 o:[ 8  7 10  5] a:4\n",
      "state=[ 8  7 10  5]\n",
      "s:None r:1.3 o:[ 8  7 10  5] a:4\n",
      "state=[ 8  7 10  5]\n",
      "s:None r:1.3 o:[ 8  7 10  5] a:4\n",
      "state=[ 8  7 10  5]\n",
      "s:None r:1.3 o:[ 8  7 10  5] a:4\n",
      "state=[ 8  7 10  5]\n",
      "s:None r:1.3 o:[ 8  7 10  5] a:4\n",
      "state=[ 8  7 10  5]\n",
      "s:None r:1.3 o:[ 8  7 10  5] a:4\n",
      "state=[ 8  7 10  5]\n",
      "s:None r:1.3 o:[ 8  7 10  5] a:4\n",
      "state=[ 8  7 10  5]\n",
      "s:None r:1.3 o:[ 8  7 10  5] a:4\n",
      "state=[ 8  7 10  5]\n",
      "s:None r:1.3 o:[ 8  7 10  5] a:4\n",
      "state=[ 8  7 10  5]\n",
      "s:None r:1.3 o:[ 8  7 10  5] a:4\n",
      "state=[ 8  7 10  5]\n",
      "s:None r:1.3 o:[ 8  7 10  5] a:4\n",
      "state=[ 8  7 10  5]\n",
      "s:None r:1.3 o:[ 8  7 10  5] a:4\n",
      "\n",
      "start obs [ 8  7 11 11]\n",
      "s:None r:1.2000000000000002 o:[ 8  8 11 11] a:2\n",
      "state=[ 8  8 11 11]\n",
      "s:None r:1.25 o:[ 8  9 11 11] a:2\n",
      "state=[ 8  9 11 11]\n",
      "s:None r:1.3000000000000003 o:[ 8 10 11 11] a:2\n",
      "state=[ 8 10 11 11]\n",
      "s:None r:1.35 o:[ 9 10 11 11] a:1\n",
      "state=[ 9 10 11 11]\n",
      "s:None r:1.4 o:[ 9 11 11 11] a:2\n",
      "state=[ 9 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:1\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "\n",
      "start obs [ 8  7 10  9]\n",
      "s:None r:1.35 o:[ 8  8 10  9] a:2\n",
      "state=[ 8  8 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:2\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:1\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "\n",
      "start obs [ 8  7 11 11]\n",
      "s:None r:1.2000000000000002 o:[ 8  8 11 11] a:2\n",
      "state=[ 8  8 11 11]\n",
      "s:None r:1.25 o:[ 8  9 11 11] a:2\n",
      "state=[ 8  9 11 11]\n",
      "s:None r:1.3000000000000003 o:[ 8 10 11 11] a:2\n",
      "state=[ 8 10 11 11]\n",
      "s:None r:1.35 o:[ 9 10 11 11] a:1\n",
      "state=[ 9 10 11 11]\n",
      "s:None r:1.4 o:[ 9 11 11 11] a:2\n",
      "state=[ 9 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:1\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "\n",
      "start obs [ 8  7 11 11]\n",
      "s:None r:1.2000000000000002 o:[ 8  8 11 11] a:2\n",
      "state=[ 8  8 11 11]\n",
      "s:None r:1.25 o:[ 8  9 11 11] a:2\n",
      "state=[ 8  9 11 11]\n",
      "s:None r:1.3000000000000003 o:[ 8 10 11 11] a:2\n",
      "state=[ 8 10 11 11]\n",
      "s:None r:1.35 o:[ 9 10 11 11] a:1\n",
      "state=[ 9 10 11 11]\n",
      "s:None r:1.4 o:[ 9 11 11 11] a:2\n",
      "state=[ 9 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:1\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "\n",
      "start obs [8 7 6 9]\n",
      "s:None r:1.35 o:[8 8 6 9] a:2\n",
      "state=[8 8 6 9]\n",
      "s:None r:1.4 o:[8 9 6 9] a:2\n",
      "state=[8 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:3\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "\n",
      "start obs [8 7 3 3]\n",
      "s:None r:1.1 o:[7 7 3 3] a:3\n",
      "state=[7 7 3 3]\n",
      "s:None r:1.1500000000000001 o:[6 7 3 3] a:3\n",
      "state=[6 7 3 3]\n",
      "s:None r:1.2000000000000002 o:[5 7 3 3] a:3\n",
      "state=[5 7 3 3]\n",
      "s:None r:1.25 o:[4 7 3 3] a:3\n",
      "state=[4 7 3 3]\n",
      "s:None r:1.3 o:[3 7 3 3] a:3\n",
      "state=[3 7 3 3]\n",
      "s:None r:1.25 o:[2 7 3 3] a:3\n",
      "state=[2 7 3 3]\n",
      "s:None r:1.3000000000000003 o:[2 6 3 3] a:0\n",
      "state=[2 6 3 3]\n",
      "s:None r:1.35 o:[2 5 3 3] a:0\n",
      "state=[2 5 3 3]\n",
      "s:None r:1.4000000000000001 o:[2 4 3 3] a:0\n",
      "state=[2 4 3 3]\n",
      "s:None r:1.4000000000000001 o:[2 4 3 3] a:4\n",
      "state=[2 4 3 3]\n",
      "s:None r:1.4000000000000001 o:[2 4 3 3] a:4\n",
      "state=[2 4 3 3]\n",
      "s:None r:1.4000000000000001 o:[2 4 3 3] a:4\n",
      "state=[2 4 3 3]\n",
      "s:None r:1.4000000000000001 o:[2 4 3 3] a:4\n",
      "state=[2 4 3 3]\n",
      "s:None r:1.4000000000000001 o:[2 4 3 3] a:4\n",
      "state=[2 4 3 3]\n",
      "s:None r:1.4000000000000001 o:[2 4 3 3] a:4\n",
      "state=[2 4 3 3]\n",
      "s:None r:1.4000000000000001 o:[2 4 3 3] a:4\n",
      "state=[2 4 3 3]\n",
      "s:None r:1.4000000000000001 o:[2 4 3 3] a:4\n",
      "state=[2 4 3 3]\n",
      "s:None r:1.4000000000000001 o:[2 4 3 3] a:4\n",
      "state=[2 4 3 3]\n",
      "s:None r:1.4000000000000001 o:[2 4 3 3] a:4\n",
      "state=[2 4 3 3]\n",
      "s:None r:1.4000000000000001 o:[2 4 3 3] a:4\n",
      "state=[2 4 3 3]\n",
      "s:None r:1.4000000000000001 o:[2 4 3 3] a:4\n",
      "state=[2 4 3 3]\n",
      "s:None r:1.4000000000000001 o:[2 4 3 3] a:4\n",
      "state=[2 4 3 3]\n",
      "s:None r:1.4000000000000001 o:[2 4 3 3] a:4\n",
      "state=[2 4 3 3]\n",
      "s:None r:1.4000000000000001 o:[2 4 3 3] a:4\n",
      "state=[2 4 3 3]\n",
      "s:None r:1.4000000000000001 o:[2 4 3 3] a:4\n",
      "\n",
      "start obs [ 8  7 10  9]\n",
      "s:None r:1.35 o:[ 8  8 10  9] a:2\n",
      "state=[ 8  8 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:2\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:1\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "\n",
      "start obs [ 8  7 10  9]\n",
      "s:None r:1.35 o:[ 8  8 10  9] a:2\n",
      "state=[ 8  8 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:2\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:1\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "\n",
      "start obs [ 8  7 11 11]\n",
      "s:None r:1.2000000000000002 o:[ 8  8 11 11] a:2\n",
      "state=[ 8  8 11 11]\n",
      "s:None r:1.25 o:[ 8  9 11 11] a:2\n",
      "state=[ 8  9 11 11]\n",
      "s:None r:1.3000000000000003 o:[ 8 10 11 11] a:2\n",
      "state=[ 8 10 11 11]\n",
      "s:None r:1.35 o:[ 9 10 11 11] a:1\n",
      "state=[ 9 10 11 11]\n",
      "s:None r:1.4 o:[ 9 11 11 11] a:2\n",
      "state=[ 9 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:1\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "state=[10 11 11 11]\n",
      "s:None r:1.4500000000000002 o:[10 11 11 11] a:4\n",
      "\n",
      "start obs [ 8  7 10  9]\n",
      "s:None r:1.35 o:[ 8  8 10  9] a:2\n",
      "state=[ 8  8 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:2\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:1\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "\n",
      "start obs [ 8  7 10  9]\n",
      "s:None r:1.35 o:[ 8  8 10  9] a:2\n",
      "state=[ 8  8 10  9]\n",
      "s:None r:1.4 o:[ 8  9 10  9] a:2\n",
      "state=[ 8  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:1\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "state=[ 9  9 10  9]\n",
      "s:None r:1.4500000000000002 o:[ 9  9 10  9] a:4\n",
      "\n",
      "start obs [8 7 6 9]\n",
      "s:None r:1.35 o:[8 8 6 9] a:2\n",
      "state=[8 8 6 9]\n",
      "s:None r:1.4 o:[8 9 6 9] a:2\n",
      "state=[8 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:3\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "state=[7 9 6 9]\n",
      "s:None r:1.4500000000000002 o:[7 9 6 9] a:4\n",
      "\n",
      "start obs [ 8  7 10  5]\n",
      "s:None r:1.3 o:[ 8  7 10  5] a:4\n",
      "state=[ 8  7 10  5]\n",
      "s:None r:1.3 o:[ 8  7 10  5] a:4\n",
      "state=[ 8  7 10  5]\n",
      "s:None r:1.3 o:[ 8  7 10  5] a:4\n",
      "state=[ 8  7 10  5]\n",
      "s:None r:1.3 o:[ 8  7 10  5] a:4\n",
      "state=[ 8  7 10  5]\n",
      "s:None r:1.3 o:[ 8  7 10  5] a:4\n",
      "state=[ 8  7 10  5]\n",
      "s:None r:1.3 o:[ 8  7 10  5] a:4\n",
      "state=[ 8  7 10  5]\n",
      "s:None r:1.3 o:[ 8  7 10  5] a:4\n",
      "state=[ 8  7 10  5]\n",
      "s:None r:1.3 o:[ 8  7 10  5] a:4\n",
      "state=[ 8  7 10  5]\n",
      "s:None r:1.3 o:[ 8  7 10  5] a:4\n",
      "state=[ 8  7 10  5]\n",
      "s:None r:1.3 o:[ 8  7 10  5] a:4\n",
      "state=[ 8  7 10  5]\n",
      "s:None r:1.3 o:[ 8  7 10  5] a:4\n",
      "state=[ 8  7 10  5]\n",
      "s:None r:1.3 o:[ 8  7 10  5] a:4\n",
      "state=[ 8  7 10  5]\n",
      "s:None r:1.3 o:[ 8  7 10  5] a:4\n",
      "state=[ 8  7 10  5]\n",
      "s:None r:1.3 o:[ 8  7 10  5] a:4\n",
      "state=[ 8  7 10  5]\n",
      "s:None r:1.3 o:[ 8  7 10  5] a:4\n",
      "state=[ 8  7 10  5]\n",
      "s:None r:1.3 o:[ 8  7 10  5] a:4\n",
      "state=[ 8  7 10  5]\n",
      "s:None r:1.3 o:[ 8  7 10  5] a:4\n",
      "state=[ 8  7 10  5]\n",
      "s:None r:1.3 o:[ 8  7 10  5] a:4\n",
      "state=[ 8  7 10  5]\n",
      "s:None r:1.3 o:[ 8  7 10  5] a:4\n",
      "state=[ 8  7 10  5]\n",
      "s:None r:1.3 o:[ 8  7 10  5] a:4\n",
      "state=[ 8  7 10  5]\n",
      "s:None r:1.3 o:[ 8  7 10  5] a:4\n",
      "state=[ 8  7 10  5]\n",
      "s:None r:1.3 o:[ 8  7 10  5] a:4\n",
      "state=[ 8  7 10  5]\n",
      "s:None r:1.3 o:[ 8  7 10  5] a:4\n",
      "state=[ 8  7 10  5]\n",
      "s:None r:1.3 o:[ 8  7 10  5] a:4\n",
      "state=[ 8  7 10  5]\n",
      "s:None r:1.3 o:[ 8  7 10  5] a:4\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('random_treasure_flat:random_treasure_flat-v1')\n",
    "\n",
    "for i in range(15):\n",
    "    obs = env.reset()\n",
    "    print(\"\\nstart \" + 'obs ' + str(obs))\n",
    "\n",
    "    while True:\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, rewards, dones, info = env.step(action)\n",
    "        print(\"s:\" + str(_states) + \" r:\" + str(rewards) + \" o:\" + str(obs) + \" a:\" + str(action))        \n",
    "        if dones:\n",
    "            break\n",
    "        env.render()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
