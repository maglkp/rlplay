{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.cem import CEMAgent\n",
    "from rl.agents.sarsa import SARSAAgent\n",
    "from rl.memory import EpisodeParameterMemory\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import LinearAnnealedPolicy\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from gym import envs\n",
    "#print(envs.registry.all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n",
      "Box(4,)\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)\n",
    "#> Discrete(2)\n",
    "print(env.observation_space)\n",
    "#> Box(4,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_3 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 2)                 34        \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 658\n",
      "Trainable params: 658\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "ENV_NAME = 'CartPole-v0'\n",
    "\n",
    "\n",
    "# Get the environment and extract the number of actions.\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "\n",
    "nb_actions = env.action_space.n\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "\n",
    "# Option 1 : Simple model\n",
    "# model = Sequential()\n",
    "# model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "# model.add(Dense(nb_actions))\n",
    "# model.add(Activation('softmax'))\n",
    "\n",
    "# Option 2: deep network\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SARSAAgent\n",
    "\n",
    "sarsa\n",
    "__init__(self, model, nb_actions, policy=None, test_policy=None, gamma=.99, nb_steps_warmup=10, train_interval=1, delta_clip=np.inf, *args, **kwargs)\n",
    "cem\n",
    "__init__(self, model, nb_actions, memory, batch_size=50, nb_steps_warmup=1000, train_interval=50, elite_frac=0.05, memory_interval=1, theta_init=None, noise_decay_const=0.0, noise_ampl=0.0, **kwargs)        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sars = SARSAAgent(model=model, nb_actions=nb_actions, nb_steps_warmup=1000, train_interval=50)\n",
    "sars.compile(optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sars.fit(env, nb_steps=500000, visualize=False, verbose=1)\n",
    "# After training is done, we save the best weights.\n",
    "#sars.save_weights(f'sars_{ENV_NAME}_params.h5f', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we configure and compile our agent. You can use every built-in tensorflow.keras optimizer and\n",
    "# even the metrics!\n",
    "memory = EpisodeParameterMemory(limit=2000, window_length=1)\n",
    "\n",
    "cem = CEMAgent(model=model, nb_actions=nb_actions, memory=memory,\n",
    "               batch_size=50, nb_steps_warmup=2000, train_interval=50, elite_frac=0.05)\n",
    "cem.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 2000000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 10s 972us/step - reward: 1.0000\n",
      "485 episodes - episode_reward: 20.464 [8.000, 147.000] - mean_best_reward: 49.286\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 9s 888us/step - reward: 1.0000\n",
      "256 episodes - episode_reward: 39.328 [8.000, 200.000] - mean_best_reward: 98.300\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 9s 867us/step - reward: 1.0000\n",
      "202 episodes - episode_reward: 49.490 [9.000, 200.000] - mean_best_reward: 124.875\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 9s 854us/step - reward: 1.0000\n",
      "147 episodes - episode_reward: 67.585 [9.000, 193.000] - mean_best_reward: 147.500\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 9s 850us/step - reward: 1.0000\n",
      "169 episodes - episode_reward: 58.964 [9.000, 200.000] - mean_best_reward: 141.375\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 9s 853us/step - reward: 1.0000\n",
      "131 episodes - episode_reward: 77.023 [9.000, 200.000] - mean_best_reward: 134.500\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 9s 851us/step - reward: 1.0000\n",
      "139 episodes - episode_reward: 72.029 [14.000, 200.000] - mean_best_reward: 142.833\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 9s 852us/step - reward: 1.0000\n",
      "146 episodes - episode_reward: 68.295 [11.000, 164.000] - mean_best_reward: 146.667\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 8s 833us/step - reward: 1.0000\n",
      "141 episodes - episode_reward: 71.050 [9.000, 187.000] - mean_best_reward: 142.667\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 8s 839us/step - reward: 1.0000\n",
      "145 episodes - episode_reward: 69.062 [12.000, 200.000] - mean_best_reward: 131.167\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 8s 823us/step - reward: 1.0000\n",
      "122 episodes - episode_reward: 80.975 [10.000, 200.000] - mean_best_reward: 145.000\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 8s 835us/step - reward: 1.0000\n",
      "136 episodes - episode_reward: 74.441 [11.000, 200.000] - mean_best_reward: 162.833\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 8s 834us/step - reward: 1.0000\n",
      "134 episodes - episode_reward: 74.060 [12.000, 200.000] - mean_best_reward: 184.500\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 8s 831us/step - reward: 1.0000\n",
      "141 episodes - episode_reward: 71.043 [10.000, 200.000] - mean_best_reward: 178.750\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 8s 850us/step - reward: 1.0000\n",
      "157 episodes - episode_reward: 63.344 [11.000, 181.000] - mean_best_reward: 170.625\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 8s 831us/step - reward: 1.0000\n",
      "135 episodes - episode_reward: 74.081 [12.000, 200.000] - mean_best_reward: 177.750\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 8s 840us/step - reward: 1.0000\n",
      "136 episodes - episode_reward: 74.140 [11.000, 200.000] - mean_best_reward: 184.667\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 8s 821us/step - reward: 1.0000\n",
      "110 episodes - episode_reward: 90.336 [9.000, 200.000] - mean_best_reward: 190.250\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 8s 827us/step - reward: 1.0000\n",
      "121 episodes - episode_reward: 82.570 [14.000, 200.000] - mean_best_reward: 187.000\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 8s 832us/step - reward: 1.0000\n",
      "135 episodes - episode_reward: 74.644 [10.000, 200.000] - mean_best_reward: 174.750\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      "10000/10000 [==============================] - 8s 818us/step - reward: 1.0000\n",
      "128 episodes - episode_reward: 78.062 [10.000, 200.000] - mean_best_reward: 166.500\n",
      "\n",
      "Interval 22 (210000 steps performed)\n",
      "10000/10000 [==============================] - 8s 828us/step - reward: 1.0000\n",
      "130 episodes - episode_reward: 76.223 [12.000, 200.000] - mean_best_reward: 174.000\n",
      "\n",
      "Interval 23 (220000 steps performed)\n",
      "10000/10000 [==============================] - 8s 826us/step - reward: 1.0000\n",
      "118 episodes - episode_reward: 85.076 [12.000, 200.000] - mean_best_reward: 184.500\n",
      "\n",
      "Interval 24 (230000 steps performed)\n",
      "10000/10000 [==============================] - 8s 817us/step - reward: 1.0000\n",
      "110 episodes - episode_reward: 91.582 [13.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 25 (240000 steps performed)\n",
      "10000/10000 [==============================] - 8s 827us/step - reward: 1.0000\n",
      "123 episodes - episode_reward: 80.130 [13.000, 200.000] - mean_best_reward: 196.750\n",
      "\n",
      "Interval 26 (250000 steps performed)\n",
      "10000/10000 [==============================] - 8s 822us/step - reward: 1.0000\n",
      "117 episodes - episode_reward: 86.624 [18.000, 200.000] - mean_best_reward: 190.000\n",
      "\n",
      "Interval 27 (260000 steps performed)\n",
      "10000/10000 [==============================] - 8s 830us/step - reward: 1.0000\n",
      "126 episodes - episode_reward: 79.302 [15.000, 200.000] - mean_best_reward: 185.250\n",
      "\n",
      "Interval 28 (270000 steps performed)\n",
      "10000/10000 [==============================] - 8s 814us/step - reward: 1.0000\n",
      "109 episodes - episode_reward: 91.954 [12.000, 200.000] - mean_best_reward: 190.250\n",
      "\n",
      "Interval 29 (280000 steps performed)\n",
      "10000/10000 [==============================] - 8s 814us/step - reward: 1.0000\n",
      "114 episodes - episode_reward: 86.693 [11.000, 200.000] - mean_best_reward: 199.500\n",
      "\n",
      "Interval 30 (290000 steps performed)\n",
      "10000/10000 [==============================] - 8s 817us/step - reward: 1.0000\n",
      "114 episodes - episode_reward: 88.088 [11.000, 200.000] - mean_best_reward: 190.250\n",
      "\n",
      "Interval 31 (300000 steps performed)\n",
      "10000/10000 [==============================] - 8s 813us/step - reward: 1.0000\n",
      "118 episodes - episode_reward: 84.703 [12.000, 200.000] - mean_best_reward: 196.250\n",
      "\n",
      "Interval 32 (310000 steps performed)\n",
      "10000/10000 [==============================] - 8s 810us/step - reward: 1.0000\n",
      "116 episodes - episode_reward: 86.862 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 33 (320000 steps performed)\n",
      "10000/10000 [==============================] - 8s 819us/step - reward: 1.0000\n",
      "109 episodes - episode_reward: 91.294 [13.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 34 (330000 steps performed)\n",
      "10000/10000 [==============================] - 8s 817us/step - reward: 1.0000\n",
      "117 episodes - episode_reward: 85.949 [12.000, 200.000] - mean_best_reward: 198.500\n",
      "\n",
      "Interval 35 (340000 steps performed)\n",
      "10000/10000 [==============================] - 8s 814us/step - reward: 1.0000\n",
      "116 episodes - episode_reward: 85.638 [15.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 36 (350000 steps performed)\n",
      "10000/10000 [==============================] - 8s 812us/step - reward: 1.0000\n",
      "105 episodes - episode_reward: 94.676 [10.000, 200.000] - mean_best_reward: 193.750\n",
      "\n",
      "Interval 37 (360000 steps performed)\n",
      "10000/10000 [==============================] - 8s 814us/step - reward: 1.0000\n",
      "118 episodes - episode_reward: 85.610 [13.000, 200.000] - mean_best_reward: 193.000\n",
      "\n",
      "Interval 38 (370000 steps performed)\n",
      "10000/10000 [==============================] - 8s 808us/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 100.939 [9.000, 200.000] - mean_best_reward: 197.500\n",
      "\n",
      "Interval 39 (380000 steps performed)\n",
      "10000/10000 [==============================] - 8s 807us/step - reward: 1.0000\n",
      "106 episodes - episode_reward: 94.377 [15.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 40 (390000 steps performed)\n",
      "10000/10000 [==============================] - 8s 806us/step - reward: 1.0000\n",
      "110 episodes - episode_reward: 91.109 [14.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 41 (400000 steps performed)\n",
      "10000/10000 [==============================] - 8s 813us/step - reward: 1.0000\n",
      "102 episodes - episode_reward: 97.745 [13.000, 200.000] - mean_best_reward: 196.500\n",
      "\n",
      "Interval 42 (410000 steps performed)\n",
      "10000/10000 [==============================] - 8s 816us/step - reward: 1.0000\n",
      "106 episodes - episode_reward: 94.358 [13.000, 200.000] - mean_best_reward: 197.250\n",
      "\n",
      "Interval 43 (420000 steps performed)\n",
      "10000/10000 [==============================] - 8s 807us/step - reward: 1.0000\n",
      "117 episodes - episode_reward: 85.017 [13.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 44 (430000 steps performed)\n",
      "10000/10000 [==============================] - 8s 801us/step - reward: 1.0000\n",
      "108 episodes - episode_reward: 92.444 [18.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 45 (440000 steps performed)\n",
      "10000/10000 [==============================] - 8s 822us/step - reward: 1.0000\n",
      "126 episodes - episode_reward: 79.937 [15.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 46 (450000 steps performed)\n",
      "10000/10000 [==============================] - 8s 810us/step - reward: 1.0000\n",
      "106 episodes - episode_reward: 94.094 [9.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 47 (460000 steps performed)\n",
      "10000/10000 [==============================] - 8s 800us/step - reward: 1.0000\n",
      "106 episodes - episode_reward: 94.113 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 48 (470000 steps performed)\n",
      "10000/10000 [==============================] - 8s 804us/step - reward: 1.0000\n",
      "104 episodes - episode_reward: 96.577 [13.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 49 (480000 steps performed)\n",
      "10000/10000 [==============================] - 8s 799us/step - reward: 1.0000\n",
      "101 episodes - episode_reward: 98.366 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 50 (490000 steps performed)\n",
      "10000/10000 [==============================] - 8s 805us/step - reward: 1.0000\n",
      "107 episodes - episode_reward: 93.953 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 51 (500000 steps performed)\n",
      "10000/10000 [==============================] - 8s 798us/step - reward: 1.0000\n",
      "106 episodes - episode_reward: 94.425 [11.000, 200.000] - mean_best_reward: 198.500\n",
      "\n",
      "Interval 52 (510000 steps performed)\n",
      "10000/10000 [==============================] - 8s 798us/step - reward: 1.0000\n",
      "102 episodes - episode_reward: 96.765 [10.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 53 (520000 steps performed)\n",
      "10000/10000 [==============================] - 8s 800us/step - reward: 1.0000\n",
      "112 episodes - episode_reward: 90.554 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 54 (530000 steps performed)\n",
      "10000/10000 [==============================] - 8s 794us/step - reward: 1.0000\n",
      "92 episodes - episode_reward: 108.207 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 55 (540000 steps performed)\n",
      "10000/10000 [==============================] - 8s 802us/step - reward: 1.0000\n",
      "116 episodes - episode_reward: 86.828 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 56 (550000 steps performed)\n",
      "10000/10000 [==============================] - 8s 797us/step - reward: 1.0000\n",
      "107 episodes - episode_reward: 93.000 [10.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 57 (560000 steps performed)\n",
      "10000/10000 [==============================] - 8s 797us/step - reward: 1.0000\n",
      "122 episodes - episode_reward: 82.164 [10.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 58 (570000 steps performed)\n",
      "10000/10000 [==============================] - 8s 796us/step - reward: 1.0000\n",
      "105 episodes - episode_reward: 95.162 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 59 (580000 steps performed)\n",
      "10000/10000 [==============================] - 8s 802us/step - reward: 1.0000\n",
      "107 episodes - episode_reward: 93.570 [12.000, 200.000] - mean_best_reward: 197.000\n",
      "\n",
      "Interval 60 (590000 steps performed)\n",
      "10000/10000 [==============================] - 8s 794us/step - reward: 1.0000\n",
      "102 episodes - episode_reward: 97.235 [10.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 61 (600000 steps performed)\n",
      "10000/10000 [==============================] - 8s 801us/step - reward: 1.0000\n",
      "107 episodes - episode_reward: 94.374 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 62 (610000 steps performed)\n",
      "10000/10000 [==============================] - 8s 809us/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.081 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 63 (620000 steps performed)\n",
      "10000/10000 [==============================] - 8s 796us/step - reward: 1.0000\n",
      "102 episodes - episode_reward: 97.275 [13.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 64 (630000 steps performed)\n",
      "10000/10000 [==============================] - 8s 795us/step - reward: 1.0000\n",
      "100 episodes - episode_reward: 100.210 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 65 (640000 steps performed)\n",
      "10000/10000 [==============================] - 8s 802us/step - reward: 1.0000\n",
      "107 episodes - episode_reward: 92.953 [8.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 66 (650000 steps performed)\n",
      "10000/10000 [==============================] - 8s 792us/step - reward: 1.0000\n",
      "102 episodes - episode_reward: 98.735 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 67 (660000 steps performed)\n",
      "10000/10000 [==============================] - 8s 786us/step - reward: 1.0000\n",
      "103 episodes - episode_reward: 96.864 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 68 (670000 steps performed)\n",
      "10000/10000 [==============================] - 8s 793us/step - reward: 1.0000\n",
      "103 episodes - episode_reward: 97.612 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 69 (680000 steps performed)\n",
      "10000/10000 [==============================] - 8s 797us/step - reward: 1.0000\n",
      "118 episodes - episode_reward: 84.331 [11.000, 200.000] - mean_best_reward: 195.000\n",
      "\n",
      "Interval 70 (690000 steps performed)\n",
      "10000/10000 [==============================] - 8s 793us/step - reward: 1.0000\n",
      "105 episodes - episode_reward: 95.076 [10.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 71 (700000 steps performed)\n",
      "10000/10000 [==============================] - 8s 786us/step - reward: 1.0000\n",
      "101 episodes - episode_reward: 99.554 [10.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 72 (710000 steps performed)\n",
      "10000/10000 [==============================] - 8s 788us/step - reward: 1.0000\n",
      "109 episodes - episode_reward: 91.853 [14.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 73 (720000 steps performed)\n",
      "10000/10000 [==============================] - 8s 788us/step - reward: 1.0000\n",
      "107 episodes - episode_reward: 93.019 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 74 (730000 steps performed)\n",
      "10000/10000 [==============================] - 8s 792us/step - reward: 1.0000\n",
      "96 episodes - episode_reward: 103.354 [11.000, 200.000] - mean_best_reward: 195.500\n",
      "\n",
      "Interval 75 (740000 steps performed)\n",
      "10000/10000 [==============================] - 8s 791us/step - reward: 1.0000\n",
      "106 episodes - episode_reward: 95.528 [12.000, 200.000] - mean_best_reward: 188.500\n",
      "\n",
      "Interval 76 (750000 steps performed)\n",
      "10000/10000 [==============================] - 8s 781us/step - reward: 1.0000\n",
      "101 episodes - episode_reward: 98.059 [15.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 77 (760000 steps performed)\n",
      "10000/10000 [==============================] - 8s 783us/step - reward: 1.0000\n",
      "96 episodes - episode_reward: 105.010 [13.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 78 (770000 steps performed)\n",
      "10000/10000 [==============================] - 8s 788us/step - reward: 1.0000\n",
      "106 episodes - episode_reward: 94.236 [10.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 79 (780000 steps performed)\n",
      "10000/10000 [==============================] - 8s 783us/step - reward: 1.0000\n",
      "113 episodes - episode_reward: 88.336 [10.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 80 (790000 steps performed)\n",
      "10000/10000 [==============================] - 8s 783us/step - reward: 1.0000\n",
      "106 episodes - episode_reward: 94.406 [13.000, 200.000] - mean_best_reward: 199.250\n",
      "\n",
      "Interval 81 (800000 steps performed)\n",
      "10000/10000 [==============================] - 8s 784us/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 99.626 [10.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 82 (810000 steps performed)\n",
      "10000/10000 [==============================] - 8s 789us/step - reward: 1.0000\n",
      "104 episodes - episode_reward: 97.731 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 83 (820000 steps performed)\n",
      "10000/10000 [==============================] - 8s 799us/step - reward: 1.0000\n",
      "95 episodes - episode_reward: 105.053 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 84 (830000 steps performed)\n",
      "10000/10000 [==============================] - 8s 781us/step - reward: 1.0000\n",
      "97 episodes - episode_reward: 102.258 [13.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 85 (840000 steps performed)\n",
      "10000/10000 [==============================] - 8s 781us/step - reward: 1.0000\n",
      "88 episodes - episode_reward: 113.261 [18.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 86 (850000 steps performed)\n",
      "10000/10000 [==============================] - 8s 792us/step - reward: 1.0000\n",
      "115 episodes - episode_reward: 87.591 [10.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 87 (860000 steps performed)\n",
      "10000/10000 [==============================] - 8s 787us/step - reward: 1.0000\n",
      "100 episodes - episode_reward: 100.120 [18.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 88 (870000 steps performed)\n",
      "10000/10000 [==============================] - 8s 782us/step - reward: 1.0000\n",
      "104 episodes - episode_reward: 96.413 [14.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 89 (880000 steps performed)\n",
      "10000/10000 [==============================] - 8s 781us/step - reward: 1.0000\n",
      "104 episodes - episode_reward: 96.500 [14.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 90 (890000 steps performed)\n",
      "10000/10000 [==============================] - 8s 785us/step - reward: 1.0000\n",
      "98 episodes - episode_reward: 101.612 [13.000, 200.000] - mean_best_reward: 199.000\n",
      "\n",
      "Interval 91 (900000 steps performed)\n",
      "10000/10000 [==============================] - 8s 785us/step - reward: 1.0000\n",
      "97 episodes - episode_reward: 102.196 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 92 (910000 steps performed)\n",
      "10000/10000 [==============================] - 8s 786us/step - reward: 1.0000\n",
      "102 episodes - episode_reward: 97.971 [10.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 93 (920000 steps performed)\n",
      "10000/10000 [==============================] - 8s 780us/step - reward: 1.0000\n",
      "91 episodes - episode_reward: 110.429 [18.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 94 (930000 steps performed)\n",
      "10000/10000 [==============================] - 8s 779us/step - reward: 1.0000\n",
      "106 episodes - episode_reward: 93.774 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 95 (940000 steps performed)\n",
      "10000/10000 [==============================] - 8s 772us/step - reward: 1.0000\n",
      "96 episodes - episode_reward: 105.667 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 96 (950000 steps performed)\n",
      "10000/10000 [==============================] - 8s 789us/step - reward: 1.0000\n",
      "101 episodes - episode_reward: 99.059 [14.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 97 (960000 steps performed)\n",
      "10000/10000 [==============================] - 8s 785us/step - reward: 1.0000\n",
      "98 episodes - episode_reward: 101.224 [15.000, 200.000] - mean_best_reward: 196.000\n",
      "\n",
      "Interval 98 (970000 steps performed)\n",
      "10000/10000 [==============================] - 8s 777us/step - reward: 1.0000\n",
      "97 episodes - episode_reward: 102.928 [10.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 99 (980000 steps performed)\n",
      "10000/10000 [==============================] - 8s 784us/step - reward: 1.0000\n",
      "104 episodes - episode_reward: 96.240 [13.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 100 (990000 steps performed)\n",
      "10000/10000 [==============================] - 8s 781us/step - reward: 1.0000\n",
      "98 episodes - episode_reward: 101.847 [10.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 101 (1000000 steps performed)\n",
      "10000/10000 [==============================] - 8s 787us/step - reward: 1.0000\n",
      "97 episodes - episode_reward: 103.959 [10.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 102 (1010000 steps performed)\n",
      "10000/10000 [==============================] - 8s 775us/step - reward: 1.0000\n",
      "90 episodes - episode_reward: 111.133 [15.000, 200.000] - mean_best_reward: 199.000\n",
      "\n",
      "Interval 103 (1020000 steps performed)\n",
      "10000/10000 [==============================] - 8s 780us/step - reward: 1.0000\n",
      "94 episodes - episode_reward: 105.266 [14.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 104 (1030000 steps performed)\n",
      "10000/10000 [==============================] - 8s 777us/step - reward: 1.0000\n",
      "103 episodes - episode_reward: 98.087 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 105 (1040000 steps performed)\n",
      "10000/10000 [==============================] - 8s 771us/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 100.475 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 106 (1050000 steps performed)\n",
      "10000/10000 [==============================] - 8s 771us/step - reward: 1.0000\n",
      "92 episodes - episode_reward: 109.130 [11.000, 200.000] - mean_best_reward: 197.500\n",
      "\n",
      "Interval 107 (1060000 steps performed)\n",
      "10000/10000 [==============================] - 8s 777us/step - reward: 1.0000\n",
      "96 episodes - episode_reward: 103.625 [13.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 108 (1070000 steps performed)\n",
      "10000/10000 [==============================] - 8s 776us/step - reward: 1.0000\n",
      "97 episodes - episode_reward: 103.835 [14.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 109 (1080000 steps performed)\n",
      "10000/10000 [==============================] - 8s 782us/step - reward: 1.0000\n",
      "108 episodes - episode_reward: 92.528 [10.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 110 (1090000 steps performed)\n",
      "10000/10000 [==============================] - 8s 772us/step - reward: 1.0000\n",
      "100 episodes - episode_reward: 99.300 [15.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 111 (1100000 steps performed)\n",
      "10000/10000 [==============================] - 8s 783us/step - reward: 1.0000\n",
      "105 episodes - episode_reward: 95.619 [10.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 112 (1110000 steps performed)\n",
      "10000/10000 [==============================] - 8s 772us/step - reward: 1.0000\n",
      "101 episodes - episode_reward: 99.416 [10.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 113 (1120000 steps performed)\n",
      "10000/10000 [==============================] - 8s 780us/step - reward: 1.0000\n",
      "105 episodes - episode_reward: 94.924 [8.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 114 (1130000 steps performed)\n",
      "10000/10000 [==============================] - 8s 769us/step - reward: 1.0000\n",
      "96 episodes - episode_reward: 102.750 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 115 (1140000 steps performed)\n",
      "10000/10000 [==============================] - 8s 773us/step - reward: 1.0000\n",
      "92 episodes - episode_reward: 108.707 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 116 (1150000 steps performed)\n",
      "10000/10000 [==============================] - 8s 774us/step - reward: 1.0000\n",
      "104 episodes - episode_reward: 97.731 [10.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 117 (1160000 steps performed)\n",
      "10000/10000 [==============================] - 8s 772us/step - reward: 1.0000\n",
      "92 episodes - episode_reward: 107.522 [10.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 118 (1170000 steps performed)\n",
      "10000/10000 [==============================] - 8s 777us/step - reward: 1.0000\n",
      "98 episodes - episode_reward: 102.704 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 119 (1180000 steps performed)\n",
      "10000/10000 [==============================] - 8s 775us/step - reward: 1.0000\n",
      "101 episodes - episode_reward: 98.218 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 120 (1190000 steps performed)\n",
      "10000/10000 [==============================] - 8s 774us/step - reward: 1.0000\n",
      "90 episodes - episode_reward: 112.056 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 121 (1200000 steps performed)\n",
      "10000/10000 [==============================] - 8s 789us/step - reward: 1.0000\n",
      "95 episodes - episode_reward: 105.800 [16.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 122 (1210000 steps performed)\n",
      "10000/10000 [==============================] - 8s 771us/step - reward: 1.0000\n",
      "97 episodes - episode_reward: 102.979 [15.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 123 (1220000 steps performed)\n",
      "10000/10000 [==============================] - 8s 775us/step - reward: 1.0000\n",
      "105 episodes - episode_reward: 95.152 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 124 (1230000 steps performed)\n",
      "10000/10000 [==============================] - 8s 771us/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 100.657 [16.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 125 (1240000 steps performed)\n",
      "10000/10000 [==============================] - 8s 770us/step - reward: 1.0000\n",
      "101 episodes - episode_reward: 99.178 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 126 (1250000 steps performed)\n",
      "10000/10000 [==============================] - 8s 782us/step - reward: 1.0000\n",
      "94 episodes - episode_reward: 105.628 [10.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 127 (1260000 steps performed)\n",
      "10000/10000 [==============================] - 8s 772us/step - reward: 1.0000\n",
      "91 episodes - episode_reward: 111.055 [13.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 128 (1270000 steps performed)\n",
      "10000/10000 [==============================] - 8s 781us/step - reward: 1.0000\n",
      "106 episodes - episode_reward: 93.500 [13.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 129 (1280000 steps performed)\n",
      "10000/10000 [==============================] - 8s 773us/step - reward: 1.0000\n",
      "101 episodes - episode_reward: 98.465 [9.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 130 (1290000 steps performed)\n",
      "10000/10000 [==============================] - 8s 771us/step - reward: 1.0000\n",
      "103 episodes - episode_reward: 97.524 [14.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 131 (1300000 steps performed)\n",
      "10000/10000 [==============================] - 8s 763us/step - reward: 1.0000\n",
      "94 episodes - episode_reward: 107.181 [13.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 132 (1310000 steps performed)\n",
      "10000/10000 [==============================] - 8s 760us/step - reward: 1.0000\n",
      "90 episodes - episode_reward: 110.544 [15.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 133 (1320000 steps performed)\n",
      "10000/10000 [==============================] - 8s 769us/step - reward: 1.0000\n",
      "101 episodes - episode_reward: 98.683 [9.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 134 (1330000 steps performed)\n",
      "10000/10000 [==============================] - 8s 758us/step - reward: 1.0000\n",
      "93 episodes - episode_reward: 108.129 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 135 (1340000 steps performed)\n",
      "10000/10000 [==============================] - 8s 760us/step - reward: 1.0000\n",
      "102 episodes - episode_reward: 96.892 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 136 (1350000 steps performed)\n",
      "10000/10000 [==============================] - 8s 761us/step - reward: 1.0000\n",
      "91 episodes - episode_reward: 110.549 [13.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 137 (1360000 steps performed)\n",
      "10000/10000 [==============================] - 8s 766us/step - reward: 1.0000\n",
      "106 episodes - episode_reward: 95.104 [14.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 138 (1370000 steps performed)\n",
      "10000/10000 [==============================] - 8s 750us/step - reward: 1.0000\n",
      "86 episodes - episode_reward: 116.570 [9.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 139 (1380000 steps performed)\n",
      "10000/10000 [==============================] - 8s 758us/step - reward: 1.0000\n",
      "103 episodes - episode_reward: 96.835 [10.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 140 (1390000 steps performed)\n",
      "10000/10000 [==============================] - 8s 775us/step - reward: 1.0000\n",
      "101 episodes - episode_reward: 98.673 [10.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 141 (1400000 steps performed)\n",
      "10000/10000 [==============================] - 8s 764us/step - reward: 1.0000\n",
      "93 episodes - episode_reward: 107.882 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 142 (1410000 steps performed)\n",
      "10000/10000 [==============================] - 8s 763us/step - reward: 1.0000\n",
      "102 episodes - episode_reward: 98.382 [15.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 143 (1420000 steps performed)\n",
      "10000/10000 [==============================] - 8s 759us/step - reward: 1.0000\n",
      "101 episodes - episode_reward: 98.000 [9.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 144 (1430000 steps performed)\n",
      "10000/10000 [==============================] - 8s 758us/step - reward: 1.0000\n",
      "90 episodes - episode_reward: 111.722 [13.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 145 (1440000 steps performed)\n",
      "10000/10000 [==============================] - 8s 768us/step - reward: 1.0000\n",
      "106 episodes - episode_reward: 94.434 [10.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 146 (1450000 steps performed)\n",
      "10000/10000 [==============================] - 8s 753us/step - reward: 1.0000\n",
      "94 episodes - episode_reward: 106.766 [15.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 147 (1460000 steps performed)\n",
      "10000/10000 [==============================] - 8s 755us/step - reward: 1.0000\n",
      "84 episodes - episode_reward: 118.881 [19.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 148 (1470000 steps performed)\n",
      "10000/10000 [==============================] - 8s 757us/step - reward: 1.0000\n",
      "90 episodes - episode_reward: 110.778 [13.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 149 (1480000 steps performed)\n",
      "10000/10000 [==============================] - 8s 763us/step - reward: 1.0000\n",
      "102 episodes - episode_reward: 98.412 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 150 (1490000 steps performed)\n",
      "10000/10000 [==============================] - 8s 756us/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.091 [9.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 151 (1500000 steps performed)\n",
      "10000/10000 [==============================] - 8s 771us/step - reward: 1.0000\n",
      "96 episodes - episode_reward: 103.698 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 152 (1510000 steps performed)\n",
      "10000/10000 [==============================] - 8s 759us/step - reward: 1.0000\n",
      "101 episodes - episode_reward: 99.376 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 153 (1520000 steps performed)\n",
      "10000/10000 [==============================] - 8s 757us/step - reward: 1.0000\n",
      "91 episodes - episode_reward: 108.132 [14.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 154 (1530000 steps performed)\n",
      "10000/10000 [==============================] - 8s 762us/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.970 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 155 (1540000 steps performed)\n",
      "10000/10000 [==============================] - 8s 761us/step - reward: 1.0000\n",
      "98 episodes - episode_reward: 102.602 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 156 (1550000 steps performed)\n",
      "10000/10000 [==============================] - 7s 748us/step - reward: 1.0000\n",
      "94 episodes - episode_reward: 106.585 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 157 (1560000 steps performed)\n",
      "10000/10000 [==============================] - 8s 757us/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 100.545 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 158 (1570000 steps performed)\n",
      "10000/10000 [==============================] - 8s 771us/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.424 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 159 (1580000 steps performed)\n",
      "10000/10000 [==============================] - 8s 756us/step - reward: 1.0000\n",
      "98 episodes - episode_reward: 101.092 [10.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 160 (1590000 steps performed)\n",
      "10000/10000 [==============================] - 8s 752us/step - reward: 1.0000\n",
      "100 episodes - episode_reward: 100.270 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 161 (1600000 steps performed)\n",
      "10000/10000 [==============================] - 8s 762us/step - reward: 1.0000\n",
      "93 episodes - episode_reward: 107.151 [16.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 162 (1610000 steps performed)\n",
      "10000/10000 [==============================] - 8s 754us/step - reward: 1.0000\n",
      "95 episodes - episode_reward: 105.579 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 163 (1620000 steps performed)\n",
      "10000/10000 [==============================] - 8s 754us/step - reward: 1.0000\n",
      "88 episodes - episode_reward: 114.148 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 164 (1630000 steps performed)\n",
      "10000/10000 [==============================] - 8s 753us/step - reward: 1.0000\n",
      "91 episodes - episode_reward: 109.132 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 165 (1640000 steps performed)\n",
      "10000/10000 [==============================] - 7s 749us/step - reward: 1.0000\n",
      "102 episodes - episode_reward: 98.745 [14.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 166 (1650000 steps performed)\n",
      "10000/10000 [==============================] - 7s 747us/step - reward: 1.0000\n",
      "93 episodes - episode_reward: 106.602 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 167 (1660000 steps performed)\n",
      "10000/10000 [==============================] - 7s 747us/step - reward: 1.0000\n",
      "95 episodes - episode_reward: 105.368 [16.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 168 (1670000 steps performed)\n",
      "10000/10000 [==============================] - 8s 751us/step - reward: 1.0000\n",
      "88 episodes - episode_reward: 113.511 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 169 (1680000 steps performed)\n",
      "10000/10000 [==============================] - 7s 748us/step - reward: 1.0000\n",
      "101 episodes - episode_reward: 100.059 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 170 (1690000 steps performed)\n",
      "10000/10000 [==============================] - 7s 747us/step - reward: 1.0000\n",
      "93 episodes - episode_reward: 107.462 [13.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 171 (1700000 steps performed)\n",
      "10000/10000 [==============================] - 7s 744us/step - reward: 1.0000\n",
      "95 episodes - episode_reward: 105.105 [13.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 172 (1710000 steps performed)\n",
      "10000/10000 [==============================] - 7s 743us/step - reward: 1.0000\n",
      "90 episodes - episode_reward: 111.311 [9.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 173 (1720000 steps performed)\n",
      "10000/10000 [==============================] - 7s 749us/step - reward: 1.0000\n",
      "93 episodes - episode_reward: 106.527 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 174 (1730000 steps performed)\n",
      "10000/10000 [==============================] - 7s 744us/step - reward: 1.0000\n",
      "87 episodes - episode_reward: 113.966 [10.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 175 (1740000 steps performed)\n",
      "10000/10000 [==============================] - 8s 762us/step - reward: 1.0000\n",
      "96 episodes - episode_reward: 105.865 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 176 (1750000 steps performed)\n",
      "10000/10000 [==============================] - 7s 743us/step - reward: 1.0000\n",
      "88 episodes - episode_reward: 113.602 [10.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 177 (1760000 steps performed)\n",
      "10000/10000 [==============================] - 7s 749us/step - reward: 1.0000\n",
      "95 episodes - episode_reward: 105.116 [15.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 178 (1770000 steps performed)\n",
      "10000/10000 [==============================] - 7s 749us/step - reward: 1.0000\n",
      "97 episodes - episode_reward: 103.330 [10.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 179 (1780000 steps performed)\n",
      "10000/10000 [==============================] - 7s 743us/step - reward: 1.0000\n",
      "93 episodes - episode_reward: 106.667 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 180 (1790000 steps performed)\n",
      "10000/10000 [==============================] - 7s 747us/step - reward: 1.0000\n",
      "94 episodes - episode_reward: 105.468 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 181 (1800000 steps performed)\n",
      "10000/10000 [==============================] - 7s 743us/step - reward: 1.0000\n",
      "98 episodes - episode_reward: 103.898 [10.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 182 (1810000 steps performed)\n",
      "10000/10000 [==============================] - 7s 742us/step - reward: 1.0000\n",
      "93 episodes - episode_reward: 107.366 [9.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 183 (1820000 steps performed)\n",
      "10000/10000 [==============================] - 7s 743us/step - reward: 1.0000\n",
      "96 episodes - episode_reward: 104.135 [10.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 184 (1830000 steps performed)\n",
      "10000/10000 [==============================] - 7s 735us/step - reward: 1.0000\n",
      "86 episodes - episode_reward: 114.419 [15.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 185 (1840000 steps performed)\n",
      "10000/10000 [==============================] - 8s 751us/step - reward: 1.0000\n",
      "103 episodes - episode_reward: 98.641 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 186 (1850000 steps performed)\n",
      "10000/10000 [==============================] - 8s 752us/step - reward: 1.0000\n",
      "103 episodes - episode_reward: 97.087 [13.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 187 (1860000 steps performed)\n",
      "10000/10000 [==============================] - 7s 741us/step - reward: 1.0000\n",
      "101 episodes - episode_reward: 97.733 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 188 (1870000 steps performed)\n",
      "10000/10000 [==============================] - 7s 743us/step - reward: 1.0000\n",
      "93 episodes - episode_reward: 109.086 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 189 (1880000 steps performed)\n",
      "10000/10000 [==============================] - 7s 740us/step - reward: 1.0000\n",
      "98 episodes - episode_reward: 101.918 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 190 (1890000 steps performed)\n",
      "10000/10000 [==============================] - 7s 741us/step - reward: 1.0000\n",
      "89 episodes - episode_reward: 111.854 [13.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 191 (1900000 steps performed)\n",
      "10000/10000 [==============================] - 7s 749us/step - reward: 1.0000\n",
      "91 episodes - episode_reward: 110.000 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 192 (1910000 steps performed)\n",
      "10000/10000 [==============================] - 8s 757us/step - reward: 1.0000\n",
      "93 episodes - episode_reward: 106.290 [14.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 193 (1920000 steps performed)\n",
      "10000/10000 [==============================] - 7s 742us/step - reward: 1.0000\n",
      "94 episodes - episode_reward: 107.904 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 194 (1930000 steps performed)\n",
      "10000/10000 [==============================] - 7s 739us/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.081 [10.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 195 (1940000 steps performed)\n",
      "10000/10000 [==============================] - 7s 734us/step - reward: 1.0000\n",
      "91 episodes - episode_reward: 109.857 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 196 (1950000 steps performed)\n",
      "10000/10000 [==============================] - 7s 741us/step - reward: 1.0000\n",
      "100 episodes - episode_reward: 98.950 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 197 (1960000 steps performed)\n",
      "10000/10000 [==============================] - 7s 738us/step - reward: 1.0000\n",
      "90 episodes - episode_reward: 112.022 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 198 (1970000 steps performed)\n",
      "10000/10000 [==============================] - 7s 743us/step - reward: 1.0000\n",
      "103 episodes - episode_reward: 96.854 [12.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 199 (1980000 steps performed)\n",
      "10000/10000 [==============================] - 7s 736us/step - reward: 1.0000\n",
      "95 episodes - episode_reward: 105.958 [11.000, 200.000] - mean_best_reward: 200.000\n",
      "\n",
      "Interval 200 (1990000 steps performed)\n",
      "10000/10000 [==============================] - 7s 736us/step - reward: 1.0000\n",
      "done, took 1571.902 seconds\n"
     ]
    }
   ],
   "source": [
    "# Okay, now it's time to learn something! We visualize the training here for show, but this\n",
    "# slows down training quite a lot. You can always safely abort the training prematurely using\n",
    "# Ctrl + C.\n",
    "cem.fit(env, nb_steps=2000000, visualize=False, verbose=1)\n",
    "\n",
    "# After training is done, we save the best weights.\n",
    "cem.save_weights(f'cem_{ENV_NAME}_l3k_w10_ba50_st2m_params.h5f', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 120.000, steps: 120\n",
      "Episode 2: reward: 128.000, steps: 128\n",
      "Episode 3: reward: 112.000, steps: 112\n",
      "Episode 4: reward: 128.000, steps: 128\n",
      "Episode 5: reward: 118.000, steps: 118\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff4e2f05c70>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, evaluate our algorithm for 5 episodes.\n",
    "cem.test(env, nb_episodes=5, visualize=True)\n",
    "#sars.test(env, nb_episodes=5, visualize=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 186.000, steps: 186\n",
      "Episode 2: reward: 175.000, steps: 175\n",
      "Episode 3: reward: 151.000, steps: 151\n",
      "Episode 4: reward: 188.000, steps: 188\n",
      "Episode 5: reward: 148.000, steps: 148\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff4e2c424f0>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cem.test(env, nb_episodes=5, visualize=True)\n",
    "\n",
    "# l5k_w10_ba50_st2m = 90\n",
    "# l3k_w10_ba50_st2m = 148"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 200.000, steps: 200\n",
      "Episode 2: reward: 200.000, steps: 200\n",
      "Episode 3: reward: 200.000, steps: 200\n",
      "Episode 4: reward: 200.000, steps: 200\n",
      "Episode 5: reward: 200.000, steps: 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff5ca733ac0>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cem.test(env, nb_episodes=5, visualize=True)\n",
    "# l2k_w10_ba50_st2m = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=5000, window_length=1)\n",
    "policy = BoltzmannQPolicy()\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
    "               target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dqn.fit(env, nb_steps=2500, visualize=True, verbose=1)\n",
    "\n",
    "# # After training is done, we save the final weights.\n",
    "# dqn.save_weights('dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)\n",
    "\n",
    "# # Finally, evaluate our algorithm for 5 episodes.\n",
    "# dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dqn.test(env, nb_episodes=22, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(state_size, num_actions):\n",
    "    input = Input(shape=(1,state_size))\n",
    "    x = Flatten()(input)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    #x = Dense(32, activation='relu')(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    output = Dense(num_actions, activation='linear')(x)\n",
    "    model = Model(inputs=input, outputs=output)\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=.6, value_min=.05, value_test=.02, nb_steps=10000)\n",
    "\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=20,\n",
    "               target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_callbacks(env_name):\n",
    "    checkpoint_weights_filename = 'dqn_' + env_name + '_weights_{step}.h5f'\n",
    "    log_filename = 'dqn_{}_log.json'.format(env_name)\n",
    "    callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=5000)]\n",
    "    callbacks += [FileLogger(log_filename, interval=100)]\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 150000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 1.0000\n",
      "292 episodes - episode_reward: 34.247 [15.000, 66.000] - loss: 0.650 - mae: 0.600 - mean_q: 0.999 - mean_eps: 0.175\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 1.0000\n",
      "286 episodes - episode_reward: 34.899 [22.000, 52.000] - loss: 0.581 - mae: 0.553 - mean_q: 0.999 - mean_eps: 0.050\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 1.0000\n",
      "279 episodes - episode_reward: 35.896 [22.000, 56.000] - loss: 0.558 - mae: 0.537 - mean_q: 0.999 - mean_eps: 0.050\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 65s 7ms/step - reward: 1.0000\n",
      "284 episodes - episode_reward: 35.169 [21.000, 55.000] - loss: 0.548 - mae: 0.531 - mean_q: 0.999 - mean_eps: 0.050\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 68s 7ms/step - reward: 1.0000\n",
      "286 episodes - episode_reward: 35.010 [20.000, 52.000] - loss: 0.543 - mae: 0.527 - mean_q: 0.999 - mean_eps: 0.050\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 68s 7ms/step - reward: 1.0000\n",
      "285 episodes - episode_reward: 35.088 [18.000, 56.000] - loss: 0.539 - mae: 0.524 - mean_q: 0.999 - mean_eps: 0.050\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 70s 7ms/step - reward: 1.0000\n",
      "282 episodes - episode_reward: 35.426 [20.000, 54.000] - loss: 0.535 - mae: 0.522 - mean_q: 0.999 - mean_eps: 0.050\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 70s 7ms/step - reward: 1.0000\n",
      "286 episodes - episode_reward: 34.986 [22.000, 55.000] - loss: 0.534 - mae: 0.521 - mean_q: 0.999 - mean_eps: 0.050\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 74s 7ms/step - reward: 1.0000\n",
      "278 episodes - episode_reward: 35.993 [21.000, 53.000] - loss: 0.532 - mae: 0.520 - mean_q: 0.999 - mean_eps: 0.050\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 72s 7ms/step - reward: 1.0000\n",
      "283 episodes - episode_reward: 35.329 [20.000, 58.000] - loss: 0.530 - mae: 0.518 - mean_q: 0.999 - mean_eps: 0.050\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 74s 7ms/step - reward: 1.0000\n",
      "290 episodes - episode_reward: 34.483 [20.000, 55.000] - loss: 0.530 - mae: 0.518 - mean_q: 0.999 - mean_eps: 0.050\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 76s 8ms/step - reward: 1.0000\n",
      "281 episodes - episode_reward: 35.566 [22.000, 55.000] - loss: 0.529 - mae: 0.518 - mean_q: 0.999 - mean_eps: 0.050\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 82s 8ms/step - reward: 1.0000\n",
      "288 episodes - episode_reward: 34.688 [23.000, 54.000] - loss: 0.528 - mae: 0.517 - mean_q: 0.999 - mean_eps: 0.050\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 84s 8ms/step - reward: 1.0000\n",
      "281 episodes - episode_reward: 35.520 [23.000, 56.000] - loss: 0.527 - mae: 0.516 - mean_q: 0.999 - mean_eps: 0.050\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 86s 9ms/step - reward: 1.0000\n",
      "done, took 1083.067 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff4e914b910>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = build_callbacks(ENV_NAME)\n",
    "\n",
    "dqn.fit(env, nb_steps=150000, visualize=False, verbose=1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 44.000, steps: 44\n",
      "Episode 2: reward: 35.000, steps: 35\n",
      "Episode 3: reward: 38.000, steps: 38\n",
      "Episode 4: reward: 40.000, steps: 40\n",
      "Episode 5: reward: 30.000, steps: 30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff4e8740c10>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
