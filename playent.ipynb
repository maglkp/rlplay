{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.cem import CEMAgent\n",
    "from rl.agents.sarsa import SARSAAgent\n",
    "from rl.memory import EpisodeParameterMemory\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import LinearAnnealedPolicy\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n",
      "Discrete(5)\n",
      "5\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"gym_ent:ent-v0\")\n",
    "\n",
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "\n",
    "nb_actions = env.action_space.n\n",
    "obs_dim = env.observation_space.shape\n",
    "print(nb_actions)\n",
    "print(obs_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_3 (Flatten)          (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 10        \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 10\n",
      "Trainable params: 10\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Option 1 : Simple model\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,)))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sars = SARSAAgent(model=model, nb_actions=nb_actions, nb_steps_warmup=1000, train_interval=50)\n",
    "sars.compile(optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 200000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 9s 930us/step - reward: 0.8418\n",
      "10000 episodes - episode_reward: 0.842 [-1.000, 1.000]\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 9s 940us/step - reward: 0.8456\n",
      "10000 episodes - episode_reward: 0.846 [-1.000, 1.000]\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 9s 921us/step - reward: 0.8246\n",
      "10000 episodes - episode_reward: 0.825 [-1.000, 1.000]\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 10s 964us/step - reward: 0.8452\n",
      "10000 episodes - episode_reward: 0.845 [-1.000, 1.000]\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 9s 892us/step - reward: 0.8392\n",
      "10000 episodes - episode_reward: 0.839 [-1.000, 1.000]\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 8s 831us/step - reward: 0.8392\n",
      "10000 episodes - episode_reward: 0.839 [-1.000, 1.000]\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 9s 870us/step - reward: 0.8358\n",
      "10000 episodes - episode_reward: 0.836 [-1.000, 1.000]\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 8s 849us/step - reward: 0.8432\n",
      "10000 episodes - episode_reward: 0.843 [-1.000, 1.000]\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 8s 812us/step - reward: 0.8286\n",
      "10000 episodes - episode_reward: 0.829 [-1.000, 1.000]\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 9s 889us/step - reward: 0.8480\n",
      "10000 episodes - episode_reward: 0.848 [-1.000, 1.000]\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 9s 935us/step - reward: 0.8360\n",
      "10000 episodes - episode_reward: 0.836 [-1.000, 1.000]\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 9s 918us/step - reward: 0.8248\n",
      "10000 episodes - episode_reward: 0.825 [-1.000, 1.000]\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 9s 856us/step - reward: 0.8368\n",
      "10000 episodes - episode_reward: 0.837 [-1.000, 1.000]\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 9s 934us/step - reward: 0.8382\n",
      "10000 episodes - episode_reward: 0.838 [-1.000, 1.000]\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 9s 930us/step - reward: 0.8442\n",
      "10000 episodes - episode_reward: 0.844 [-1.000, 1.000]\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 9s 919us/step - reward: 0.8358\n",
      "10000 episodes - episode_reward: 0.836 [-1.000, 1.000]\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 9s 911us/step - reward: 0.8346\n",
      "10000 episodes - episode_reward: 0.835 [-1.000, 1.000]\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 9s 923us/step - reward: 0.8474\n",
      "10000 episodes - episode_reward: 0.847 [-1.000, 1.000]\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 11s 1ms/step - reward: 0.8388\n",
      "10000 episodes - episode_reward: 0.839 [-1.000, 1.000]\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 9s 909us/step - reward: 0.8396\n",
      "done, took 182.418 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f53f0745a30>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sars.fit(env, nb_steps=200000, visualize=False, verbose=1)\n",
    "# After training is done, we save the best weights.\n",
    "#sars.save_weights(f'sars_{ENV_NAME}_params.h5f', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 1.000, steps: 1\n",
      "Episode 2: reward: 1.000, steps: 1\n",
      "Episode 3: reward: 1.000, steps: 1\n",
      "Episode 4: reward: 1.000, steps: 1\n",
      "Episode 5: reward: 1.000, steps: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f54025ea4f0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sars.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 18s 2ms/step - reward: -0.5846\n",
      "done, took 17.598 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f53fc4d4880>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = EpisodeParameterMemory(limit=2000, window_length=1)\n",
    "cem = CEMAgent(model=model, nb_actions=nb_actions, memory=memory,\n",
    "               batch_size=50, nb_steps_warmup=500, train_interval=50, elite_frac=0.05)\n",
    "cem.compile()\n",
    "\n",
    "cem.fit(env, nb_steps=10000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 1.000, steps: 1\n",
      "Episode 2: reward: 1.000, steps: 1\n",
      "Episode 3: reward: 1.000, steps: 1\n",
      "Episode 4: reward: 1.000, steps: 1\n",
      "Episode 5: reward: 1.000, steps: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f54025e1130>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cem.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
